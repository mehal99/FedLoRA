{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.12751677852349,
  "eval_steps": 500,
  "global_step": 3400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0026845637583892616,
      "grad_norm": 0.20558564364910126,
      "learning_rate": 0.0002999193548387097,
      "loss": 1.5104,
      "step": 1
    },
    {
      "epoch": 0.005369127516778523,
      "grad_norm": 0.17654922604560852,
      "learning_rate": 0.00029983870967741934,
      "loss": 1.4552,
      "step": 2
    },
    {
      "epoch": 0.008053691275167786,
      "grad_norm": 0.13088832795619965,
      "learning_rate": 0.000299758064516129,
      "loss": 1.447,
      "step": 3
    },
    {
      "epoch": 0.010738255033557046,
      "grad_norm": 0.12835407257080078,
      "learning_rate": 0.00029967741935483865,
      "loss": 1.5961,
      "step": 4
    },
    {
      "epoch": 0.013422818791946308,
      "grad_norm": 0.1199553981423378,
      "learning_rate": 0.00029959677419354836,
      "loss": 1.5795,
      "step": 5
    },
    {
      "epoch": 0.016107382550335572,
      "grad_norm": 0.12139048427343369,
      "learning_rate": 0.000299516129032258,
      "loss": 1.4553,
      "step": 6
    },
    {
      "epoch": 0.01879194630872483,
      "grad_norm": 0.12315629422664642,
      "learning_rate": 0.00029943548387096773,
      "loss": 1.4536,
      "step": 7
    },
    {
      "epoch": 0.021476510067114093,
      "grad_norm": 0.12232065200805664,
      "learning_rate": 0.0002993548387096774,
      "loss": 1.5836,
      "step": 8
    },
    {
      "epoch": 0.024161073825503355,
      "grad_norm": 0.10371574759483337,
      "learning_rate": 0.00029927419354838704,
      "loss": 1.4873,
      "step": 9
    },
    {
      "epoch": 0.026845637583892617,
      "grad_norm": 0.10417584329843521,
      "learning_rate": 0.00029919354838709675,
      "loss": 1.4396,
      "step": 10
    },
    {
      "epoch": 0.02953020134228188,
      "grad_norm": 0.10901926457881927,
      "learning_rate": 0.0002991129032258064,
      "loss": 1.587,
      "step": 11
    },
    {
      "epoch": 0.032214765100671144,
      "grad_norm": 0.10332956165075302,
      "learning_rate": 0.0002990322580645161,
      "loss": 1.5043,
      "step": 12
    },
    {
      "epoch": 0.0348993288590604,
      "grad_norm": 0.10228747129440308,
      "learning_rate": 0.0002989516129032258,
      "loss": 1.6409,
      "step": 13
    },
    {
      "epoch": 0.03758389261744966,
      "grad_norm": 0.08858837932348251,
      "learning_rate": 0.00029887096774193543,
      "loss": 1.5888,
      "step": 14
    },
    {
      "epoch": 0.040268456375838924,
      "grad_norm": 0.10993199795484543,
      "learning_rate": 0.00029879032258064514,
      "loss": 1.5664,
      "step": 15
    },
    {
      "epoch": 0.042953020134228186,
      "grad_norm": 0.11338508129119873,
      "learning_rate": 0.0002987096774193548,
      "loss": 1.5146,
      "step": 16
    },
    {
      "epoch": 0.04563758389261745,
      "grad_norm": 0.09572850912809372,
      "learning_rate": 0.0002986290322580645,
      "loss": 1.513,
      "step": 17
    },
    {
      "epoch": 0.04832214765100671,
      "grad_norm": 0.09782791882753372,
      "learning_rate": 0.00029854838709677417,
      "loss": 1.5628,
      "step": 18
    },
    {
      "epoch": 0.05100671140939597,
      "grad_norm": 0.07841985672712326,
      "learning_rate": 0.0002984677419354838,
      "loss": 1.632,
      "step": 19
    },
    {
      "epoch": 0.053691275167785234,
      "grad_norm": 0.09175480902194977,
      "learning_rate": 0.00029838709677419353,
      "loss": 1.5929,
      "step": 20
    },
    {
      "epoch": 0.056375838926174496,
      "grad_norm": 0.09978857636451721,
      "learning_rate": 0.0002983064516129032,
      "loss": 1.4785,
      "step": 21
    },
    {
      "epoch": 0.05906040268456376,
      "grad_norm": 0.10764475166797638,
      "learning_rate": 0.0002982258064516129,
      "loss": 1.5957,
      "step": 22
    },
    {
      "epoch": 0.06174496644295302,
      "grad_norm": 0.10127782821655273,
      "learning_rate": 0.00029814516129032255,
      "loss": 1.5738,
      "step": 23
    },
    {
      "epoch": 0.06442953020134229,
      "grad_norm": 0.09715720266103745,
      "learning_rate": 0.0002980645161290322,
      "loss": 1.4372,
      "step": 24
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 0.08167348057031631,
      "learning_rate": 0.0002979838709677419,
      "loss": 1.49,
      "step": 25
    },
    {
      "epoch": 0.0697986577181208,
      "grad_norm": 0.08505312353372574,
      "learning_rate": 0.0002979032258064516,
      "loss": 1.532,
      "step": 26
    },
    {
      "epoch": 0.07248322147651007,
      "grad_norm": 0.09409274160861969,
      "learning_rate": 0.0002978225806451613,
      "loss": 1.551,
      "step": 27
    },
    {
      "epoch": 0.07516778523489932,
      "grad_norm": 0.10035400092601776,
      "learning_rate": 0.00029774193548387094,
      "loss": 1.5514,
      "step": 28
    },
    {
      "epoch": 0.07785234899328859,
      "grad_norm": 0.1096728965640068,
      "learning_rate": 0.0002976612903225806,
      "loss": 1.5829,
      "step": 29
    },
    {
      "epoch": 0.08053691275167785,
      "grad_norm": 0.08074664324522018,
      "learning_rate": 0.0002975806451612903,
      "loss": 1.6423,
      "step": 30
    },
    {
      "epoch": 0.08322147651006712,
      "grad_norm": 0.08147025853395462,
      "learning_rate": 0.00029749999999999997,
      "loss": 1.5503,
      "step": 31
    },
    {
      "epoch": 0.08590604026845637,
      "grad_norm": 0.10276640951633453,
      "learning_rate": 0.0002974193548387097,
      "loss": 1.7063,
      "step": 32
    },
    {
      "epoch": 0.08859060402684564,
      "grad_norm": 0.09082796424627304,
      "learning_rate": 0.00029733870967741933,
      "loss": 1.5178,
      "step": 33
    },
    {
      "epoch": 0.0912751677852349,
      "grad_norm": 0.09031905233860016,
      "learning_rate": 0.000297258064516129,
      "loss": 1.5074,
      "step": 34
    },
    {
      "epoch": 0.09395973154362416,
      "grad_norm": 0.1095111295580864,
      "learning_rate": 0.0002971774193548387,
      "loss": 1.51,
      "step": 35
    },
    {
      "epoch": 0.09664429530201342,
      "grad_norm": 0.10180409997701645,
      "learning_rate": 0.00029709677419354836,
      "loss": 1.583,
      "step": 36
    },
    {
      "epoch": 0.09932885906040269,
      "grad_norm": 0.09432892501354218,
      "learning_rate": 0.00029701612903225807,
      "loss": 1.6353,
      "step": 37
    },
    {
      "epoch": 0.10201342281879194,
      "grad_norm": 0.1179272010922432,
      "learning_rate": 0.0002969354838709677,
      "loss": 1.4961,
      "step": 38
    },
    {
      "epoch": 0.10469798657718121,
      "grad_norm": 0.10532833635807037,
      "learning_rate": 0.0002968548387096774,
      "loss": 1.5139,
      "step": 39
    },
    {
      "epoch": 0.10738255033557047,
      "grad_norm": 0.10244033485651016,
      "learning_rate": 0.0002967741935483871,
      "loss": 1.5324,
      "step": 40
    },
    {
      "epoch": 0.11006711409395974,
      "grad_norm": 0.09362659603357315,
      "learning_rate": 0.00029669354838709675,
      "loss": 1.4227,
      "step": 41
    },
    {
      "epoch": 0.11275167785234899,
      "grad_norm": 0.09737493097782135,
      "learning_rate": 0.00029661290322580646,
      "loss": 1.3544,
      "step": 42
    },
    {
      "epoch": 0.11543624161073826,
      "grad_norm": 0.08502919971942902,
      "learning_rate": 0.0002965322580645161,
      "loss": 1.5577,
      "step": 43
    },
    {
      "epoch": 0.11812080536912752,
      "grad_norm": 0.09093403816223145,
      "learning_rate": 0.00029645161290322577,
      "loss": 1.4191,
      "step": 44
    },
    {
      "epoch": 0.12080536912751678,
      "grad_norm": 0.10236882418394089,
      "learning_rate": 0.0002963709677419355,
      "loss": 1.4066,
      "step": 45
    },
    {
      "epoch": 0.12348993288590604,
      "grad_norm": 0.08423979580402374,
      "learning_rate": 0.00029629032258064514,
      "loss": 1.5561,
      "step": 46
    },
    {
      "epoch": 0.1261744966442953,
      "grad_norm": 0.09223490208387375,
      "learning_rate": 0.0002962096774193548,
      "loss": 1.5368,
      "step": 47
    },
    {
      "epoch": 0.12885906040268458,
      "grad_norm": 0.09747636318206787,
      "learning_rate": 0.0002961290322580645,
      "loss": 1.4714,
      "step": 48
    },
    {
      "epoch": 0.13154362416107382,
      "grad_norm": 0.08963077515363693,
      "learning_rate": 0.00029604838709677416,
      "loss": 1.4816,
      "step": 49
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 0.08846667408943176,
      "learning_rate": 0.00029596774193548387,
      "loss": 1.53,
      "step": 50
    },
    {
      "epoch": 0.13691275167785236,
      "grad_norm": 0.08539048582315445,
      "learning_rate": 0.0002958870967741935,
      "loss": 1.4609,
      "step": 51
    },
    {
      "epoch": 0.1395973154362416,
      "grad_norm": 0.09680970013141632,
      "learning_rate": 0.0002958064516129032,
      "loss": 1.4289,
      "step": 52
    },
    {
      "epoch": 0.14228187919463087,
      "grad_norm": 0.08628497272729874,
      "learning_rate": 0.0002957258064516129,
      "loss": 1.5471,
      "step": 53
    },
    {
      "epoch": 0.14496644295302014,
      "grad_norm": 0.1024695560336113,
      "learning_rate": 0.00029564516129032255,
      "loss": 1.6629,
      "step": 54
    },
    {
      "epoch": 0.1476510067114094,
      "grad_norm": 0.1072537750005722,
      "learning_rate": 0.00029556451612903226,
      "loss": 1.5112,
      "step": 55
    },
    {
      "epoch": 0.15033557046979865,
      "grad_norm": 0.07863384485244751,
      "learning_rate": 0.0002954838709677419,
      "loss": 1.4567,
      "step": 56
    },
    {
      "epoch": 0.15302013422818792,
      "grad_norm": 0.09761989861726761,
      "learning_rate": 0.00029540322580645157,
      "loss": 1.5025,
      "step": 57
    },
    {
      "epoch": 0.15570469798657718,
      "grad_norm": 0.0858558639883995,
      "learning_rate": 0.0002953225806451613,
      "loss": 1.5073,
      "step": 58
    },
    {
      "epoch": 0.15838926174496645,
      "grad_norm": 0.10879310965538025,
      "learning_rate": 0.00029524193548387094,
      "loss": 1.5226,
      "step": 59
    },
    {
      "epoch": 0.1610738255033557,
      "grad_norm": 0.09171314537525177,
      "learning_rate": 0.00029516129032258065,
      "loss": 1.6281,
      "step": 60
    },
    {
      "epoch": 0.16375838926174496,
      "grad_norm": 0.08161914348602295,
      "learning_rate": 0.0002950806451612903,
      "loss": 1.5452,
      "step": 61
    },
    {
      "epoch": 0.16644295302013423,
      "grad_norm": 0.08703446388244629,
      "learning_rate": 0.00029499999999999996,
      "loss": 1.5192,
      "step": 62
    },
    {
      "epoch": 0.1691275167785235,
      "grad_norm": 0.08118464797735214,
      "learning_rate": 0.0002949193548387096,
      "loss": 1.6413,
      "step": 63
    },
    {
      "epoch": 0.17181208053691274,
      "grad_norm": 0.0935589000582695,
      "learning_rate": 0.00029483870967741933,
      "loss": 1.468,
      "step": 64
    },
    {
      "epoch": 0.174496644295302,
      "grad_norm": 0.09690678864717484,
      "learning_rate": 0.00029475806451612904,
      "loss": 1.4635,
      "step": 65
    },
    {
      "epoch": 0.17718120805369128,
      "grad_norm": 0.09814620018005371,
      "learning_rate": 0.0002946774193548387,
      "loss": 1.4759,
      "step": 66
    },
    {
      "epoch": 0.17986577181208055,
      "grad_norm": 0.1002948060631752,
      "learning_rate": 0.00029459677419354835,
      "loss": 1.6027,
      "step": 67
    },
    {
      "epoch": 0.1825503355704698,
      "grad_norm": 0.08883915096521378,
      "learning_rate": 0.000294516129032258,
      "loss": 1.4938,
      "step": 68
    },
    {
      "epoch": 0.18523489932885906,
      "grad_norm": 0.08855848759412766,
      "learning_rate": 0.0002944354838709677,
      "loss": 1.5757,
      "step": 69
    },
    {
      "epoch": 0.18791946308724833,
      "grad_norm": 0.09628409892320633,
      "learning_rate": 0.0002943548387096774,
      "loss": 1.4002,
      "step": 70
    },
    {
      "epoch": 0.1906040268456376,
      "grad_norm": 0.09597039967775345,
      "learning_rate": 0.0002942741935483871,
      "loss": 1.613,
      "step": 71
    },
    {
      "epoch": 0.19328859060402684,
      "grad_norm": 0.09791480749845505,
      "learning_rate": 0.00029419354838709674,
      "loss": 1.4991,
      "step": 72
    },
    {
      "epoch": 0.1959731543624161,
      "grad_norm": 0.07937482744455338,
      "learning_rate": 0.0002941129032258064,
      "loss": 1.5757,
      "step": 73
    },
    {
      "epoch": 0.19865771812080538,
      "grad_norm": 0.07216282933950424,
      "learning_rate": 0.0002940322580645161,
      "loss": 1.5192,
      "step": 74
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 0.0955415740609169,
      "learning_rate": 0.00029395161290322576,
      "loss": 1.566,
      "step": 75
    },
    {
      "epoch": 0.2040268456375839,
      "grad_norm": 0.10099492967128754,
      "learning_rate": 0.0002938709677419355,
      "loss": 1.522,
      "step": 76
    },
    {
      "epoch": 0.20671140939597316,
      "grad_norm": 0.12348904460668564,
      "learning_rate": 0.00029379032258064513,
      "loss": 1.5231,
      "step": 77
    },
    {
      "epoch": 0.20939597315436242,
      "grad_norm": 0.08789882808923721,
      "learning_rate": 0.0002937096774193548,
      "loss": 1.5324,
      "step": 78
    },
    {
      "epoch": 0.21208053691275167,
      "grad_norm": 0.08712241053581238,
      "learning_rate": 0.0002936290322580645,
      "loss": 1.4601,
      "step": 79
    },
    {
      "epoch": 0.21476510067114093,
      "grad_norm": 0.08374416828155518,
      "learning_rate": 0.00029354838709677415,
      "loss": 1.5201,
      "step": 80
    },
    {
      "epoch": 0.2174496644295302,
      "grad_norm": 0.08675125986337662,
      "learning_rate": 0.00029346774193548386,
      "loss": 1.4888,
      "step": 81
    },
    {
      "epoch": 0.22013422818791947,
      "grad_norm": 0.10974252223968506,
      "learning_rate": 0.0002933870967741935,
      "loss": 1.587,
      "step": 82
    },
    {
      "epoch": 0.22281879194630871,
      "grad_norm": 0.08687406778335571,
      "learning_rate": 0.0002933064516129032,
      "loss": 1.6252,
      "step": 83
    },
    {
      "epoch": 0.22550335570469798,
      "grad_norm": 0.09957846254110336,
      "learning_rate": 0.0002932258064516129,
      "loss": 1.3848,
      "step": 84
    },
    {
      "epoch": 0.22818791946308725,
      "grad_norm": 0.08263196796178818,
      "learning_rate": 0.00029314516129032254,
      "loss": 1.5534,
      "step": 85
    },
    {
      "epoch": 0.23087248322147652,
      "grad_norm": 0.09560411423444748,
      "learning_rate": 0.00029306451612903225,
      "loss": 1.509,
      "step": 86
    },
    {
      "epoch": 0.23355704697986576,
      "grad_norm": 0.08937713503837585,
      "learning_rate": 0.0002929838709677419,
      "loss": 1.6386,
      "step": 87
    },
    {
      "epoch": 0.23624161073825503,
      "grad_norm": 0.08324557542800903,
      "learning_rate": 0.00029290322580645156,
      "loss": 1.5755,
      "step": 88
    },
    {
      "epoch": 0.2389261744966443,
      "grad_norm": 0.09888904541730881,
      "learning_rate": 0.0002928225806451613,
      "loss": 1.6565,
      "step": 89
    },
    {
      "epoch": 0.24161073825503357,
      "grad_norm": 0.08642084896564484,
      "learning_rate": 0.00029274193548387093,
      "loss": 1.4816,
      "step": 90
    },
    {
      "epoch": 0.2442953020134228,
      "grad_norm": 0.08429338783025742,
      "learning_rate": 0.00029266129032258064,
      "loss": 1.5546,
      "step": 91
    },
    {
      "epoch": 0.24697986577181208,
      "grad_norm": 0.09113439172506332,
      "learning_rate": 0.0002925806451612903,
      "loss": 1.53,
      "step": 92
    },
    {
      "epoch": 0.24966442953020135,
      "grad_norm": 0.08571590483188629,
      "learning_rate": 0.00029249999999999995,
      "loss": 1.5739,
      "step": 93
    },
    {
      "epoch": 0.2523489932885906,
      "grad_norm": 0.08819155395030975,
      "learning_rate": 0.00029241935483870966,
      "loss": 1.5547,
      "step": 94
    },
    {
      "epoch": 0.2550335570469799,
      "grad_norm": 0.0941157191991806,
      "learning_rate": 0.0002923387096774193,
      "loss": 1.3873,
      "step": 95
    },
    {
      "epoch": 0.25771812080536916,
      "grad_norm": 0.08902405947446823,
      "learning_rate": 0.00029225806451612903,
      "loss": 1.571,
      "step": 96
    },
    {
      "epoch": 0.26040268456375837,
      "grad_norm": 0.09964869916439056,
      "learning_rate": 0.0002921774193548387,
      "loss": 1.4733,
      "step": 97
    },
    {
      "epoch": 0.26308724832214764,
      "grad_norm": 0.08325023949146271,
      "learning_rate": 0.00029209677419354834,
      "loss": 1.5234,
      "step": 98
    },
    {
      "epoch": 0.2657718120805369,
      "grad_norm": 0.08743739873170853,
      "learning_rate": 0.00029201612903225805,
      "loss": 1.6005,
      "step": 99
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 0.0790792927145958,
      "learning_rate": 0.0002919354838709677,
      "loss": 1.4529,
      "step": 100
    },
    {
      "epoch": 0.27114093959731544,
      "grad_norm": 0.09270064532756805,
      "learning_rate": 0.0002918548387096774,
      "loss": 1.5137,
      "step": 101
    },
    {
      "epoch": 0.2738255033557047,
      "grad_norm": 0.084723100066185,
      "learning_rate": 0.0002917741935483871,
      "loss": 1.4565,
      "step": 102
    },
    {
      "epoch": 0.276510067114094,
      "grad_norm": 0.10260818153619766,
      "learning_rate": 0.00029169354838709673,
      "loss": 1.4879,
      "step": 103
    },
    {
      "epoch": 0.2791946308724832,
      "grad_norm": 0.08381914347410202,
      "learning_rate": 0.00029161290322580644,
      "loss": 1.4304,
      "step": 104
    },
    {
      "epoch": 0.28187919463087246,
      "grad_norm": 0.10984048992395401,
      "learning_rate": 0.0002915322580645161,
      "loss": 1.5898,
      "step": 105
    },
    {
      "epoch": 0.28456375838926173,
      "grad_norm": 0.09535829722881317,
      "learning_rate": 0.0002914516129032258,
      "loss": 1.5889,
      "step": 106
    },
    {
      "epoch": 0.287248322147651,
      "grad_norm": 0.09083189815282822,
      "learning_rate": 0.00029137096774193547,
      "loss": 1.3888,
      "step": 107
    },
    {
      "epoch": 0.28993288590604027,
      "grad_norm": 0.08299960196018219,
      "learning_rate": 0.0002912903225806451,
      "loss": 1.5401,
      "step": 108
    },
    {
      "epoch": 0.29261744966442954,
      "grad_norm": 0.09686686843633652,
      "learning_rate": 0.00029120967741935483,
      "loss": 1.4574,
      "step": 109
    },
    {
      "epoch": 0.2953020134228188,
      "grad_norm": 0.087775319814682,
      "learning_rate": 0.0002911290322580645,
      "loss": 1.5629,
      "step": 110
    },
    {
      "epoch": 0.2979865771812081,
      "grad_norm": 0.10649145394563675,
      "learning_rate": 0.0002910483870967742,
      "loss": 1.4795,
      "step": 111
    },
    {
      "epoch": 0.3006711409395973,
      "grad_norm": 0.09702689200639725,
      "learning_rate": 0.00029096774193548386,
      "loss": 1.4675,
      "step": 112
    },
    {
      "epoch": 0.30335570469798656,
      "grad_norm": 0.08776209503412247,
      "learning_rate": 0.0002908870967741935,
      "loss": 1.5181,
      "step": 113
    },
    {
      "epoch": 0.30604026845637583,
      "grad_norm": 0.09834353625774384,
      "learning_rate": 0.0002908064516129032,
      "loss": 1.5845,
      "step": 114
    },
    {
      "epoch": 0.3087248322147651,
      "grad_norm": 0.0969540923833847,
      "learning_rate": 0.0002907258064516129,
      "loss": 1.4918,
      "step": 115
    },
    {
      "epoch": 0.31140939597315437,
      "grad_norm": 0.08686940371990204,
      "learning_rate": 0.0002906451612903226,
      "loss": 1.4707,
      "step": 116
    },
    {
      "epoch": 0.31409395973154364,
      "grad_norm": 0.09164819121360779,
      "learning_rate": 0.00029056451612903225,
      "loss": 1.5122,
      "step": 117
    },
    {
      "epoch": 0.3167785234899329,
      "grad_norm": 0.08842843770980835,
      "learning_rate": 0.0002904838709677419,
      "loss": 1.4825,
      "step": 118
    },
    {
      "epoch": 0.3194630872483222,
      "grad_norm": 0.08705668896436691,
      "learning_rate": 0.0002904032258064516,
      "loss": 1.5962,
      "step": 119
    },
    {
      "epoch": 0.3221476510067114,
      "grad_norm": 0.08279246836900711,
      "learning_rate": 0.00029032258064516127,
      "loss": 1.5617,
      "step": 120
    },
    {
      "epoch": 0.32483221476510066,
      "grad_norm": 0.0894027054309845,
      "learning_rate": 0.000290241935483871,
      "loss": 1.534,
      "step": 121
    },
    {
      "epoch": 0.3275167785234899,
      "grad_norm": 0.09586476534605026,
      "learning_rate": 0.00029016129032258064,
      "loss": 1.5319,
      "step": 122
    },
    {
      "epoch": 0.3302013422818792,
      "grad_norm": 0.09736008197069168,
      "learning_rate": 0.0002900806451612903,
      "loss": 1.4634,
      "step": 123
    },
    {
      "epoch": 0.33288590604026846,
      "grad_norm": 0.0876336470246315,
      "learning_rate": 0.00029,
      "loss": 1.4592,
      "step": 124
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 0.08475355058908463,
      "learning_rate": 0.00028991935483870966,
      "loss": 1.5579,
      "step": 125
    },
    {
      "epoch": 0.338255033557047,
      "grad_norm": 0.09190596640110016,
      "learning_rate": 0.00028983870967741937,
      "loss": 1.4528,
      "step": 126
    },
    {
      "epoch": 0.3409395973154362,
      "grad_norm": 0.09146800637245178,
      "learning_rate": 0.00028975806451612897,
      "loss": 1.5605,
      "step": 127
    },
    {
      "epoch": 0.3436241610738255,
      "grad_norm": 0.08956688642501831,
      "learning_rate": 0.0002896774193548387,
      "loss": 1.5726,
      "step": 128
    },
    {
      "epoch": 0.34630872483221475,
      "grad_norm": 0.08943502604961395,
      "learning_rate": 0.0002895967741935484,
      "loss": 1.469,
      "step": 129
    },
    {
      "epoch": 0.348993288590604,
      "grad_norm": 0.086200051009655,
      "learning_rate": 0.00028951612903225805,
      "loss": 1.6464,
      "step": 130
    },
    {
      "epoch": 0.3516778523489933,
      "grad_norm": 0.09913887828588486,
      "learning_rate": 0.00028943548387096776,
      "loss": 1.6242,
      "step": 131
    },
    {
      "epoch": 0.35436241610738256,
      "grad_norm": 0.08538252115249634,
      "learning_rate": 0.00028935483870967736,
      "loss": 1.6311,
      "step": 132
    },
    {
      "epoch": 0.35704697986577183,
      "grad_norm": 0.10217378288507462,
      "learning_rate": 0.00028927419354838707,
      "loss": 1.518,
      "step": 133
    },
    {
      "epoch": 0.3597315436241611,
      "grad_norm": 0.09871487319469452,
      "learning_rate": 0.0002891935483870967,
      "loss": 1.4668,
      "step": 134
    },
    {
      "epoch": 0.3624161073825503,
      "grad_norm": 0.09019996225833893,
      "learning_rate": 0.00028911290322580644,
      "loss": 1.4975,
      "step": 135
    },
    {
      "epoch": 0.3651006711409396,
      "grad_norm": 0.08303547650575638,
      "learning_rate": 0.0002890322580645161,
      "loss": 1.5832,
      "step": 136
    },
    {
      "epoch": 0.36778523489932885,
      "grad_norm": 0.09780310839414597,
      "learning_rate": 0.00028895161290322575,
      "loss": 1.525,
      "step": 137
    },
    {
      "epoch": 0.3704697986577181,
      "grad_norm": 0.07638603448867798,
      "learning_rate": 0.00028887096774193546,
      "loss": 1.5962,
      "step": 138
    },
    {
      "epoch": 0.3731543624161074,
      "grad_norm": 0.08250907063484192,
      "learning_rate": 0.0002887903225806451,
      "loss": 1.5633,
      "step": 139
    },
    {
      "epoch": 0.37583892617449666,
      "grad_norm": 0.08873105049133301,
      "learning_rate": 0.0002887096774193548,
      "loss": 1.5729,
      "step": 140
    },
    {
      "epoch": 0.3785234899328859,
      "grad_norm": 0.09515102952718735,
      "learning_rate": 0.0002886290322580645,
      "loss": 1.4401,
      "step": 141
    },
    {
      "epoch": 0.3812080536912752,
      "grad_norm": 0.10675616562366486,
      "learning_rate": 0.00028854838709677414,
      "loss": 1.4399,
      "step": 142
    },
    {
      "epoch": 0.3838926174496644,
      "grad_norm": 0.07888739556074142,
      "learning_rate": 0.00028846774193548385,
      "loss": 1.5377,
      "step": 143
    },
    {
      "epoch": 0.3865771812080537,
      "grad_norm": 0.10188248008489609,
      "learning_rate": 0.0002883870967741935,
      "loss": 1.5346,
      "step": 144
    },
    {
      "epoch": 0.38926174496644295,
      "grad_norm": 0.07744655758142471,
      "learning_rate": 0.0002883064516129032,
      "loss": 1.5148,
      "step": 145
    },
    {
      "epoch": 0.3919463087248322,
      "grad_norm": 0.08117124438285828,
      "learning_rate": 0.00028822580645161287,
      "loss": 1.5118,
      "step": 146
    },
    {
      "epoch": 0.3946308724832215,
      "grad_norm": 0.12538988888263702,
      "learning_rate": 0.00028814516129032253,
      "loss": 1.3606,
      "step": 147
    },
    {
      "epoch": 0.39731543624161075,
      "grad_norm": 0.10077260434627533,
      "learning_rate": 0.00028806451612903224,
      "loss": 1.4725,
      "step": 148
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.08658582717180252,
      "learning_rate": 0.0002879838709677419,
      "loss": 1.492,
      "step": 149
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 0.10030975937843323,
      "learning_rate": 0.0002879032258064516,
      "loss": 1.5496,
      "step": 150
    },
    {
      "epoch": 0.4053691275167785,
      "grad_norm": 0.0929160788655281,
      "learning_rate": 0.00028782258064516126,
      "loss": 1.4495,
      "step": 151
    },
    {
      "epoch": 0.4080536912751678,
      "grad_norm": 0.09158740937709808,
      "learning_rate": 0.0002877419354838709,
      "loss": 1.5618,
      "step": 152
    },
    {
      "epoch": 0.41073825503355704,
      "grad_norm": 0.09411980956792831,
      "learning_rate": 0.00028766129032258063,
      "loss": 1.5136,
      "step": 153
    },
    {
      "epoch": 0.4134228187919463,
      "grad_norm": 0.08850657194852829,
      "learning_rate": 0.0002875806451612903,
      "loss": 1.5381,
      "step": 154
    },
    {
      "epoch": 0.4161073825503356,
      "grad_norm": 0.08878663182258606,
      "learning_rate": 0.0002875,
      "loss": 1.596,
      "step": 155
    },
    {
      "epoch": 0.41879194630872485,
      "grad_norm": 0.09423625469207764,
      "learning_rate": 0.00028741935483870965,
      "loss": 1.4085,
      "step": 156
    },
    {
      "epoch": 0.4214765100671141,
      "grad_norm": 0.09013598412275314,
      "learning_rate": 0.0002873387096774193,
      "loss": 1.4768,
      "step": 157
    },
    {
      "epoch": 0.42416107382550333,
      "grad_norm": 0.07825768738985062,
      "learning_rate": 0.000287258064516129,
      "loss": 1.5214,
      "step": 158
    },
    {
      "epoch": 0.4268456375838926,
      "grad_norm": 0.0976124256849289,
      "learning_rate": 0.0002871774193548387,
      "loss": 1.4933,
      "step": 159
    },
    {
      "epoch": 0.42953020134228187,
      "grad_norm": 0.0956694483757019,
      "learning_rate": 0.0002870967741935484,
      "loss": 1.4404,
      "step": 160
    },
    {
      "epoch": 0.43221476510067114,
      "grad_norm": 0.08763837069272995,
      "learning_rate": 0.00028701612903225804,
      "loss": 1.4207,
      "step": 161
    },
    {
      "epoch": 0.4348993288590604,
      "grad_norm": 0.09374265372753143,
      "learning_rate": 0.0002869354838709677,
      "loss": 1.6444,
      "step": 162
    },
    {
      "epoch": 0.4375838926174497,
      "grad_norm": 0.08985880762338638,
      "learning_rate": 0.0002868548387096774,
      "loss": 1.5336,
      "step": 163
    },
    {
      "epoch": 0.44026845637583895,
      "grad_norm": 0.10773177444934845,
      "learning_rate": 0.00028677419354838706,
      "loss": 1.5295,
      "step": 164
    },
    {
      "epoch": 0.4429530201342282,
      "grad_norm": 0.08230676501989365,
      "learning_rate": 0.0002866935483870968,
      "loss": 1.6227,
      "step": 165
    },
    {
      "epoch": 0.44563758389261743,
      "grad_norm": 0.08903081715106964,
      "learning_rate": 0.00028661290322580643,
      "loss": 1.5341,
      "step": 166
    },
    {
      "epoch": 0.4483221476510067,
      "grad_norm": 0.09030477702617645,
      "learning_rate": 0.0002865322580645161,
      "loss": 1.5349,
      "step": 167
    },
    {
      "epoch": 0.45100671140939597,
      "grad_norm": 0.10406991839408875,
      "learning_rate": 0.0002864516129032258,
      "loss": 1.4646,
      "step": 168
    },
    {
      "epoch": 0.45369127516778524,
      "grad_norm": 0.10210450738668442,
      "learning_rate": 0.00028637096774193545,
      "loss": 1.4684,
      "step": 169
    },
    {
      "epoch": 0.4563758389261745,
      "grad_norm": 0.08101095259189606,
      "learning_rate": 0.00028629032258064516,
      "loss": 1.4877,
      "step": 170
    },
    {
      "epoch": 0.4590604026845638,
      "grad_norm": 0.0805128812789917,
      "learning_rate": 0.0002862096774193548,
      "loss": 1.6003,
      "step": 171
    },
    {
      "epoch": 0.46174496644295304,
      "grad_norm": 0.0929357036948204,
      "learning_rate": 0.0002861290322580645,
      "loss": 1.582,
      "step": 172
    },
    {
      "epoch": 0.46442953020134226,
      "grad_norm": 0.0865984857082367,
      "learning_rate": 0.0002860483870967742,
      "loss": 1.4869,
      "step": 173
    },
    {
      "epoch": 0.4671140939597315,
      "grad_norm": 0.07849575579166412,
      "learning_rate": 0.00028596774193548384,
      "loss": 1.5881,
      "step": 174
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 0.07971488684415817,
      "learning_rate": 0.00028588709677419355,
      "loss": 1.6562,
      "step": 175
    },
    {
      "epoch": 0.47248322147651006,
      "grad_norm": 0.1022072285413742,
      "learning_rate": 0.0002858064516129032,
      "loss": 1.3815,
      "step": 176
    },
    {
      "epoch": 0.47516778523489933,
      "grad_norm": 0.0854041650891304,
      "learning_rate": 0.00028572580645161287,
      "loss": 1.4991,
      "step": 177
    },
    {
      "epoch": 0.4778523489932886,
      "grad_norm": 0.09787251055240631,
      "learning_rate": 0.0002856451612903226,
      "loss": 1.534,
      "step": 178
    },
    {
      "epoch": 0.48053691275167787,
      "grad_norm": 0.08083993196487427,
      "learning_rate": 0.00028556451612903223,
      "loss": 1.5552,
      "step": 179
    },
    {
      "epoch": 0.48322147651006714,
      "grad_norm": 0.09060528874397278,
      "learning_rate": 0.00028548387096774194,
      "loss": 1.539,
      "step": 180
    },
    {
      "epoch": 0.48590604026845635,
      "grad_norm": 0.0987260714173317,
      "learning_rate": 0.0002854032258064516,
      "loss": 1.4738,
      "step": 181
    },
    {
      "epoch": 0.4885906040268456,
      "grad_norm": 0.0929023027420044,
      "learning_rate": 0.00028532258064516126,
      "loss": 1.4435,
      "step": 182
    },
    {
      "epoch": 0.4912751677852349,
      "grad_norm": 0.10223402082920074,
      "learning_rate": 0.00028524193548387097,
      "loss": 1.508,
      "step": 183
    },
    {
      "epoch": 0.49395973154362416,
      "grad_norm": 0.08924339711666107,
      "learning_rate": 0.0002851612903225806,
      "loss": 1.5322,
      "step": 184
    },
    {
      "epoch": 0.4966442953020134,
      "grad_norm": 0.09501748532056808,
      "learning_rate": 0.00028508064516129033,
      "loss": 1.5076,
      "step": 185
    },
    {
      "epoch": 0.4993288590604027,
      "grad_norm": 0.08649859577417374,
      "learning_rate": 0.000285,
      "loss": 1.6087,
      "step": 186
    },
    {
      "epoch": 0.5020134228187919,
      "grad_norm": 0.09322786331176758,
      "learning_rate": 0.00028491935483870964,
      "loss": 1.4514,
      "step": 187
    },
    {
      "epoch": 0.5046979865771812,
      "grad_norm": 0.10902290046215057,
      "learning_rate": 0.00028483870967741936,
      "loss": 1.5119,
      "step": 188
    },
    {
      "epoch": 0.5073825503355704,
      "grad_norm": 0.08020865172147751,
      "learning_rate": 0.000284758064516129,
      "loss": 1.5392,
      "step": 189
    },
    {
      "epoch": 0.5100671140939598,
      "grad_norm": 0.08804459124803543,
      "learning_rate": 0.0002846774193548387,
      "loss": 1.5308,
      "step": 190
    },
    {
      "epoch": 0.512751677852349,
      "grad_norm": 0.11032235622406006,
      "learning_rate": 0.0002845967741935483,
      "loss": 1.5455,
      "step": 191
    },
    {
      "epoch": 0.5154362416107383,
      "grad_norm": 0.11950933188199997,
      "learning_rate": 0.00028451612903225803,
      "loss": 1.3742,
      "step": 192
    },
    {
      "epoch": 0.5181208053691275,
      "grad_norm": 0.09128417074680328,
      "learning_rate": 0.0002844354838709677,
      "loss": 1.4602,
      "step": 193
    },
    {
      "epoch": 0.5208053691275167,
      "grad_norm": 0.09178005158901215,
      "learning_rate": 0.0002843548387096774,
      "loss": 1.5147,
      "step": 194
    },
    {
      "epoch": 0.5234899328859061,
      "grad_norm": 0.1010177955031395,
      "learning_rate": 0.0002842741935483871,
      "loss": 1.4526,
      "step": 195
    },
    {
      "epoch": 0.5261744966442953,
      "grad_norm": 0.08932501077651978,
      "learning_rate": 0.0002841935483870967,
      "loss": 1.5624,
      "step": 196
    },
    {
      "epoch": 0.5288590604026846,
      "grad_norm": 0.09279706329107285,
      "learning_rate": 0.0002841129032258064,
      "loss": 1.4316,
      "step": 197
    },
    {
      "epoch": 0.5315436241610738,
      "grad_norm": 0.09316860884428024,
      "learning_rate": 0.0002840322580645161,
      "loss": 1.4692,
      "step": 198
    },
    {
      "epoch": 0.5342281879194631,
      "grad_norm": 0.10630147904157639,
      "learning_rate": 0.0002839516129032258,
      "loss": 1.5816,
      "step": 199
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 0.08509702980518341,
      "learning_rate": 0.00028387096774193545,
      "loss": 1.5588,
      "step": 200
    },
    {
      "epoch": 0.5395973154362416,
      "grad_norm": 0.08644192665815353,
      "learning_rate": 0.0002837903225806451,
      "loss": 1.5003,
      "step": 201
    },
    {
      "epoch": 0.5422818791946309,
      "grad_norm": 0.09489339590072632,
      "learning_rate": 0.0002837096774193548,
      "loss": 1.369,
      "step": 202
    },
    {
      "epoch": 0.5449664429530201,
      "grad_norm": 0.08948791772127151,
      "learning_rate": 0.00028362903225806447,
      "loss": 1.4952,
      "step": 203
    },
    {
      "epoch": 0.5476510067114094,
      "grad_norm": 0.09636342525482178,
      "learning_rate": 0.0002835483870967742,
      "loss": 1.482,
      "step": 204
    },
    {
      "epoch": 0.5503355704697986,
      "grad_norm": 0.08015603572130203,
      "learning_rate": 0.00028346774193548384,
      "loss": 1.5419,
      "step": 205
    },
    {
      "epoch": 0.553020134228188,
      "grad_norm": 0.09669773280620575,
      "learning_rate": 0.0002833870967741935,
      "loss": 1.458,
      "step": 206
    },
    {
      "epoch": 0.5557046979865772,
      "grad_norm": 0.07747410237789154,
      "learning_rate": 0.0002833064516129032,
      "loss": 1.5335,
      "step": 207
    },
    {
      "epoch": 0.5583892617449664,
      "grad_norm": 0.10010784864425659,
      "learning_rate": 0.00028322580645161286,
      "loss": 1.5246,
      "step": 208
    },
    {
      "epoch": 0.5610738255033557,
      "grad_norm": 0.09344718605279922,
      "learning_rate": 0.00028314516129032257,
      "loss": 1.592,
      "step": 209
    },
    {
      "epoch": 0.5637583892617449,
      "grad_norm": 0.08629239350557327,
      "learning_rate": 0.0002830645161290322,
      "loss": 1.5374,
      "step": 210
    },
    {
      "epoch": 0.5664429530201343,
      "grad_norm": 0.0869876965880394,
      "learning_rate": 0.0002829838709677419,
      "loss": 1.5714,
      "step": 211
    },
    {
      "epoch": 0.5691275167785235,
      "grad_norm": 0.08429379016160965,
      "learning_rate": 0.0002829032258064516,
      "loss": 1.4994,
      "step": 212
    },
    {
      "epoch": 0.5718120805369128,
      "grad_norm": 0.08673202991485596,
      "learning_rate": 0.00028282258064516125,
      "loss": 1.5344,
      "step": 213
    },
    {
      "epoch": 0.574496644295302,
      "grad_norm": 0.09604047983884811,
      "learning_rate": 0.00028274193548387096,
      "loss": 1.514,
      "step": 214
    },
    {
      "epoch": 0.5771812080536913,
      "grad_norm": 0.08462756872177124,
      "learning_rate": 0.0002826612903225806,
      "loss": 1.5257,
      "step": 215
    },
    {
      "epoch": 0.5798657718120805,
      "grad_norm": 0.10587559640407562,
      "learning_rate": 0.00028258064516129027,
      "loss": 1.5182,
      "step": 216
    },
    {
      "epoch": 0.5825503355704698,
      "grad_norm": 0.08347217738628387,
      "learning_rate": 0.0002825,
      "loss": 1.5049,
      "step": 217
    },
    {
      "epoch": 0.5852348993288591,
      "grad_norm": 0.08292727917432785,
      "learning_rate": 0.00028241935483870964,
      "loss": 1.512,
      "step": 218
    },
    {
      "epoch": 0.5879194630872483,
      "grad_norm": 0.0956907570362091,
      "learning_rate": 0.00028233870967741935,
      "loss": 1.4492,
      "step": 219
    },
    {
      "epoch": 0.5906040268456376,
      "grad_norm": 0.0770203247666359,
      "learning_rate": 0.000282258064516129,
      "loss": 1.4697,
      "step": 220
    },
    {
      "epoch": 0.5932885906040268,
      "grad_norm": 0.08749311417341232,
      "learning_rate": 0.00028217741935483866,
      "loss": 1.5675,
      "step": 221
    },
    {
      "epoch": 0.5959731543624162,
      "grad_norm": 0.08295530825853348,
      "learning_rate": 0.00028209677419354837,
      "loss": 1.4687,
      "step": 222
    },
    {
      "epoch": 0.5986577181208054,
      "grad_norm": 0.0911007896065712,
      "learning_rate": 0.00028201612903225803,
      "loss": 1.436,
      "step": 223
    },
    {
      "epoch": 0.6013422818791946,
      "grad_norm": 0.10946038365364075,
      "learning_rate": 0.00028193548387096774,
      "loss": 1.4335,
      "step": 224
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 0.08918443322181702,
      "learning_rate": 0.0002818548387096774,
      "loss": 1.5688,
      "step": 225
    },
    {
      "epoch": 0.6067114093959731,
      "grad_norm": 0.09000124782323837,
      "learning_rate": 0.00028177419354838705,
      "loss": 1.496,
      "step": 226
    },
    {
      "epoch": 0.6093959731543624,
      "grad_norm": 0.10064871609210968,
      "learning_rate": 0.00028169354838709676,
      "loss": 1.4674,
      "step": 227
    },
    {
      "epoch": 0.6120805369127517,
      "grad_norm": 0.09350521862506866,
      "learning_rate": 0.0002816129032258064,
      "loss": 1.4898,
      "step": 228
    },
    {
      "epoch": 0.614765100671141,
      "grad_norm": 0.12193227559328079,
      "learning_rate": 0.00028153225806451613,
      "loss": 1.5621,
      "step": 229
    },
    {
      "epoch": 0.6174496644295302,
      "grad_norm": 0.10505539923906326,
      "learning_rate": 0.0002814516129032258,
      "loss": 1.5193,
      "step": 230
    },
    {
      "epoch": 0.6201342281879194,
      "grad_norm": 0.08329972624778748,
      "learning_rate": 0.00028137096774193544,
      "loss": 1.4838,
      "step": 231
    },
    {
      "epoch": 0.6228187919463087,
      "grad_norm": 0.08945256471633911,
      "learning_rate": 0.00028129032258064515,
      "loss": 1.5036,
      "step": 232
    },
    {
      "epoch": 0.625503355704698,
      "grad_norm": 0.08961595594882965,
      "learning_rate": 0.0002812096774193548,
      "loss": 1.5172,
      "step": 233
    },
    {
      "epoch": 0.6281879194630873,
      "grad_norm": 0.09381468594074249,
      "learning_rate": 0.0002811290322580645,
      "loss": 1.4537,
      "step": 234
    },
    {
      "epoch": 0.6308724832214765,
      "grad_norm": 0.1049695536494255,
      "learning_rate": 0.0002810483870967742,
      "loss": 1.3409,
      "step": 235
    },
    {
      "epoch": 0.6335570469798658,
      "grad_norm": 0.10759352892637253,
      "learning_rate": 0.00028096774193548383,
      "loss": 1.6068,
      "step": 236
    },
    {
      "epoch": 0.636241610738255,
      "grad_norm": 0.1000305637717247,
      "learning_rate": 0.00028088709677419354,
      "loss": 1.5901,
      "step": 237
    },
    {
      "epoch": 0.6389261744966444,
      "grad_norm": 0.10737358033657074,
      "learning_rate": 0.0002808064516129032,
      "loss": 1.4456,
      "step": 238
    },
    {
      "epoch": 0.6416107382550336,
      "grad_norm": 0.08573377877473831,
      "learning_rate": 0.0002807258064516129,
      "loss": 1.5231,
      "step": 239
    },
    {
      "epoch": 0.6442953020134228,
      "grad_norm": 0.09131212532520294,
      "learning_rate": 0.00028064516129032256,
      "loss": 1.5167,
      "step": 240
    },
    {
      "epoch": 0.6469798657718121,
      "grad_norm": 0.092092365026474,
      "learning_rate": 0.0002805645161290322,
      "loss": 1.5164,
      "step": 241
    },
    {
      "epoch": 0.6496644295302013,
      "grad_norm": 0.08638697117567062,
      "learning_rate": 0.00028048387096774193,
      "loss": 1.5071,
      "step": 242
    },
    {
      "epoch": 0.6523489932885906,
      "grad_norm": 0.09391458332538605,
      "learning_rate": 0.0002804032258064516,
      "loss": 1.5125,
      "step": 243
    },
    {
      "epoch": 0.6550335570469799,
      "grad_norm": 0.08593601733446121,
      "learning_rate": 0.0002803225806451613,
      "loss": 1.5093,
      "step": 244
    },
    {
      "epoch": 0.6577181208053692,
      "grad_norm": 0.12650765478610992,
      "learning_rate": 0.00028024193548387095,
      "loss": 1.4685,
      "step": 245
    },
    {
      "epoch": 0.6604026845637584,
      "grad_norm": 0.08671514689922333,
      "learning_rate": 0.0002801612903225806,
      "loss": 1.5455,
      "step": 246
    },
    {
      "epoch": 0.6630872483221476,
      "grad_norm": 0.08844470977783203,
      "learning_rate": 0.0002800806451612903,
      "loss": 1.5057,
      "step": 247
    },
    {
      "epoch": 0.6657718120805369,
      "grad_norm": 0.10138334333896637,
      "learning_rate": 0.00028,
      "loss": 1.5681,
      "step": 248
    },
    {
      "epoch": 0.6684563758389261,
      "grad_norm": 0.09083431214094162,
      "learning_rate": 0.0002799193548387097,
      "loss": 1.4411,
      "step": 249
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 0.09664376080036163,
      "learning_rate": 0.0002798387096774193,
      "loss": 1.598,
      "step": 250
    },
    {
      "epoch": 0.6738255033557047,
      "grad_norm": 0.09780740737915039,
      "learning_rate": 0.000279758064516129,
      "loss": 1.5661,
      "step": 251
    },
    {
      "epoch": 0.676510067114094,
      "grad_norm": 0.08740967512130737,
      "learning_rate": 0.0002796774193548387,
      "loss": 1.574,
      "step": 252
    },
    {
      "epoch": 0.6791946308724832,
      "grad_norm": 0.08985444903373718,
      "learning_rate": 0.00027959677419354837,
      "loss": 1.441,
      "step": 253
    },
    {
      "epoch": 0.6818791946308724,
      "grad_norm": 0.09195587038993835,
      "learning_rate": 0.0002795161290322581,
      "loss": 1.4478,
      "step": 254
    },
    {
      "epoch": 0.6845637583892618,
      "grad_norm": 0.09175048023462296,
      "learning_rate": 0.0002794354838709677,
      "loss": 1.4552,
      "step": 255
    },
    {
      "epoch": 0.687248322147651,
      "grad_norm": 0.0833110436797142,
      "learning_rate": 0.0002793548387096774,
      "loss": 1.7583,
      "step": 256
    },
    {
      "epoch": 0.6899328859060403,
      "grad_norm": 0.08300696313381195,
      "learning_rate": 0.00027927419354838704,
      "loss": 1.4233,
      "step": 257
    },
    {
      "epoch": 0.6926174496644295,
      "grad_norm": 0.09553459286689758,
      "learning_rate": 0.00027919354838709675,
      "loss": 1.5639,
      "step": 258
    },
    {
      "epoch": 0.6953020134228188,
      "grad_norm": 0.08241695165634155,
      "learning_rate": 0.00027911290322580646,
      "loss": 1.5198,
      "step": 259
    },
    {
      "epoch": 0.697986577181208,
      "grad_norm": 0.09171895682811737,
      "learning_rate": 0.00027903225806451607,
      "loss": 1.6139,
      "step": 260
    },
    {
      "epoch": 0.7006711409395974,
      "grad_norm": 0.07652505487203598,
      "learning_rate": 0.0002789516129032258,
      "loss": 1.5699,
      "step": 261
    },
    {
      "epoch": 0.7033557046979866,
      "grad_norm": 0.08585059642791748,
      "learning_rate": 0.00027887096774193543,
      "loss": 1.637,
      "step": 262
    },
    {
      "epoch": 0.7060402684563758,
      "grad_norm": 0.09535997360944748,
      "learning_rate": 0.00027879032258064514,
      "loss": 1.4794,
      "step": 263
    },
    {
      "epoch": 0.7087248322147651,
      "grad_norm": 0.07802009582519531,
      "learning_rate": 0.0002787096774193548,
      "loss": 1.5224,
      "step": 264
    },
    {
      "epoch": 0.7114093959731543,
      "grad_norm": 0.1102946326136589,
      "learning_rate": 0.00027862903225806446,
      "loss": 1.4361,
      "step": 265
    },
    {
      "epoch": 0.7140939597315437,
      "grad_norm": 0.07799957692623138,
      "learning_rate": 0.00027854838709677417,
      "loss": 1.509,
      "step": 266
    },
    {
      "epoch": 0.7167785234899329,
      "grad_norm": 0.09784425050020218,
      "learning_rate": 0.0002784677419354838,
      "loss": 1.4926,
      "step": 267
    },
    {
      "epoch": 0.7194630872483222,
      "grad_norm": 0.0985407829284668,
      "learning_rate": 0.00027838709677419353,
      "loss": 1.6388,
      "step": 268
    },
    {
      "epoch": 0.7221476510067114,
      "grad_norm": 0.08048056811094284,
      "learning_rate": 0.0002783064516129032,
      "loss": 1.4545,
      "step": 269
    },
    {
      "epoch": 0.7248322147651006,
      "grad_norm": 0.09138192981481552,
      "learning_rate": 0.00027822580645161285,
      "loss": 1.3262,
      "step": 270
    },
    {
      "epoch": 0.72751677852349,
      "grad_norm": 0.08072664588689804,
      "learning_rate": 0.00027814516129032256,
      "loss": 1.5327,
      "step": 271
    },
    {
      "epoch": 0.7302013422818792,
      "grad_norm": 0.08233004808425903,
      "learning_rate": 0.0002780645161290322,
      "loss": 1.6308,
      "step": 272
    },
    {
      "epoch": 0.7328859060402685,
      "grad_norm": 0.10295577347278595,
      "learning_rate": 0.0002779838709677419,
      "loss": 1.3934,
      "step": 273
    },
    {
      "epoch": 0.7355704697986577,
      "grad_norm": 0.08474202454090118,
      "learning_rate": 0.0002779032258064516,
      "loss": 1.5634,
      "step": 274
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 0.08695262670516968,
      "learning_rate": 0.00027782258064516124,
      "loss": 1.4826,
      "step": 275
    },
    {
      "epoch": 0.7409395973154362,
      "grad_norm": 0.08978070318698883,
      "learning_rate": 0.00027774193548387095,
      "loss": 1.4805,
      "step": 276
    },
    {
      "epoch": 0.7436241610738255,
      "grad_norm": 0.09236134588718414,
      "learning_rate": 0.0002776612903225806,
      "loss": 1.4558,
      "step": 277
    },
    {
      "epoch": 0.7463087248322148,
      "grad_norm": 0.08904656767845154,
      "learning_rate": 0.0002775806451612903,
      "loss": 1.5415,
      "step": 278
    },
    {
      "epoch": 0.748993288590604,
      "grad_norm": 0.08491117507219315,
      "learning_rate": 0.00027749999999999997,
      "loss": 1.4479,
      "step": 279
    },
    {
      "epoch": 0.7516778523489933,
      "grad_norm": 0.07751195877790451,
      "learning_rate": 0.0002774193548387096,
      "loss": 1.6267,
      "step": 280
    },
    {
      "epoch": 0.7543624161073825,
      "grad_norm": 0.10433290153741837,
      "learning_rate": 0.00027733870967741934,
      "loss": 1.4381,
      "step": 281
    },
    {
      "epoch": 0.7570469798657719,
      "grad_norm": 0.08837215602397919,
      "learning_rate": 0.000277258064516129,
      "loss": 1.5064,
      "step": 282
    },
    {
      "epoch": 0.7597315436241611,
      "grad_norm": 0.07602883875370026,
      "learning_rate": 0.0002771774193548387,
      "loss": 1.4824,
      "step": 283
    },
    {
      "epoch": 0.7624161073825504,
      "grad_norm": 0.08045510202646255,
      "learning_rate": 0.00027709677419354836,
      "loss": 1.5864,
      "step": 284
    },
    {
      "epoch": 0.7651006711409396,
      "grad_norm": 0.10047757625579834,
      "learning_rate": 0.000277016129032258,
      "loss": 1.4851,
      "step": 285
    },
    {
      "epoch": 0.7677852348993288,
      "grad_norm": 0.09028919041156769,
      "learning_rate": 0.0002769354838709677,
      "loss": 1.5774,
      "step": 286
    },
    {
      "epoch": 0.7704697986577181,
      "grad_norm": 0.0888698399066925,
      "learning_rate": 0.0002768548387096774,
      "loss": 1.4633,
      "step": 287
    },
    {
      "epoch": 0.7731543624161074,
      "grad_norm": 0.08234205096960068,
      "learning_rate": 0.0002767741935483871,
      "loss": 1.5358,
      "step": 288
    },
    {
      "epoch": 0.7758389261744967,
      "grad_norm": 0.09201568365097046,
      "learning_rate": 0.00027669354838709675,
      "loss": 1.5595,
      "step": 289
    },
    {
      "epoch": 0.7785234899328859,
      "grad_norm": 0.08741433173418045,
      "learning_rate": 0.0002766129032258064,
      "loss": 1.4867,
      "step": 290
    },
    {
      "epoch": 0.7812080536912752,
      "grad_norm": 0.09567445516586304,
      "learning_rate": 0.0002765322580645161,
      "loss": 1.3941,
      "step": 291
    },
    {
      "epoch": 0.7838926174496644,
      "grad_norm": 0.11785757541656494,
      "learning_rate": 0.00027645161290322577,
      "loss": 1.5007,
      "step": 292
    },
    {
      "epoch": 0.7865771812080536,
      "grad_norm": 0.10671613365411758,
      "learning_rate": 0.0002763709677419355,
      "loss": 1.4416,
      "step": 293
    },
    {
      "epoch": 0.789261744966443,
      "grad_norm": 0.09216000884771347,
      "learning_rate": 0.00027629032258064514,
      "loss": 1.362,
      "step": 294
    },
    {
      "epoch": 0.7919463087248322,
      "grad_norm": 0.07932919263839722,
      "learning_rate": 0.0002762096774193548,
      "loss": 1.5804,
      "step": 295
    },
    {
      "epoch": 0.7946308724832215,
      "grad_norm": 0.08904877305030823,
      "learning_rate": 0.0002761290322580645,
      "loss": 1.4889,
      "step": 296
    },
    {
      "epoch": 0.7973154362416107,
      "grad_norm": 0.08783069998025894,
      "learning_rate": 0.00027604838709677416,
      "loss": 1.4797,
      "step": 297
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.08392615616321564,
      "learning_rate": 0.00027596774193548387,
      "loss": 1.4262,
      "step": 298
    },
    {
      "epoch": 0.8026845637583893,
      "grad_norm": 0.0745639055967331,
      "learning_rate": 0.0002758870967741935,
      "loss": 1.5292,
      "step": 299
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 0.09953822940587997,
      "learning_rate": 0.0002758064516129032,
      "loss": 1.4701,
      "step": 300
    },
    {
      "epoch": 0.8080536912751678,
      "grad_norm": 0.07724858075380325,
      "learning_rate": 0.0002757258064516129,
      "loss": 1.6305,
      "step": 301
    },
    {
      "epoch": 0.810738255033557,
      "grad_norm": 0.08216144889593124,
      "learning_rate": 0.00027564516129032255,
      "loss": 1.4953,
      "step": 302
    },
    {
      "epoch": 0.8134228187919463,
      "grad_norm": 0.08821827918291092,
      "learning_rate": 0.00027556451612903226,
      "loss": 1.5528,
      "step": 303
    },
    {
      "epoch": 0.8161073825503355,
      "grad_norm": 0.10677901655435562,
      "learning_rate": 0.0002754838709677419,
      "loss": 1.4575,
      "step": 304
    },
    {
      "epoch": 0.8187919463087249,
      "grad_norm": 0.08920221030712128,
      "learning_rate": 0.00027540322580645157,
      "loss": 1.421,
      "step": 305
    },
    {
      "epoch": 0.8214765100671141,
      "grad_norm": 0.08925128728151321,
      "learning_rate": 0.0002753225806451613,
      "loss": 1.6577,
      "step": 306
    },
    {
      "epoch": 0.8241610738255034,
      "grad_norm": 0.1058211624622345,
      "learning_rate": 0.00027524193548387094,
      "loss": 1.5401,
      "step": 307
    },
    {
      "epoch": 0.8268456375838926,
      "grad_norm": 0.09264281392097473,
      "learning_rate": 0.00027516129032258065,
      "loss": 1.5395,
      "step": 308
    },
    {
      "epoch": 0.8295302013422818,
      "grad_norm": 0.0991794690489769,
      "learning_rate": 0.0002750806451612903,
      "loss": 1.544,
      "step": 309
    },
    {
      "epoch": 0.8322147651006712,
      "grad_norm": 0.07660131901502609,
      "learning_rate": 0.00027499999999999996,
      "loss": 1.5944,
      "step": 310
    },
    {
      "epoch": 0.8348993288590604,
      "grad_norm": 0.09056171774864197,
      "learning_rate": 0.00027491935483870967,
      "loss": 1.4495,
      "step": 311
    },
    {
      "epoch": 0.8375838926174497,
      "grad_norm": 0.08449455350637436,
      "learning_rate": 0.00027483870967741933,
      "loss": 1.5139,
      "step": 312
    },
    {
      "epoch": 0.8402684563758389,
      "grad_norm": 0.08634129166603088,
      "learning_rate": 0.00027475806451612904,
      "loss": 1.57,
      "step": 313
    },
    {
      "epoch": 0.8429530201342282,
      "grad_norm": 0.08201010525226593,
      "learning_rate": 0.0002746774193548387,
      "loss": 1.5563,
      "step": 314
    },
    {
      "epoch": 0.8456375838926175,
      "grad_norm": 0.09803067147731781,
      "learning_rate": 0.00027459677419354835,
      "loss": 1.5,
      "step": 315
    },
    {
      "epoch": 0.8483221476510067,
      "grad_norm": 0.08780922740697861,
      "learning_rate": 0.00027451612903225806,
      "loss": 1.4915,
      "step": 316
    },
    {
      "epoch": 0.851006711409396,
      "grad_norm": 0.0815521702170372,
      "learning_rate": 0.0002744354838709677,
      "loss": 1.7661,
      "step": 317
    },
    {
      "epoch": 0.8536912751677852,
      "grad_norm": 0.08568288385868073,
      "learning_rate": 0.00027435483870967743,
      "loss": 1.4742,
      "step": 318
    },
    {
      "epoch": 0.8563758389261745,
      "grad_norm": 0.09370534121990204,
      "learning_rate": 0.0002742741935483871,
      "loss": 1.5207,
      "step": 319
    },
    {
      "epoch": 0.8590604026845637,
      "grad_norm": 0.0870642364025116,
      "learning_rate": 0.00027419354838709674,
      "loss": 1.5184,
      "step": 320
    },
    {
      "epoch": 0.8617449664429531,
      "grad_norm": 0.08652450144290924,
      "learning_rate": 0.0002741129032258064,
      "loss": 1.4753,
      "step": 321
    },
    {
      "epoch": 0.8644295302013423,
      "grad_norm": 0.08257633447647095,
      "learning_rate": 0.0002740322580645161,
      "loss": 1.5191,
      "step": 322
    },
    {
      "epoch": 0.8671140939597315,
      "grad_norm": 0.08058968186378479,
      "learning_rate": 0.0002739516129032258,
      "loss": 1.6154,
      "step": 323
    },
    {
      "epoch": 0.8697986577181208,
      "grad_norm": 0.09099645167589188,
      "learning_rate": 0.0002738709677419355,
      "loss": 1.4091,
      "step": 324
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 0.08776850253343582,
      "learning_rate": 0.00027379032258064513,
      "loss": 1.4685,
      "step": 325
    },
    {
      "epoch": 0.8751677852348994,
      "grad_norm": 0.09109494835138321,
      "learning_rate": 0.0002737096774193548,
      "loss": 1.5675,
      "step": 326
    },
    {
      "epoch": 0.8778523489932886,
      "grad_norm": 0.09288930147886276,
      "learning_rate": 0.0002736290322580645,
      "loss": 1.5719,
      "step": 327
    },
    {
      "epoch": 0.8805369127516779,
      "grad_norm": 0.09728993475437164,
      "learning_rate": 0.00027354838709677415,
      "loss": 1.4494,
      "step": 328
    },
    {
      "epoch": 0.8832214765100671,
      "grad_norm": 0.08220478892326355,
      "learning_rate": 0.0002734677419354838,
      "loss": 1.5712,
      "step": 329
    },
    {
      "epoch": 0.8859060402684564,
      "grad_norm": 0.09096299856901169,
      "learning_rate": 0.0002733870967741935,
      "loss": 1.4748,
      "step": 330
    },
    {
      "epoch": 0.8885906040268456,
      "grad_norm": 0.09984097629785538,
      "learning_rate": 0.0002733064516129032,
      "loss": 1.4097,
      "step": 331
    },
    {
      "epoch": 0.8912751677852349,
      "grad_norm": 0.07888457179069519,
      "learning_rate": 0.0002732258064516129,
      "loss": 1.4516,
      "step": 332
    },
    {
      "epoch": 0.8939597315436242,
      "grad_norm": 0.08242087811231613,
      "learning_rate": 0.00027314516129032254,
      "loss": 1.5683,
      "step": 333
    },
    {
      "epoch": 0.8966442953020134,
      "grad_norm": 0.09110995382070541,
      "learning_rate": 0.0002730645161290322,
      "loss": 1.5073,
      "step": 334
    },
    {
      "epoch": 0.8993288590604027,
      "grad_norm": 0.10085379332304001,
      "learning_rate": 0.0002729838709677419,
      "loss": 1.4711,
      "step": 335
    },
    {
      "epoch": 0.9020134228187919,
      "grad_norm": 0.08370004594326019,
      "learning_rate": 0.00027290322580645157,
      "loss": 1.5208,
      "step": 336
    },
    {
      "epoch": 0.9046979865771813,
      "grad_norm": 0.09433230012655258,
      "learning_rate": 0.0002728225806451613,
      "loss": 1.4697,
      "step": 337
    },
    {
      "epoch": 0.9073825503355705,
      "grad_norm": 0.08894544094800949,
      "learning_rate": 0.00027274193548387093,
      "loss": 1.6153,
      "step": 338
    },
    {
      "epoch": 0.9100671140939597,
      "grad_norm": 0.10217151045799255,
      "learning_rate": 0.0002726612903225806,
      "loss": 1.6169,
      "step": 339
    },
    {
      "epoch": 0.912751677852349,
      "grad_norm": 0.1206960380077362,
      "learning_rate": 0.0002725806451612903,
      "loss": 1.3602,
      "step": 340
    },
    {
      "epoch": 0.9154362416107382,
      "grad_norm": 0.08390479534864426,
      "learning_rate": 0.00027249999999999996,
      "loss": 1.6013,
      "step": 341
    },
    {
      "epoch": 0.9181208053691275,
      "grad_norm": 0.07467100024223328,
      "learning_rate": 0.00027241935483870967,
      "loss": 1.5676,
      "step": 342
    },
    {
      "epoch": 0.9208053691275168,
      "grad_norm": 0.08391605317592621,
      "learning_rate": 0.0002723387096774193,
      "loss": 1.6429,
      "step": 343
    },
    {
      "epoch": 0.9234899328859061,
      "grad_norm": 0.09821101278066635,
      "learning_rate": 0.000272258064516129,
      "loss": 1.4951,
      "step": 344
    },
    {
      "epoch": 0.9261744966442953,
      "grad_norm": 0.08497469127178192,
      "learning_rate": 0.0002721774193548387,
      "loss": 1.5785,
      "step": 345
    },
    {
      "epoch": 0.9288590604026845,
      "grad_norm": 0.08182255178689957,
      "learning_rate": 0.00027209677419354835,
      "loss": 1.5143,
      "step": 346
    },
    {
      "epoch": 0.9315436241610738,
      "grad_norm": 0.12390562891960144,
      "learning_rate": 0.00027201612903225806,
      "loss": 1.5239,
      "step": 347
    },
    {
      "epoch": 0.934228187919463,
      "grad_norm": 0.0947597548365593,
      "learning_rate": 0.0002719354838709677,
      "loss": 1.5304,
      "step": 348
    },
    {
      "epoch": 0.9369127516778524,
      "grad_norm": 0.07938230037689209,
      "learning_rate": 0.00027185483870967737,
      "loss": 1.6091,
      "step": 349
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 0.09855206310749054,
      "learning_rate": 0.0002717741935483871,
      "loss": 1.48,
      "step": 350
    },
    {
      "epoch": 0.9422818791946309,
      "grad_norm": 0.08844200521707535,
      "learning_rate": 0.00027169354838709673,
      "loss": 1.4401,
      "step": 351
    },
    {
      "epoch": 0.9449664429530201,
      "grad_norm": 0.08726613223552704,
      "learning_rate": 0.00027161290322580645,
      "loss": 1.5593,
      "step": 352
    },
    {
      "epoch": 0.9476510067114094,
      "grad_norm": 0.11343177407979965,
      "learning_rate": 0.0002715322580645161,
      "loss": 1.5238,
      "step": 353
    },
    {
      "epoch": 0.9503355704697987,
      "grad_norm": 0.08120466768741608,
      "learning_rate": 0.00027145161290322576,
      "loss": 1.6701,
      "step": 354
    },
    {
      "epoch": 0.9530201342281879,
      "grad_norm": 0.09825600683689117,
      "learning_rate": 0.00027137096774193547,
      "loss": 1.6159,
      "step": 355
    },
    {
      "epoch": 0.9557046979865772,
      "grad_norm": 0.08123867213726044,
      "learning_rate": 0.0002712903225806451,
      "loss": 1.5382,
      "step": 356
    },
    {
      "epoch": 0.9583892617449664,
      "grad_norm": 0.07553999125957489,
      "learning_rate": 0.00027120967741935483,
      "loss": 1.4625,
      "step": 357
    },
    {
      "epoch": 0.9610738255033557,
      "grad_norm": 0.07805100828409195,
      "learning_rate": 0.0002711290322580645,
      "loss": 1.5862,
      "step": 358
    },
    {
      "epoch": 0.963758389261745,
      "grad_norm": 0.07771339267492294,
      "learning_rate": 0.00027104838709677415,
      "loss": 1.485,
      "step": 359
    },
    {
      "epoch": 0.9664429530201343,
      "grad_norm": 0.09033700823783875,
      "learning_rate": 0.00027096774193548386,
      "loss": 1.4605,
      "step": 360
    },
    {
      "epoch": 0.9691275167785235,
      "grad_norm": 0.08746718615293503,
      "learning_rate": 0.0002708870967741935,
      "loss": 1.5603,
      "step": 361
    },
    {
      "epoch": 0.9718120805369127,
      "grad_norm": 0.08878222852945328,
      "learning_rate": 0.0002708064516129032,
      "loss": 1.4213,
      "step": 362
    },
    {
      "epoch": 0.974496644295302,
      "grad_norm": 0.08298494666814804,
      "learning_rate": 0.0002707258064516129,
      "loss": 1.4758,
      "step": 363
    },
    {
      "epoch": 0.9771812080536912,
      "grad_norm": 0.0796893835067749,
      "learning_rate": 0.00027064516129032254,
      "loss": 1.533,
      "step": 364
    },
    {
      "epoch": 0.9798657718120806,
      "grad_norm": 0.0968870222568512,
      "learning_rate": 0.00027056451612903225,
      "loss": 1.5097,
      "step": 365
    },
    {
      "epoch": 0.9825503355704698,
      "grad_norm": 0.08332584798336029,
      "learning_rate": 0.0002704838709677419,
      "loss": 1.4691,
      "step": 366
    },
    {
      "epoch": 0.9852348993288591,
      "grad_norm": 0.07588011026382446,
      "learning_rate": 0.0002704032258064516,
      "loss": 1.6467,
      "step": 367
    },
    {
      "epoch": 0.9879194630872483,
      "grad_norm": 0.08169185370206833,
      "learning_rate": 0.00027032258064516127,
      "loss": 1.6262,
      "step": 368
    },
    {
      "epoch": 0.9906040268456375,
      "grad_norm": 0.08697285503149033,
      "learning_rate": 0.0002702419354838709,
      "loss": 1.4026,
      "step": 369
    },
    {
      "epoch": 0.9932885906040269,
      "grad_norm": 0.09366818517446518,
      "learning_rate": 0.00027016129032258064,
      "loss": 1.48,
      "step": 370
    },
    {
      "epoch": 0.9959731543624161,
      "grad_norm": 0.08530243486166,
      "learning_rate": 0.0002700806451612903,
      "loss": 1.5645,
      "step": 371
    },
    {
      "epoch": 0.9986577181208054,
      "grad_norm": 0.08761792629957199,
      "learning_rate": 0.00027,
      "loss": 1.4867,
      "step": 372
    },
    {
      "epoch": 1.0013422818791946,
      "grad_norm": 0.10696519166231155,
      "learning_rate": 0.00026991935483870966,
      "loss": 1.481,
      "step": 373
    },
    {
      "epoch": 1.0040268456375838,
      "grad_norm": 0.09039492905139923,
      "learning_rate": 0.0002698387096774193,
      "loss": 1.4945,
      "step": 374
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 0.08686847239732742,
      "learning_rate": 0.000269758064516129,
      "loss": 1.5305,
      "step": 375
    },
    {
      "epoch": 1.0093959731543625,
      "grad_norm": 0.10045942664146423,
      "learning_rate": 0.0002696774193548387,
      "loss": 1.4171,
      "step": 376
    },
    {
      "epoch": 1.0120805369127517,
      "grad_norm": 0.0850197896361351,
      "learning_rate": 0.0002695967741935484,
      "loss": 1.4089,
      "step": 377
    },
    {
      "epoch": 1.014765100671141,
      "grad_norm": 0.08677424490451813,
      "learning_rate": 0.00026951612903225805,
      "loss": 1.4376,
      "step": 378
    },
    {
      "epoch": 1.01744966442953,
      "grad_norm": 0.08932216465473175,
      "learning_rate": 0.0002694354838709677,
      "loss": 1.5511,
      "step": 379
    },
    {
      "epoch": 1.0201342281879195,
      "grad_norm": 0.08465446531772614,
      "learning_rate": 0.0002693548387096774,
      "loss": 1.5286,
      "step": 380
    },
    {
      "epoch": 1.0228187919463088,
      "grad_norm": 0.08444210886955261,
      "learning_rate": 0.00026927419354838707,
      "loss": 1.5318,
      "step": 381
    },
    {
      "epoch": 1.025503355704698,
      "grad_norm": 0.08013264089822769,
      "learning_rate": 0.0002691935483870968,
      "loss": 1.4624,
      "step": 382
    },
    {
      "epoch": 1.0281879194630872,
      "grad_norm": 0.09296196699142456,
      "learning_rate": 0.00026911290322580644,
      "loss": 1.3983,
      "step": 383
    },
    {
      "epoch": 1.0308724832214766,
      "grad_norm": 0.09173594415187836,
      "learning_rate": 0.0002690322580645161,
      "loss": 1.4869,
      "step": 384
    },
    {
      "epoch": 1.0335570469798658,
      "grad_norm": 0.0935196503996849,
      "learning_rate": 0.00026895161290322575,
      "loss": 1.5189,
      "step": 385
    },
    {
      "epoch": 1.036241610738255,
      "grad_norm": 0.07863698899745941,
      "learning_rate": 0.00026887096774193546,
      "loss": 1.5412,
      "step": 386
    },
    {
      "epoch": 1.0389261744966443,
      "grad_norm": 0.09223775565624237,
      "learning_rate": 0.00026879032258064517,
      "loss": 1.3878,
      "step": 387
    },
    {
      "epoch": 1.0416107382550335,
      "grad_norm": 0.08431219309568405,
      "learning_rate": 0.00026870967741935483,
      "loss": 1.5763,
      "step": 388
    },
    {
      "epoch": 1.044295302013423,
      "grad_norm": 0.09243035316467285,
      "learning_rate": 0.0002686290322580645,
      "loss": 1.5085,
      "step": 389
    },
    {
      "epoch": 1.0469798657718121,
      "grad_norm": 0.08583987504243851,
      "learning_rate": 0.00026854838709677414,
      "loss": 1.4365,
      "step": 390
    },
    {
      "epoch": 1.0496644295302013,
      "grad_norm": 0.08656994253396988,
      "learning_rate": 0.00026846774193548385,
      "loss": 1.5025,
      "step": 391
    },
    {
      "epoch": 1.0523489932885906,
      "grad_norm": 0.09060908108949661,
      "learning_rate": 0.0002683870967741935,
      "loss": 1.5907,
      "step": 392
    },
    {
      "epoch": 1.0550335570469798,
      "grad_norm": 0.08600431680679321,
      "learning_rate": 0.0002683064516129032,
      "loss": 1.5203,
      "step": 393
    },
    {
      "epoch": 1.0577181208053692,
      "grad_norm": 0.08843109756708145,
      "learning_rate": 0.0002682258064516129,
      "loss": 1.4062,
      "step": 394
    },
    {
      "epoch": 1.0604026845637584,
      "grad_norm": 0.07706893980503082,
      "learning_rate": 0.00026814516129032253,
      "loss": 1.5093,
      "step": 395
    },
    {
      "epoch": 1.0630872483221476,
      "grad_norm": 0.10004575550556183,
      "learning_rate": 0.00026806451612903224,
      "loss": 1.4674,
      "step": 396
    },
    {
      "epoch": 1.0657718120805368,
      "grad_norm": 0.07701727747917175,
      "learning_rate": 0.0002679838709677419,
      "loss": 1.4893,
      "step": 397
    },
    {
      "epoch": 1.0684563758389263,
      "grad_norm": 0.09160396456718445,
      "learning_rate": 0.0002679032258064516,
      "loss": 1.4264,
      "step": 398
    },
    {
      "epoch": 1.0711409395973155,
      "grad_norm": 0.09718012809753418,
      "learning_rate": 0.00026782258064516126,
      "loss": 1.4616,
      "step": 399
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 0.08514988422393799,
      "learning_rate": 0.0002677419354838709,
      "loss": 1.4882,
      "step": 400
    },
    {
      "epoch": 1.076510067114094,
      "grad_norm": 0.08201737701892853,
      "learning_rate": 0.00026766129032258063,
      "loss": 1.4915,
      "step": 401
    },
    {
      "epoch": 1.0791946308724831,
      "grad_norm": 0.12157410383224487,
      "learning_rate": 0.0002675806451612903,
      "loss": 1.4499,
      "step": 402
    },
    {
      "epoch": 1.0818791946308726,
      "grad_norm": 0.0947425439953804,
      "learning_rate": 0.0002675,
      "loss": 1.4539,
      "step": 403
    },
    {
      "epoch": 1.0845637583892618,
      "grad_norm": 0.09072495996952057,
      "learning_rate": 0.00026741935483870965,
      "loss": 1.581,
      "step": 404
    },
    {
      "epoch": 1.087248322147651,
      "grad_norm": 0.08560173958539963,
      "learning_rate": 0.0002673387096774193,
      "loss": 1.4137,
      "step": 405
    },
    {
      "epoch": 1.0899328859060402,
      "grad_norm": 0.09870429337024689,
      "learning_rate": 0.000267258064516129,
      "loss": 1.4518,
      "step": 406
    },
    {
      "epoch": 1.0926174496644294,
      "grad_norm": 0.08758934587240219,
      "learning_rate": 0.0002671774193548387,
      "loss": 1.5904,
      "step": 407
    },
    {
      "epoch": 1.0953020134228189,
      "grad_norm": 0.08959119766950607,
      "learning_rate": 0.0002670967741935484,
      "loss": 1.4331,
      "step": 408
    },
    {
      "epoch": 1.097986577181208,
      "grad_norm": 0.08180580288171768,
      "learning_rate": 0.00026701612903225804,
      "loss": 1.4395,
      "step": 409
    },
    {
      "epoch": 1.1006711409395973,
      "grad_norm": 0.11039198935031891,
      "learning_rate": 0.0002669354838709677,
      "loss": 1.4348,
      "step": 410
    },
    {
      "epoch": 1.1033557046979865,
      "grad_norm": 0.0924336314201355,
      "learning_rate": 0.0002668548387096774,
      "loss": 1.4976,
      "step": 411
    },
    {
      "epoch": 1.106040268456376,
      "grad_norm": 0.09549345076084137,
      "learning_rate": 0.00026677419354838707,
      "loss": 1.535,
      "step": 412
    },
    {
      "epoch": 1.1087248322147651,
      "grad_norm": 0.08583744615316391,
      "learning_rate": 0.0002666935483870968,
      "loss": 1.4581,
      "step": 413
    },
    {
      "epoch": 1.1114093959731544,
      "grad_norm": 0.08958183228969574,
      "learning_rate": 0.00026661290322580643,
      "loss": 1.5471,
      "step": 414
    },
    {
      "epoch": 1.1140939597315436,
      "grad_norm": 0.08947062492370605,
      "learning_rate": 0.0002665322580645161,
      "loss": 1.5466,
      "step": 415
    },
    {
      "epoch": 1.1167785234899328,
      "grad_norm": 0.08862050622701645,
      "learning_rate": 0.0002664516129032258,
      "loss": 1.5272,
      "step": 416
    },
    {
      "epoch": 1.1194630872483222,
      "grad_norm": 0.08211484551429749,
      "learning_rate": 0.00026637096774193546,
      "loss": 1.4745,
      "step": 417
    },
    {
      "epoch": 1.1221476510067114,
      "grad_norm": 0.09039787203073502,
      "learning_rate": 0.00026629032258064517,
      "loss": 1.5761,
      "step": 418
    },
    {
      "epoch": 1.1248322147651006,
      "grad_norm": 0.0859442874789238,
      "learning_rate": 0.0002662096774193548,
      "loss": 1.6176,
      "step": 419
    },
    {
      "epoch": 1.1275167785234899,
      "grad_norm": 0.0895935446023941,
      "learning_rate": 0.0002661290322580645,
      "loss": 1.4421,
      "step": 420
    },
    {
      "epoch": 1.1302013422818793,
      "grad_norm": 0.07907012104988098,
      "learning_rate": 0.0002660483870967742,
      "loss": 1.5071,
      "step": 421
    },
    {
      "epoch": 1.1328859060402685,
      "grad_norm": 0.08316432684659958,
      "learning_rate": 0.00026596774193548384,
      "loss": 1.4412,
      "step": 422
    },
    {
      "epoch": 1.1355704697986577,
      "grad_norm": 0.08875112235546112,
      "learning_rate": 0.0002658870967741935,
      "loss": 1.4699,
      "step": 423
    },
    {
      "epoch": 1.138255033557047,
      "grad_norm": 0.11058969050645828,
      "learning_rate": 0.0002658064516129032,
      "loss": 1.4771,
      "step": 424
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 0.0815897285938263,
      "learning_rate": 0.00026572580645161287,
      "loss": 1.5912,
      "step": 425
    },
    {
      "epoch": 1.1436241610738256,
      "grad_norm": 0.08586987107992172,
      "learning_rate": 0.0002656451612903226,
      "loss": 1.4967,
      "step": 426
    },
    {
      "epoch": 1.1463087248322148,
      "grad_norm": 0.08966661989688873,
      "learning_rate": 0.00026556451612903223,
      "loss": 1.4613,
      "step": 427
    },
    {
      "epoch": 1.148993288590604,
      "grad_norm": 0.0793885812163353,
      "learning_rate": 0.0002654838709677419,
      "loss": 1.4846,
      "step": 428
    },
    {
      "epoch": 1.1516778523489932,
      "grad_norm": 0.0900491252541542,
      "learning_rate": 0.0002654032258064516,
      "loss": 1.412,
      "step": 429
    },
    {
      "epoch": 1.1543624161073827,
      "grad_norm": 0.08390199393033981,
      "learning_rate": 0.00026532258064516126,
      "loss": 1.3879,
      "step": 430
    },
    {
      "epoch": 1.1570469798657719,
      "grad_norm": 0.08347303420305252,
      "learning_rate": 0.00026524193548387097,
      "loss": 1.5778,
      "step": 431
    },
    {
      "epoch": 1.159731543624161,
      "grad_norm": 0.09254749119281769,
      "learning_rate": 0.0002651612903225806,
      "loss": 1.3695,
      "step": 432
    },
    {
      "epoch": 1.1624161073825503,
      "grad_norm": 0.08715766668319702,
      "learning_rate": 0.0002650806451612903,
      "loss": 1.4887,
      "step": 433
    },
    {
      "epoch": 1.1651006711409395,
      "grad_norm": 0.08376234024763107,
      "learning_rate": 0.000265,
      "loss": 1.6013,
      "step": 434
    },
    {
      "epoch": 1.167785234899329,
      "grad_norm": 0.08932091295719147,
      "learning_rate": 0.00026491935483870965,
      "loss": 1.6553,
      "step": 435
    },
    {
      "epoch": 1.1704697986577182,
      "grad_norm": 0.07358680665493011,
      "learning_rate": 0.00026483870967741936,
      "loss": 1.5344,
      "step": 436
    },
    {
      "epoch": 1.1731543624161074,
      "grad_norm": 0.08578741550445557,
      "learning_rate": 0.000264758064516129,
      "loss": 1.5798,
      "step": 437
    },
    {
      "epoch": 1.1758389261744966,
      "grad_norm": 0.08632464706897736,
      "learning_rate": 0.00026467741935483867,
      "loss": 1.4892,
      "step": 438
    },
    {
      "epoch": 1.178523489932886,
      "grad_norm": 0.08776199072599411,
      "learning_rate": 0.0002645967741935484,
      "loss": 1.5486,
      "step": 439
    },
    {
      "epoch": 1.1812080536912752,
      "grad_norm": 0.09902160614728928,
      "learning_rate": 0.00026451612903225804,
      "loss": 1.4475,
      "step": 440
    },
    {
      "epoch": 1.1838926174496645,
      "grad_norm": 0.07845969498157501,
      "learning_rate": 0.00026443548387096775,
      "loss": 1.5497,
      "step": 441
    },
    {
      "epoch": 1.1865771812080537,
      "grad_norm": 0.0939851924777031,
      "learning_rate": 0.0002643548387096774,
      "loss": 1.5363,
      "step": 442
    },
    {
      "epoch": 1.1892617449664429,
      "grad_norm": 0.0892031192779541,
      "learning_rate": 0.00026427419354838706,
      "loss": 1.4169,
      "step": 443
    },
    {
      "epoch": 1.191946308724832,
      "grad_norm": 0.07405054569244385,
      "learning_rate": 0.00026419354838709677,
      "loss": 1.6067,
      "step": 444
    },
    {
      "epoch": 1.1946308724832215,
      "grad_norm": 0.08916985988616943,
      "learning_rate": 0.0002641129032258064,
      "loss": 1.584,
      "step": 445
    },
    {
      "epoch": 1.1973154362416107,
      "grad_norm": 0.10485769063234329,
      "learning_rate": 0.00026403225806451614,
      "loss": 1.4914,
      "step": 446
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.09370265901088715,
      "learning_rate": 0.0002639516129032258,
      "loss": 1.5159,
      "step": 447
    },
    {
      "epoch": 1.2026845637583892,
      "grad_norm": 0.10491802543401718,
      "learning_rate": 0.00026387096774193545,
      "loss": 1.4888,
      "step": 448
    },
    {
      "epoch": 1.2053691275167786,
      "grad_norm": 0.08768217265605927,
      "learning_rate": 0.0002637903225806451,
      "loss": 1.526,
      "step": 449
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 0.09876099973917007,
      "learning_rate": 0.0002637096774193548,
      "loss": 1.5025,
      "step": 450
    },
    {
      "epoch": 1.210738255033557,
      "grad_norm": 0.0853831097483635,
      "learning_rate": 0.0002636290322580645,
      "loss": 1.486,
      "step": 451
    },
    {
      "epoch": 1.2134228187919462,
      "grad_norm": 0.0841745138168335,
      "learning_rate": 0.0002635483870967742,
      "loss": 1.6097,
      "step": 452
    },
    {
      "epoch": 1.2161073825503355,
      "grad_norm": 0.10427684336900711,
      "learning_rate": 0.00026346774193548384,
      "loss": 1.486,
      "step": 453
    },
    {
      "epoch": 1.218791946308725,
      "grad_norm": 0.07571990042924881,
      "learning_rate": 0.0002633870967741935,
      "loss": 1.3984,
      "step": 454
    },
    {
      "epoch": 1.221476510067114,
      "grad_norm": 0.09179023653268814,
      "learning_rate": 0.0002633064516129032,
      "loss": 1.5592,
      "step": 455
    },
    {
      "epoch": 1.2241610738255033,
      "grad_norm": 0.08753760159015656,
      "learning_rate": 0.00026322580645161286,
      "loss": 1.5969,
      "step": 456
    },
    {
      "epoch": 1.2268456375838925,
      "grad_norm": 0.10569967329502106,
      "learning_rate": 0.00026314516129032257,
      "loss": 1.539,
      "step": 457
    },
    {
      "epoch": 1.229530201342282,
      "grad_norm": 0.09039373695850372,
      "learning_rate": 0.00026306451612903223,
      "loss": 1.4764,
      "step": 458
    },
    {
      "epoch": 1.2322147651006712,
      "grad_norm": 0.09195617586374283,
      "learning_rate": 0.0002629838709677419,
      "loss": 1.5441,
      "step": 459
    },
    {
      "epoch": 1.2348993288590604,
      "grad_norm": 0.07705242186784744,
      "learning_rate": 0.0002629032258064516,
      "loss": 1.5459,
      "step": 460
    },
    {
      "epoch": 1.2375838926174496,
      "grad_norm": 0.08011174947023392,
      "learning_rate": 0.00026282258064516125,
      "loss": 1.5556,
      "step": 461
    },
    {
      "epoch": 1.2402684563758388,
      "grad_norm": 0.0831574872136116,
      "learning_rate": 0.00026274193548387096,
      "loss": 1.4785,
      "step": 462
    },
    {
      "epoch": 1.2429530201342283,
      "grad_norm": 0.08412429690361023,
      "learning_rate": 0.0002626612903225806,
      "loss": 1.4737,
      "step": 463
    },
    {
      "epoch": 1.2456375838926175,
      "grad_norm": 0.0831921249628067,
      "learning_rate": 0.0002625806451612903,
      "loss": 1.5697,
      "step": 464
    },
    {
      "epoch": 1.2483221476510067,
      "grad_norm": 0.08193271607160568,
      "learning_rate": 0.0002625,
      "loss": 1.4851,
      "step": 465
    },
    {
      "epoch": 1.251006711409396,
      "grad_norm": 0.08312103152275085,
      "learning_rate": 0.00026241935483870964,
      "loss": 1.481,
      "step": 466
    },
    {
      "epoch": 1.2536912751677853,
      "grad_norm": 0.08429113030433655,
      "learning_rate": 0.00026233870967741935,
      "loss": 1.5841,
      "step": 467
    },
    {
      "epoch": 1.2563758389261745,
      "grad_norm": 0.08968877792358398,
      "learning_rate": 0.000262258064516129,
      "loss": 1.4859,
      "step": 468
    },
    {
      "epoch": 1.2590604026845638,
      "grad_norm": 0.0967206358909607,
      "learning_rate": 0.00026217741935483866,
      "loss": 1.5265,
      "step": 469
    },
    {
      "epoch": 1.261744966442953,
      "grad_norm": 0.09515096992254257,
      "learning_rate": 0.0002620967741935484,
      "loss": 1.4372,
      "step": 470
    },
    {
      "epoch": 1.2644295302013422,
      "grad_norm": 0.07729626446962357,
      "learning_rate": 0.00026201612903225803,
      "loss": 1.5291,
      "step": 471
    },
    {
      "epoch": 1.2671140939597316,
      "grad_norm": 0.09539400786161423,
      "learning_rate": 0.00026193548387096774,
      "loss": 1.5172,
      "step": 472
    },
    {
      "epoch": 1.2697986577181208,
      "grad_norm": 0.09906913340091705,
      "learning_rate": 0.0002618548387096774,
      "loss": 1.5221,
      "step": 473
    },
    {
      "epoch": 1.27248322147651,
      "grad_norm": 0.07880283892154694,
      "learning_rate": 0.00026177419354838705,
      "loss": 1.5715,
      "step": 474
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 0.07993097603321075,
      "learning_rate": 0.00026169354838709676,
      "loss": 1.4832,
      "step": 475
    },
    {
      "epoch": 1.2778523489932887,
      "grad_norm": 0.07906779646873474,
      "learning_rate": 0.0002616129032258064,
      "loss": 1.4793,
      "step": 476
    },
    {
      "epoch": 1.280536912751678,
      "grad_norm": 0.07816934585571289,
      "learning_rate": 0.00026153225806451613,
      "loss": 1.5023,
      "step": 477
    },
    {
      "epoch": 1.2832214765100671,
      "grad_norm": 0.07949699461460114,
      "learning_rate": 0.0002614516129032258,
      "loss": 1.6354,
      "step": 478
    },
    {
      "epoch": 1.2859060402684563,
      "grad_norm": 0.10629761219024658,
      "learning_rate": 0.00026137096774193544,
      "loss": 1.3718,
      "step": 479
    },
    {
      "epoch": 1.2885906040268456,
      "grad_norm": 0.09003738313913345,
      "learning_rate": 0.00026129032258064515,
      "loss": 1.4626,
      "step": 480
    },
    {
      "epoch": 1.2912751677852348,
      "grad_norm": 0.10705306380987167,
      "learning_rate": 0.0002612096774193548,
      "loss": 1.5337,
      "step": 481
    },
    {
      "epoch": 1.2939597315436242,
      "grad_norm": 0.07899980247020721,
      "learning_rate": 0.0002611290322580645,
      "loss": 1.4669,
      "step": 482
    },
    {
      "epoch": 1.2966442953020134,
      "grad_norm": 0.09322527050971985,
      "learning_rate": 0.0002610483870967742,
      "loss": 1.4781,
      "step": 483
    },
    {
      "epoch": 1.2993288590604026,
      "grad_norm": 0.08872206509113312,
      "learning_rate": 0.00026096774193548383,
      "loss": 1.4949,
      "step": 484
    },
    {
      "epoch": 1.302013422818792,
      "grad_norm": 0.09321276098489761,
      "learning_rate": 0.00026088709677419354,
      "loss": 1.3673,
      "step": 485
    },
    {
      "epoch": 1.3046979865771813,
      "grad_norm": 0.09153702855110168,
      "learning_rate": 0.0002608064516129032,
      "loss": 1.6033,
      "step": 486
    },
    {
      "epoch": 1.3073825503355705,
      "grad_norm": 0.09955079853534698,
      "learning_rate": 0.0002607258064516129,
      "loss": 1.4773,
      "step": 487
    },
    {
      "epoch": 1.3100671140939597,
      "grad_norm": 0.08607271313667297,
      "learning_rate": 0.00026064516129032256,
      "loss": 1.5273,
      "step": 488
    },
    {
      "epoch": 1.312751677852349,
      "grad_norm": 0.08818971365690231,
      "learning_rate": 0.0002605645161290322,
      "loss": 1.4733,
      "step": 489
    },
    {
      "epoch": 1.3154362416107381,
      "grad_norm": 0.09807153791189194,
      "learning_rate": 0.00026048387096774193,
      "loss": 1.3834,
      "step": 490
    },
    {
      "epoch": 1.3181208053691276,
      "grad_norm": 0.10714979469776154,
      "learning_rate": 0.0002604032258064516,
      "loss": 1.4189,
      "step": 491
    },
    {
      "epoch": 1.3208053691275168,
      "grad_norm": 0.08097705990076065,
      "learning_rate": 0.0002603225806451613,
      "loss": 1.5522,
      "step": 492
    },
    {
      "epoch": 1.323489932885906,
      "grad_norm": 0.09224270284175873,
      "learning_rate": 0.00026024193548387095,
      "loss": 1.4603,
      "step": 493
    },
    {
      "epoch": 1.3261744966442954,
      "grad_norm": 0.08997325599193573,
      "learning_rate": 0.0002601612903225806,
      "loss": 1.5321,
      "step": 494
    },
    {
      "epoch": 1.3288590604026846,
      "grad_norm": 0.08170874416828156,
      "learning_rate": 0.0002600806451612903,
      "loss": 1.4071,
      "step": 495
    },
    {
      "epoch": 1.3315436241610739,
      "grad_norm": 0.08082655817270279,
      "learning_rate": 0.00026,
      "loss": 1.4073,
      "step": 496
    },
    {
      "epoch": 1.334228187919463,
      "grad_norm": 0.07950621098279953,
      "learning_rate": 0.0002599193548387097,
      "loss": 1.422,
      "step": 497
    },
    {
      "epoch": 1.3369127516778523,
      "grad_norm": 0.08706144243478775,
      "learning_rate": 0.00025983870967741934,
      "loss": 1.4783,
      "step": 498
    },
    {
      "epoch": 1.3395973154362415,
      "grad_norm": 0.08856529742479324,
      "learning_rate": 0.000259758064516129,
      "loss": 1.628,
      "step": 499
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 0.08651379495859146,
      "learning_rate": 0.0002596774193548387,
      "loss": 1.4899,
      "step": 500
    },
    {
      "epoch": 1.3449664429530201,
      "grad_norm": 0.08355185389518738,
      "learning_rate": 0.00025959677419354837,
      "loss": 1.6388,
      "step": 501
    },
    {
      "epoch": 1.3476510067114094,
      "grad_norm": 0.09189373254776001,
      "learning_rate": 0.0002595161290322581,
      "loss": 1.4374,
      "step": 502
    },
    {
      "epoch": 1.3503355704697988,
      "grad_norm": 0.07737300544977188,
      "learning_rate": 0.00025943548387096773,
      "loss": 1.5215,
      "step": 503
    },
    {
      "epoch": 1.353020134228188,
      "grad_norm": 0.08123990893363953,
      "learning_rate": 0.0002593548387096774,
      "loss": 1.464,
      "step": 504
    },
    {
      "epoch": 1.3557046979865772,
      "grad_norm": 0.08931846916675568,
      "learning_rate": 0.0002592741935483871,
      "loss": 1.5539,
      "step": 505
    },
    {
      "epoch": 1.3583892617449664,
      "grad_norm": 0.08282656967639923,
      "learning_rate": 0.00025919354838709676,
      "loss": 1.5098,
      "step": 506
    },
    {
      "epoch": 1.3610738255033556,
      "grad_norm": 0.09782326966524124,
      "learning_rate": 0.00025911290322580647,
      "loss": 1.5255,
      "step": 507
    },
    {
      "epoch": 1.3637583892617449,
      "grad_norm": 0.08461838215589523,
      "learning_rate": 0.0002590322580645161,
      "loss": 1.5384,
      "step": 508
    },
    {
      "epoch": 1.3664429530201343,
      "grad_norm": 0.09581664949655533,
      "learning_rate": 0.0002589516129032258,
      "loss": 1.5496,
      "step": 509
    },
    {
      "epoch": 1.3691275167785235,
      "grad_norm": 0.09804491698741913,
      "learning_rate": 0.0002588709677419355,
      "loss": 1.4607,
      "step": 510
    },
    {
      "epoch": 1.3718120805369127,
      "grad_norm": 0.09407637268304825,
      "learning_rate": 0.00025879032258064515,
      "loss": 1.5032,
      "step": 511
    },
    {
      "epoch": 1.374496644295302,
      "grad_norm": 0.09057356417179108,
      "learning_rate": 0.0002587096774193548,
      "loss": 1.437,
      "step": 512
    },
    {
      "epoch": 1.3771812080536914,
      "grad_norm": 0.09213230758905411,
      "learning_rate": 0.00025862903225806446,
      "loss": 1.4529,
      "step": 513
    },
    {
      "epoch": 1.3798657718120806,
      "grad_norm": 0.103126659989357,
      "learning_rate": 0.00025854838709677417,
      "loss": 1.5273,
      "step": 514
    },
    {
      "epoch": 1.3825503355704698,
      "grad_norm": 0.08084128051996231,
      "learning_rate": 0.0002584677419354838,
      "loss": 1.4127,
      "step": 515
    },
    {
      "epoch": 1.385234899328859,
      "grad_norm": 0.10124743729829788,
      "learning_rate": 0.00025838709677419354,
      "loss": 1.5819,
      "step": 516
    },
    {
      "epoch": 1.3879194630872482,
      "grad_norm": 0.08396533876657486,
      "learning_rate": 0.0002583064516129032,
      "loss": 1.6179,
      "step": 517
    },
    {
      "epoch": 1.3906040268456377,
      "grad_norm": 0.0930340513586998,
      "learning_rate": 0.00025822580645161285,
      "loss": 1.4698,
      "step": 518
    },
    {
      "epoch": 1.3932885906040269,
      "grad_norm": 0.1009446382522583,
      "learning_rate": 0.00025814516129032256,
      "loss": 1.5467,
      "step": 519
    },
    {
      "epoch": 1.395973154362416,
      "grad_norm": 0.07542748004198074,
      "learning_rate": 0.0002580645161290322,
      "loss": 1.5987,
      "step": 520
    },
    {
      "epoch": 1.3986577181208053,
      "grad_norm": 0.0813923329114914,
      "learning_rate": 0.0002579838709677419,
      "loss": 1.6357,
      "step": 521
    },
    {
      "epoch": 1.4013422818791947,
      "grad_norm": 0.09362021088600159,
      "learning_rate": 0.0002579032258064516,
      "loss": 1.546,
      "step": 522
    },
    {
      "epoch": 1.404026845637584,
      "grad_norm": 0.08163196593523026,
      "learning_rate": 0.00025782258064516124,
      "loss": 1.5773,
      "step": 523
    },
    {
      "epoch": 1.4067114093959732,
      "grad_norm": 0.08252521604299545,
      "learning_rate": 0.00025774193548387095,
      "loss": 1.4903,
      "step": 524
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 0.07706792652606964,
      "learning_rate": 0.0002576612903225806,
      "loss": 1.5273,
      "step": 525
    },
    {
      "epoch": 1.4120805369127516,
      "grad_norm": 0.08037301152944565,
      "learning_rate": 0.0002575806451612903,
      "loss": 1.4824,
      "step": 526
    },
    {
      "epoch": 1.4147651006711408,
      "grad_norm": 0.08776941150426865,
      "learning_rate": 0.00025749999999999997,
      "loss": 1.4376,
      "step": 527
    },
    {
      "epoch": 1.4174496644295302,
      "grad_norm": 0.08202077448368073,
      "learning_rate": 0.0002574193548387096,
      "loss": 1.564,
      "step": 528
    },
    {
      "epoch": 1.4201342281879195,
      "grad_norm": 0.08703737705945969,
      "learning_rate": 0.00025733870967741934,
      "loss": 1.4623,
      "step": 529
    },
    {
      "epoch": 1.4228187919463087,
      "grad_norm": 0.0865999087691307,
      "learning_rate": 0.000257258064516129,
      "loss": 1.4863,
      "step": 530
    },
    {
      "epoch": 1.425503355704698,
      "grad_norm": 0.09749078750610352,
      "learning_rate": 0.0002571774193548387,
      "loss": 1.5596,
      "step": 531
    },
    {
      "epoch": 1.4281879194630873,
      "grad_norm": 0.08145223557949066,
      "learning_rate": 0.00025709677419354836,
      "loss": 1.5181,
      "step": 532
    },
    {
      "epoch": 1.4308724832214765,
      "grad_norm": 0.09700234234333038,
      "learning_rate": 0.000257016129032258,
      "loss": 1.4548,
      "step": 533
    },
    {
      "epoch": 1.4335570469798657,
      "grad_norm": 0.07859249413013458,
      "learning_rate": 0.0002569354838709677,
      "loss": 1.5833,
      "step": 534
    },
    {
      "epoch": 1.436241610738255,
      "grad_norm": 0.08476351946592331,
      "learning_rate": 0.0002568548387096774,
      "loss": 1.5687,
      "step": 535
    },
    {
      "epoch": 1.4389261744966442,
      "grad_norm": 0.09406504034996033,
      "learning_rate": 0.0002567741935483871,
      "loss": 1.4748,
      "step": 536
    },
    {
      "epoch": 1.4416107382550336,
      "grad_norm": 0.10211994498968124,
      "learning_rate": 0.00025669354838709675,
      "loss": 1.4686,
      "step": 537
    },
    {
      "epoch": 1.4442953020134228,
      "grad_norm": 0.08699703961610794,
      "learning_rate": 0.0002566129032258064,
      "loss": 1.5001,
      "step": 538
    },
    {
      "epoch": 1.446979865771812,
      "grad_norm": 0.0738489031791687,
      "learning_rate": 0.0002565322580645161,
      "loss": 1.4422,
      "step": 539
    },
    {
      "epoch": 1.4496644295302015,
      "grad_norm": 0.09125514328479767,
      "learning_rate": 0.00025645161290322577,
      "loss": 1.4407,
      "step": 540
    },
    {
      "epoch": 1.4523489932885907,
      "grad_norm": 0.08948425948619843,
      "learning_rate": 0.0002563709677419355,
      "loss": 1.5979,
      "step": 541
    },
    {
      "epoch": 1.45503355704698,
      "grad_norm": 0.08322823792695999,
      "learning_rate": 0.00025629032258064514,
      "loss": 1.4311,
      "step": 542
    },
    {
      "epoch": 1.457718120805369,
      "grad_norm": 0.08584883064031601,
      "learning_rate": 0.0002562096774193548,
      "loss": 1.4315,
      "step": 543
    },
    {
      "epoch": 1.4604026845637583,
      "grad_norm": 0.08816306293010712,
      "learning_rate": 0.0002561290322580645,
      "loss": 1.3436,
      "step": 544
    },
    {
      "epoch": 1.4630872483221475,
      "grad_norm": 0.0961395874619484,
      "learning_rate": 0.00025604838709677416,
      "loss": 1.4536,
      "step": 545
    },
    {
      "epoch": 1.465771812080537,
      "grad_norm": 0.09032248705625534,
      "learning_rate": 0.00025596774193548387,
      "loss": 1.6107,
      "step": 546
    },
    {
      "epoch": 1.4684563758389262,
      "grad_norm": 0.08553306758403778,
      "learning_rate": 0.00025588709677419353,
      "loss": 1.3936,
      "step": 547
    },
    {
      "epoch": 1.4711409395973154,
      "grad_norm": 0.09877913445234299,
      "learning_rate": 0.0002558064516129032,
      "loss": 1.5053,
      "step": 548
    },
    {
      "epoch": 1.4738255033557048,
      "grad_norm": 0.09502401202917099,
      "learning_rate": 0.0002557258064516129,
      "loss": 1.5119,
      "step": 549
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 0.08362889289855957,
      "learning_rate": 0.00025564516129032255,
      "loss": 1.5336,
      "step": 550
    },
    {
      "epoch": 1.4791946308724833,
      "grad_norm": 0.090540811419487,
      "learning_rate": 0.00025556451612903226,
      "loss": 1.5152,
      "step": 551
    },
    {
      "epoch": 1.4818791946308725,
      "grad_norm": 0.07332811504602432,
      "learning_rate": 0.0002554838709677419,
      "loss": 1.5704,
      "step": 552
    },
    {
      "epoch": 1.4845637583892617,
      "grad_norm": 0.08427606523036957,
      "learning_rate": 0.0002554032258064516,
      "loss": 1.4714,
      "step": 553
    },
    {
      "epoch": 1.487248322147651,
      "grad_norm": 0.08568070083856583,
      "learning_rate": 0.0002553225806451613,
      "loss": 1.5208,
      "step": 554
    },
    {
      "epoch": 1.4899328859060403,
      "grad_norm": 0.08648577332496643,
      "learning_rate": 0.00025524193548387094,
      "loss": 1.3442,
      "step": 555
    },
    {
      "epoch": 1.4926174496644296,
      "grad_norm": 0.09121348708868027,
      "learning_rate": 0.00025516129032258065,
      "loss": 1.4235,
      "step": 556
    },
    {
      "epoch": 1.4953020134228188,
      "grad_norm": 0.08416840434074402,
      "learning_rate": 0.0002550806451612903,
      "loss": 1.4337,
      "step": 557
    },
    {
      "epoch": 1.497986577181208,
      "grad_norm": 0.08243627846240997,
      "learning_rate": 0.00025499999999999996,
      "loss": 1.4769,
      "step": 558
    },
    {
      "epoch": 1.5006711409395974,
      "grad_norm": 0.07925841212272644,
      "learning_rate": 0.0002549193548387097,
      "loss": 1.4767,
      "step": 559
    },
    {
      "epoch": 1.5033557046979866,
      "grad_norm": 0.09581931680440903,
      "learning_rate": 0.00025483870967741933,
      "loss": 1.4361,
      "step": 560
    },
    {
      "epoch": 1.5060402684563758,
      "grad_norm": 0.08657612651586533,
      "learning_rate": 0.00025475806451612904,
      "loss": 1.4788,
      "step": 561
    },
    {
      "epoch": 1.508724832214765,
      "grad_norm": 0.08471605181694031,
      "learning_rate": 0.0002546774193548387,
      "loss": 1.4764,
      "step": 562
    },
    {
      "epoch": 1.5114093959731543,
      "grad_norm": 0.09510819613933563,
      "learning_rate": 0.00025459677419354835,
      "loss": 1.3694,
      "step": 563
    },
    {
      "epoch": 1.5140939597315435,
      "grad_norm": 0.08865015208721161,
      "learning_rate": 0.00025451612903225806,
      "loss": 1.4961,
      "step": 564
    },
    {
      "epoch": 1.516778523489933,
      "grad_norm": 0.09578723460435867,
      "learning_rate": 0.0002544354838709677,
      "loss": 1.4007,
      "step": 565
    },
    {
      "epoch": 1.5194630872483221,
      "grad_norm": 0.08617044985294342,
      "learning_rate": 0.00025435483870967743,
      "loss": 1.5871,
      "step": 566
    },
    {
      "epoch": 1.5221476510067116,
      "grad_norm": 0.08926606923341751,
      "learning_rate": 0.0002542741935483871,
      "loss": 1.5205,
      "step": 567
    },
    {
      "epoch": 1.5248322147651008,
      "grad_norm": 0.09428057819604874,
      "learning_rate": 0.00025419354838709674,
      "loss": 1.4991,
      "step": 568
    },
    {
      "epoch": 1.52751677852349,
      "grad_norm": 0.10501308739185333,
      "learning_rate": 0.00025411290322580645,
      "loss": 1.4857,
      "step": 569
    },
    {
      "epoch": 1.5302013422818792,
      "grad_norm": 0.08975903689861298,
      "learning_rate": 0.0002540322580645161,
      "loss": 1.5017,
      "step": 570
    },
    {
      "epoch": 1.5328859060402684,
      "grad_norm": 0.08647438883781433,
      "learning_rate": 0.0002539516129032258,
      "loss": 1.5739,
      "step": 571
    },
    {
      "epoch": 1.5355704697986576,
      "grad_norm": 0.09135519713163376,
      "learning_rate": 0.0002538709677419354,
      "loss": 1.4869,
      "step": 572
    },
    {
      "epoch": 1.5382550335570468,
      "grad_norm": 0.09197715669870377,
      "learning_rate": 0.00025379032258064513,
      "loss": 1.5052,
      "step": 573
    },
    {
      "epoch": 1.5409395973154363,
      "grad_norm": 0.09494907408952713,
      "learning_rate": 0.00025370967741935484,
      "loss": 1.3406,
      "step": 574
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 0.08725005388259888,
      "learning_rate": 0.0002536290322580645,
      "loss": 1.4069,
      "step": 575
    },
    {
      "epoch": 1.546308724832215,
      "grad_norm": 0.09201180934906006,
      "learning_rate": 0.0002535483870967742,
      "loss": 1.503,
      "step": 576
    },
    {
      "epoch": 1.5489932885906041,
      "grad_norm": 0.08350527286529541,
      "learning_rate": 0.0002534677419354838,
      "loss": 1.5277,
      "step": 577
    },
    {
      "epoch": 1.5516778523489934,
      "grad_norm": 0.10197166353464127,
      "learning_rate": 0.0002533870967741935,
      "loss": 1.3931,
      "step": 578
    },
    {
      "epoch": 1.5543624161073826,
      "grad_norm": 0.0993698313832283,
      "learning_rate": 0.0002533064516129032,
      "loss": 1.4341,
      "step": 579
    },
    {
      "epoch": 1.5570469798657718,
      "grad_norm": 0.08856377005577087,
      "learning_rate": 0.0002532258064516129,
      "loss": 1.5438,
      "step": 580
    },
    {
      "epoch": 1.559731543624161,
      "grad_norm": 0.08652646094560623,
      "learning_rate": 0.0002531451612903226,
      "loss": 1.5301,
      "step": 581
    },
    {
      "epoch": 1.5624161073825502,
      "grad_norm": 0.07923541963100433,
      "learning_rate": 0.0002530645161290322,
      "loss": 1.5742,
      "step": 582
    },
    {
      "epoch": 1.5651006711409396,
      "grad_norm": 0.07798930257558823,
      "learning_rate": 0.0002529838709677419,
      "loss": 1.5745,
      "step": 583
    },
    {
      "epoch": 1.5677852348993289,
      "grad_norm": 0.095528744161129,
      "learning_rate": 0.00025290322580645157,
      "loss": 1.5879,
      "step": 584
    },
    {
      "epoch": 1.570469798657718,
      "grad_norm": 0.09443731606006622,
      "learning_rate": 0.0002528225806451613,
      "loss": 1.4785,
      "step": 585
    },
    {
      "epoch": 1.5731543624161075,
      "grad_norm": 0.07665207982063293,
      "learning_rate": 0.00025274193548387093,
      "loss": 1.5677,
      "step": 586
    },
    {
      "epoch": 1.5758389261744967,
      "grad_norm": 0.08367009460926056,
      "learning_rate": 0.0002526612903225806,
      "loss": 1.6061,
      "step": 587
    },
    {
      "epoch": 1.578523489932886,
      "grad_norm": 0.08186925202608109,
      "learning_rate": 0.0002525806451612903,
      "loss": 1.4618,
      "step": 588
    },
    {
      "epoch": 1.5812080536912752,
      "grad_norm": 0.09087203443050385,
      "learning_rate": 0.00025249999999999996,
      "loss": 1.5312,
      "step": 589
    },
    {
      "epoch": 1.5838926174496644,
      "grad_norm": 0.08953844010829926,
      "learning_rate": 0.00025241935483870967,
      "loss": 1.4806,
      "step": 590
    },
    {
      "epoch": 1.5865771812080536,
      "grad_norm": 0.08685079962015152,
      "learning_rate": 0.0002523387096774193,
      "loss": 1.4445,
      "step": 591
    },
    {
      "epoch": 1.5892617449664428,
      "grad_norm": 0.09241853654384613,
      "learning_rate": 0.000252258064516129,
      "loss": 1.4497,
      "step": 592
    },
    {
      "epoch": 1.5919463087248322,
      "grad_norm": 0.08607541769742966,
      "learning_rate": 0.0002521774193548387,
      "loss": 1.5403,
      "step": 593
    },
    {
      "epoch": 1.5946308724832214,
      "grad_norm": 0.07952491194009781,
      "learning_rate": 0.00025209677419354835,
      "loss": 1.5811,
      "step": 594
    },
    {
      "epoch": 1.5973154362416109,
      "grad_norm": 0.0844491496682167,
      "learning_rate": 0.00025201612903225806,
      "loss": 1.398,
      "step": 595
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.09986941516399384,
      "learning_rate": 0.0002519354838709677,
      "loss": 1.4247,
      "step": 596
    },
    {
      "epoch": 1.6026845637583893,
      "grad_norm": 0.10037145763635635,
      "learning_rate": 0.00025185483870967737,
      "loss": 1.4676,
      "step": 597
    },
    {
      "epoch": 1.6053691275167785,
      "grad_norm": 0.10279489308595657,
      "learning_rate": 0.0002517741935483871,
      "loss": 1.429,
      "step": 598
    },
    {
      "epoch": 1.6080536912751677,
      "grad_norm": 0.08823813498020172,
      "learning_rate": 0.00025169354838709674,
      "loss": 1.3781,
      "step": 599
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 0.08899696171283722,
      "learning_rate": 0.00025161290322580645,
      "loss": 1.4654,
      "step": 600
    },
    {
      "epoch": 1.6134228187919462,
      "grad_norm": 0.08676249533891678,
      "learning_rate": 0.0002515322580645161,
      "loss": 1.5042,
      "step": 601
    },
    {
      "epoch": 1.6161073825503356,
      "grad_norm": 0.09400346130132675,
      "learning_rate": 0.00025145161290322576,
      "loss": 1.5619,
      "step": 602
    },
    {
      "epoch": 1.6187919463087248,
      "grad_norm": 0.08692251890897751,
      "learning_rate": 0.00025137096774193547,
      "loss": 1.477,
      "step": 603
    },
    {
      "epoch": 1.6214765100671142,
      "grad_norm": 0.08945789933204651,
      "learning_rate": 0.0002512903225806451,
      "loss": 1.5369,
      "step": 604
    },
    {
      "epoch": 1.6241610738255035,
      "grad_norm": 0.08273133635520935,
      "learning_rate": 0.00025120967741935484,
      "loss": 1.4663,
      "step": 605
    },
    {
      "epoch": 1.6268456375838927,
      "grad_norm": 0.08920068293809891,
      "learning_rate": 0.0002511290322580645,
      "loss": 1.4797,
      "step": 606
    },
    {
      "epoch": 1.6295302013422819,
      "grad_norm": 0.08983076363801956,
      "learning_rate": 0.00025104838709677415,
      "loss": 1.4195,
      "step": 607
    },
    {
      "epoch": 1.632214765100671,
      "grad_norm": 0.08915294706821442,
      "learning_rate": 0.00025096774193548386,
      "loss": 1.405,
      "step": 608
    },
    {
      "epoch": 1.6348993288590603,
      "grad_norm": 0.08373590558767319,
      "learning_rate": 0.0002508870967741935,
      "loss": 1.4913,
      "step": 609
    },
    {
      "epoch": 1.6375838926174495,
      "grad_norm": 0.10068989545106888,
      "learning_rate": 0.0002508064516129032,
      "loss": 1.4267,
      "step": 610
    },
    {
      "epoch": 1.640268456375839,
      "grad_norm": 0.08985983580350876,
      "learning_rate": 0.0002507258064516129,
      "loss": 1.4103,
      "step": 611
    },
    {
      "epoch": 1.6429530201342282,
      "grad_norm": 0.10216963291168213,
      "learning_rate": 0.00025064516129032254,
      "loss": 1.4635,
      "step": 612
    },
    {
      "epoch": 1.6456375838926176,
      "grad_norm": 0.08674748986959457,
      "learning_rate": 0.00025056451612903225,
      "loss": 1.4552,
      "step": 613
    },
    {
      "epoch": 1.6483221476510068,
      "grad_norm": 0.08595388382673264,
      "learning_rate": 0.0002504838709677419,
      "loss": 1.522,
      "step": 614
    },
    {
      "epoch": 1.651006711409396,
      "grad_norm": 0.07960347831249237,
      "learning_rate": 0.0002504032258064516,
      "loss": 1.5498,
      "step": 615
    },
    {
      "epoch": 1.6536912751677852,
      "grad_norm": 0.08639302849769592,
      "learning_rate": 0.00025032258064516127,
      "loss": 1.4847,
      "step": 616
    },
    {
      "epoch": 1.6563758389261745,
      "grad_norm": 0.11410137265920639,
      "learning_rate": 0.00025024193548387093,
      "loss": 1.6276,
      "step": 617
    },
    {
      "epoch": 1.6590604026845637,
      "grad_norm": 0.11004699766635895,
      "learning_rate": 0.00025016129032258064,
      "loss": 1.4834,
      "step": 618
    },
    {
      "epoch": 1.6617449664429529,
      "grad_norm": 0.10517321527004242,
      "learning_rate": 0.0002500806451612903,
      "loss": 1.4571,
      "step": 619
    },
    {
      "epoch": 1.6644295302013423,
      "grad_norm": 0.09012433886528015,
      "learning_rate": 0.00025,
      "loss": 1.4234,
      "step": 620
    },
    {
      "epoch": 1.6671140939597315,
      "grad_norm": 0.0958704873919487,
      "learning_rate": 0.00024991935483870966,
      "loss": 1.4584,
      "step": 621
    },
    {
      "epoch": 1.669798657718121,
      "grad_norm": 0.07789608091115952,
      "learning_rate": 0.0002498387096774193,
      "loss": 1.5217,
      "step": 622
    },
    {
      "epoch": 1.6724832214765102,
      "grad_norm": 0.09795726090669632,
      "learning_rate": 0.00024975806451612903,
      "loss": 1.5409,
      "step": 623
    },
    {
      "epoch": 1.6751677852348994,
      "grad_norm": 0.09593166410923004,
      "learning_rate": 0.0002496774193548387,
      "loss": 1.4935,
      "step": 624
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 0.10195325314998627,
      "learning_rate": 0.0002495967741935484,
      "loss": 1.5823,
      "step": 625
    },
    {
      "epoch": 1.6805369127516778,
      "grad_norm": 0.09121008217334747,
      "learning_rate": 0.00024951612903225805,
      "loss": 1.419,
      "step": 626
    },
    {
      "epoch": 1.683221476510067,
      "grad_norm": 0.09284203499555588,
      "learning_rate": 0.0002494354838709677,
      "loss": 1.5544,
      "step": 627
    },
    {
      "epoch": 1.6859060402684563,
      "grad_norm": 0.09382161498069763,
      "learning_rate": 0.0002493548387096774,
      "loss": 1.5369,
      "step": 628
    },
    {
      "epoch": 1.6885906040268457,
      "grad_norm": 0.07773222029209137,
      "learning_rate": 0.0002492741935483871,
      "loss": 1.6396,
      "step": 629
    },
    {
      "epoch": 1.691275167785235,
      "grad_norm": 0.09594681113958359,
      "learning_rate": 0.0002491935483870968,
      "loss": 1.5235,
      "step": 630
    },
    {
      "epoch": 1.6939597315436241,
      "grad_norm": 0.08619973063468933,
      "learning_rate": 0.00024911290322580644,
      "loss": 1.5074,
      "step": 631
    },
    {
      "epoch": 1.6966442953020135,
      "grad_norm": 0.0748210996389389,
      "learning_rate": 0.0002490322580645161,
      "loss": 1.4296,
      "step": 632
    },
    {
      "epoch": 1.6993288590604028,
      "grad_norm": 0.09601756185293198,
      "learning_rate": 0.0002489516129032258,
      "loss": 1.5149,
      "step": 633
    },
    {
      "epoch": 1.702013422818792,
      "grad_norm": 0.08935841917991638,
      "learning_rate": 0.00024887096774193546,
      "loss": 1.4177,
      "step": 634
    },
    {
      "epoch": 1.7046979865771812,
      "grad_norm": 0.09710521996021271,
      "learning_rate": 0.0002487903225806452,
      "loss": 1.5383,
      "step": 635
    },
    {
      "epoch": 1.7073825503355704,
      "grad_norm": 0.08486348390579224,
      "learning_rate": 0.0002487096774193548,
      "loss": 1.4568,
      "step": 636
    },
    {
      "epoch": 1.7100671140939596,
      "grad_norm": 0.08135449141263962,
      "learning_rate": 0.0002486290322580645,
      "loss": 1.4187,
      "step": 637
    },
    {
      "epoch": 1.7127516778523488,
      "grad_norm": 0.09323802590370178,
      "learning_rate": 0.0002485483870967742,
      "loss": 1.3568,
      "step": 638
    },
    {
      "epoch": 1.7154362416107383,
      "grad_norm": 0.07967578619718552,
      "learning_rate": 0.00024846774193548385,
      "loss": 1.6655,
      "step": 639
    },
    {
      "epoch": 1.7181208053691275,
      "grad_norm": 0.08564920723438263,
      "learning_rate": 0.00024838709677419356,
      "loss": 1.479,
      "step": 640
    },
    {
      "epoch": 1.720805369127517,
      "grad_norm": 0.08105545490980148,
      "learning_rate": 0.00024830645161290317,
      "loss": 1.4947,
      "step": 641
    },
    {
      "epoch": 1.7234899328859061,
      "grad_norm": 0.09279259294271469,
      "learning_rate": 0.0002482258064516129,
      "loss": 1.5107,
      "step": 642
    },
    {
      "epoch": 1.7261744966442953,
      "grad_norm": 0.0827261283993721,
      "learning_rate": 0.00024814516129032253,
      "loss": 1.5655,
      "step": 643
    },
    {
      "epoch": 1.7288590604026846,
      "grad_norm": 0.08226583153009415,
      "learning_rate": 0.00024806451612903224,
      "loss": 1.4924,
      "step": 644
    },
    {
      "epoch": 1.7315436241610738,
      "grad_norm": 0.07882718741893768,
      "learning_rate": 0.00024798387096774195,
      "loss": 1.4666,
      "step": 645
    },
    {
      "epoch": 1.734228187919463,
      "grad_norm": 0.07895389199256897,
      "learning_rate": 0.00024790322580645155,
      "loss": 1.5077,
      "step": 646
    },
    {
      "epoch": 1.7369127516778522,
      "grad_norm": 0.08567406237125397,
      "learning_rate": 0.00024782258064516127,
      "loss": 1.471,
      "step": 647
    },
    {
      "epoch": 1.7395973154362416,
      "grad_norm": 0.09326346218585968,
      "learning_rate": 0.0002477419354838709,
      "loss": 1.3559,
      "step": 648
    },
    {
      "epoch": 1.7422818791946308,
      "grad_norm": 0.08791505545377731,
      "learning_rate": 0.00024766129032258063,
      "loss": 1.5026,
      "step": 649
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 0.08348291367292404,
      "learning_rate": 0.0002475806451612903,
      "loss": 1.4732,
      "step": 650
    },
    {
      "epoch": 1.7476510067114095,
      "grad_norm": 0.0707394927740097,
      "learning_rate": 0.00024749999999999994,
      "loss": 1.5492,
      "step": 651
    },
    {
      "epoch": 1.7503355704697987,
      "grad_norm": 0.10424160212278366,
      "learning_rate": 0.00024741935483870965,
      "loss": 1.5122,
      "step": 652
    },
    {
      "epoch": 1.753020134228188,
      "grad_norm": 0.08227095007896423,
      "learning_rate": 0.0002473387096774193,
      "loss": 1.4962,
      "step": 653
    },
    {
      "epoch": 1.7557046979865771,
      "grad_norm": 0.091006338596344,
      "learning_rate": 0.000247258064516129,
      "loss": 1.4747,
      "step": 654
    },
    {
      "epoch": 1.7583892617449663,
      "grad_norm": 0.0861247181892395,
      "learning_rate": 0.0002471774193548387,
      "loss": 1.4705,
      "step": 655
    },
    {
      "epoch": 1.7610738255033556,
      "grad_norm": 0.0896545797586441,
      "learning_rate": 0.00024709677419354833,
      "loss": 1.6334,
      "step": 656
    },
    {
      "epoch": 1.763758389261745,
      "grad_norm": 0.08022215962409973,
      "learning_rate": 0.00024701612903225804,
      "loss": 1.5128,
      "step": 657
    },
    {
      "epoch": 1.7664429530201342,
      "grad_norm": 0.07739681750535965,
      "learning_rate": 0.0002469354838709677,
      "loss": 1.6049,
      "step": 658
    },
    {
      "epoch": 1.7691275167785236,
      "grad_norm": 0.09251812845468521,
      "learning_rate": 0.0002468548387096774,
      "loss": 1.3736,
      "step": 659
    },
    {
      "epoch": 1.7718120805369129,
      "grad_norm": 0.08683338016271591,
      "learning_rate": 0.00024677419354838707,
      "loss": 1.4097,
      "step": 660
    },
    {
      "epoch": 1.774496644295302,
      "grad_norm": 0.0996997207403183,
      "learning_rate": 0.0002466935483870967,
      "loss": 1.4002,
      "step": 661
    },
    {
      "epoch": 1.7771812080536913,
      "grad_norm": 0.08796467632055283,
      "learning_rate": 0.00024661290322580643,
      "loss": 1.4665,
      "step": 662
    },
    {
      "epoch": 1.7798657718120805,
      "grad_norm": 0.09139148145914078,
      "learning_rate": 0.0002465322580645161,
      "loss": 1.5637,
      "step": 663
    },
    {
      "epoch": 1.7825503355704697,
      "grad_norm": 0.08345766365528107,
      "learning_rate": 0.0002464516129032258,
      "loss": 1.5748,
      "step": 664
    },
    {
      "epoch": 1.785234899328859,
      "grad_norm": 0.08085847645998001,
      "learning_rate": 0.00024637096774193546,
      "loss": 1.5338,
      "step": 665
    },
    {
      "epoch": 1.7879194630872484,
      "grad_norm": 0.10018507391214371,
      "learning_rate": 0.0002462903225806451,
      "loss": 1.5149,
      "step": 666
    },
    {
      "epoch": 1.7906040268456376,
      "grad_norm": 0.09289786219596863,
      "learning_rate": 0.0002462096774193548,
      "loss": 1.4178,
      "step": 667
    },
    {
      "epoch": 1.793288590604027,
      "grad_norm": 0.09497584402561188,
      "learning_rate": 0.0002461290322580645,
      "loss": 1.4779,
      "step": 668
    },
    {
      "epoch": 1.7959731543624162,
      "grad_norm": 0.10581051558256149,
      "learning_rate": 0.0002460483870967742,
      "loss": 1.5463,
      "step": 669
    },
    {
      "epoch": 1.7986577181208054,
      "grad_norm": 0.09495978057384491,
      "learning_rate": 0.00024596774193548385,
      "loss": 1.3615,
      "step": 670
    },
    {
      "epoch": 1.8013422818791947,
      "grad_norm": 0.09733212739229202,
      "learning_rate": 0.0002458870967741935,
      "loss": 1.3948,
      "step": 671
    },
    {
      "epoch": 1.8040268456375839,
      "grad_norm": 0.08558397740125656,
      "learning_rate": 0.0002458064516129032,
      "loss": 1.6061,
      "step": 672
    },
    {
      "epoch": 1.806711409395973,
      "grad_norm": 0.09004022181034088,
      "learning_rate": 0.00024572580645161287,
      "loss": 1.473,
      "step": 673
    },
    {
      "epoch": 1.8093959731543623,
      "grad_norm": 0.081833615899086,
      "learning_rate": 0.0002456451612903226,
      "loss": 1.5035,
      "step": 674
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 0.10137204825878143,
      "learning_rate": 0.00024556451612903224,
      "loss": 1.3269,
      "step": 675
    },
    {
      "epoch": 1.814765100671141,
      "grad_norm": 0.090270034968853,
      "learning_rate": 0.0002454838709677419,
      "loss": 1.4589,
      "step": 676
    },
    {
      "epoch": 1.8174496644295302,
      "grad_norm": 0.10638986527919769,
      "learning_rate": 0.0002454032258064516,
      "loss": 1.4062,
      "step": 677
    },
    {
      "epoch": 1.8201342281879196,
      "grad_norm": 0.09152580052614212,
      "learning_rate": 0.00024532258064516126,
      "loss": 1.4686,
      "step": 678
    },
    {
      "epoch": 1.8228187919463088,
      "grad_norm": 0.09794974327087402,
      "learning_rate": 0.00024524193548387097,
      "loss": 1.4549,
      "step": 679
    },
    {
      "epoch": 1.825503355704698,
      "grad_norm": 0.08087985217571259,
      "learning_rate": 0.0002451612903225806,
      "loss": 1.3734,
      "step": 680
    },
    {
      "epoch": 1.8281879194630872,
      "grad_norm": 0.0923236608505249,
      "learning_rate": 0.0002450806451612903,
      "loss": 1.4856,
      "step": 681
    },
    {
      "epoch": 1.8308724832214764,
      "grad_norm": 0.08777862042188644,
      "learning_rate": 0.000245,
      "loss": 1.468,
      "step": 682
    },
    {
      "epoch": 1.8335570469798657,
      "grad_norm": 0.08501493185758591,
      "learning_rate": 0.00024491935483870965,
      "loss": 1.513,
      "step": 683
    },
    {
      "epoch": 1.8362416107382549,
      "grad_norm": 0.10792340338230133,
      "learning_rate": 0.00024483870967741936,
      "loss": 1.6688,
      "step": 684
    },
    {
      "epoch": 1.8389261744966443,
      "grad_norm": 0.08774790167808533,
      "learning_rate": 0.000244758064516129,
      "loss": 1.4702,
      "step": 685
    },
    {
      "epoch": 1.8416107382550335,
      "grad_norm": 0.08250267803668976,
      "learning_rate": 0.00024467741935483867,
      "loss": 1.5099,
      "step": 686
    },
    {
      "epoch": 1.844295302013423,
      "grad_norm": 0.09037285298109055,
      "learning_rate": 0.0002445967741935484,
      "loss": 1.4041,
      "step": 687
    },
    {
      "epoch": 1.8469798657718122,
      "grad_norm": 0.09285866469144821,
      "learning_rate": 0.00024451612903225804,
      "loss": 1.5039,
      "step": 688
    },
    {
      "epoch": 1.8496644295302014,
      "grad_norm": 0.07847631722688675,
      "learning_rate": 0.00024443548387096775,
      "loss": 1.4497,
      "step": 689
    },
    {
      "epoch": 1.8523489932885906,
      "grad_norm": 0.07768559455871582,
      "learning_rate": 0.0002443548387096774,
      "loss": 1.5364,
      "step": 690
    },
    {
      "epoch": 1.8550335570469798,
      "grad_norm": 0.08013877272605896,
      "learning_rate": 0.00024427419354838706,
      "loss": 1.4852,
      "step": 691
    },
    {
      "epoch": 1.857718120805369,
      "grad_norm": 0.08294056355953217,
      "learning_rate": 0.00024419354838709677,
      "loss": 1.4332,
      "step": 692
    },
    {
      "epoch": 1.8604026845637582,
      "grad_norm": 0.07820028066635132,
      "learning_rate": 0.00024411290322580643,
      "loss": 1.4209,
      "step": 693
    },
    {
      "epoch": 1.8630872483221477,
      "grad_norm": 0.09818243235349655,
      "learning_rate": 0.0002440322580645161,
      "loss": 1.5605,
      "step": 694
    },
    {
      "epoch": 1.8657718120805369,
      "grad_norm": 0.09038176387548447,
      "learning_rate": 0.0002439516129032258,
      "loss": 1.5159,
      "step": 695
    },
    {
      "epoch": 1.8684563758389263,
      "grad_norm": 0.07562927901744843,
      "learning_rate": 0.00024387096774193545,
      "loss": 1.5038,
      "step": 696
    },
    {
      "epoch": 1.8711409395973155,
      "grad_norm": 0.08307965844869614,
      "learning_rate": 0.00024379032258064513,
      "loss": 1.3852,
      "step": 697
    },
    {
      "epoch": 1.8738255033557047,
      "grad_norm": 0.08118221908807755,
      "learning_rate": 0.00024370967741935482,
      "loss": 1.642,
      "step": 698
    },
    {
      "epoch": 1.876510067114094,
      "grad_norm": 0.09437835961580276,
      "learning_rate": 0.0002436290322580645,
      "loss": 1.3754,
      "step": 699
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 0.0823817178606987,
      "learning_rate": 0.00024354838709677416,
      "loss": 1.4407,
      "step": 700
    },
    {
      "epoch": 1.8818791946308724,
      "grad_norm": 0.08487346768379211,
      "learning_rate": 0.00024346774193548384,
      "loss": 1.5339,
      "step": 701
    },
    {
      "epoch": 1.8845637583892616,
      "grad_norm": 0.12072370946407318,
      "learning_rate": 0.00024338709677419352,
      "loss": 1.3372,
      "step": 702
    },
    {
      "epoch": 1.887248322147651,
      "grad_norm": 0.08650468289852142,
      "learning_rate": 0.0002433064516129032,
      "loss": 1.4412,
      "step": 703
    },
    {
      "epoch": 1.8899328859060402,
      "grad_norm": 0.07894740998744965,
      "learning_rate": 0.0002432258064516129,
      "loss": 1.5027,
      "step": 704
    },
    {
      "epoch": 1.8926174496644297,
      "grad_norm": 0.09037928283214569,
      "learning_rate": 0.00024314516129032255,
      "loss": 1.4745,
      "step": 705
    },
    {
      "epoch": 1.895302013422819,
      "grad_norm": 0.091584712266922,
      "learning_rate": 0.00024306451612903223,
      "loss": 1.3915,
      "step": 706
    },
    {
      "epoch": 1.8979865771812081,
      "grad_norm": 0.09229692816734314,
      "learning_rate": 0.0002429838709677419,
      "loss": 1.4847,
      "step": 707
    },
    {
      "epoch": 1.9006711409395973,
      "grad_norm": 0.09400992840528488,
      "learning_rate": 0.0002429032258064516,
      "loss": 1.3854,
      "step": 708
    },
    {
      "epoch": 1.9033557046979865,
      "grad_norm": 0.10078743100166321,
      "learning_rate": 0.00024282258064516128,
      "loss": 1.4741,
      "step": 709
    },
    {
      "epoch": 1.9060402684563758,
      "grad_norm": 0.0867576152086258,
      "learning_rate": 0.00024274193548387094,
      "loss": 1.551,
      "step": 710
    },
    {
      "epoch": 1.908724832214765,
      "grad_norm": 0.08115433156490326,
      "learning_rate": 0.00024266129032258062,
      "loss": 1.4311,
      "step": 711
    },
    {
      "epoch": 1.9114093959731544,
      "grad_norm": 0.08272423595190048,
      "learning_rate": 0.0002425806451612903,
      "loss": 1.5561,
      "step": 712
    },
    {
      "epoch": 1.9140939597315436,
      "grad_norm": 0.07493527233600616,
      "learning_rate": 0.00024249999999999999,
      "loss": 1.4356,
      "step": 713
    },
    {
      "epoch": 1.916778523489933,
      "grad_norm": 0.08458862453699112,
      "learning_rate": 0.00024241935483870967,
      "loss": 1.4999,
      "step": 714
    },
    {
      "epoch": 1.9194630872483223,
      "grad_norm": 0.08515248447656631,
      "learning_rate": 0.00024233870967741933,
      "loss": 1.5195,
      "step": 715
    },
    {
      "epoch": 1.9221476510067115,
      "grad_norm": 0.08777490258216858,
      "learning_rate": 0.000242258064516129,
      "loss": 1.3342,
      "step": 716
    },
    {
      "epoch": 1.9248322147651007,
      "grad_norm": 0.08317354321479797,
      "learning_rate": 0.0002421774193548387,
      "loss": 1.5275,
      "step": 717
    },
    {
      "epoch": 1.92751677852349,
      "grad_norm": 0.09294366836547852,
      "learning_rate": 0.00024209677419354838,
      "loss": 1.4154,
      "step": 718
    },
    {
      "epoch": 1.9302013422818791,
      "grad_norm": 0.08961651474237442,
      "learning_rate": 0.00024201612903225806,
      "loss": 1.5841,
      "step": 719
    },
    {
      "epoch": 1.9328859060402683,
      "grad_norm": 0.07972785830497742,
      "learning_rate": 0.00024193548387096771,
      "loss": 1.5754,
      "step": 720
    },
    {
      "epoch": 1.9355704697986578,
      "grad_norm": 0.09812812507152557,
      "learning_rate": 0.0002418548387096774,
      "loss": 1.4218,
      "step": 721
    },
    {
      "epoch": 1.938255033557047,
      "grad_norm": 0.08603761345148087,
      "learning_rate": 0.00024177419354838708,
      "loss": 1.4527,
      "step": 722
    },
    {
      "epoch": 1.9409395973154362,
      "grad_norm": 0.08365925401449203,
      "learning_rate": 0.00024169354838709676,
      "loss": 1.4715,
      "step": 723
    },
    {
      "epoch": 1.9436241610738256,
      "grad_norm": 0.0870501697063446,
      "learning_rate": 0.00024161290322580645,
      "loss": 1.5275,
      "step": 724
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 0.08943300694227219,
      "learning_rate": 0.0002415322580645161,
      "loss": 1.4573,
      "step": 725
    },
    {
      "epoch": 1.948993288590604,
      "grad_norm": 0.10525393486022949,
      "learning_rate": 0.0002414516129032258,
      "loss": 1.5167,
      "step": 726
    },
    {
      "epoch": 1.9516778523489933,
      "grad_norm": 0.08279038965702057,
      "learning_rate": 0.00024137096774193547,
      "loss": 1.5411,
      "step": 727
    },
    {
      "epoch": 1.9543624161073825,
      "grad_norm": 0.08388220518827438,
      "learning_rate": 0.00024129032258064515,
      "loss": 1.5911,
      "step": 728
    },
    {
      "epoch": 1.9570469798657717,
      "grad_norm": 0.11171344667673111,
      "learning_rate": 0.00024120967741935484,
      "loss": 1.4478,
      "step": 729
    },
    {
      "epoch": 1.959731543624161,
      "grad_norm": 0.08823715895414352,
      "learning_rate": 0.0002411290322580645,
      "loss": 1.3628,
      "step": 730
    },
    {
      "epoch": 1.9624161073825503,
      "grad_norm": 0.08172259479761124,
      "learning_rate": 0.00024104838709677418,
      "loss": 1.564,
      "step": 731
    },
    {
      "epoch": 1.9651006711409396,
      "grad_norm": 0.07787687331438065,
      "learning_rate": 0.00024096774193548386,
      "loss": 1.507,
      "step": 732
    },
    {
      "epoch": 1.967785234899329,
      "grad_norm": 0.08387961983680725,
      "learning_rate": 0.00024088709677419354,
      "loss": 1.529,
      "step": 733
    },
    {
      "epoch": 1.9704697986577182,
      "grad_norm": 0.08519332110881805,
      "learning_rate": 0.00024080645161290323,
      "loss": 1.626,
      "step": 734
    },
    {
      "epoch": 1.9731543624161074,
      "grad_norm": 0.08785837143659592,
      "learning_rate": 0.00024072580645161286,
      "loss": 1.5056,
      "step": 735
    },
    {
      "epoch": 1.9758389261744966,
      "grad_norm": 0.08728067576885223,
      "learning_rate": 0.00024064516129032254,
      "loss": 1.4839,
      "step": 736
    },
    {
      "epoch": 1.9785234899328858,
      "grad_norm": 0.07799176126718521,
      "learning_rate": 0.00024056451612903225,
      "loss": 1.5936,
      "step": 737
    },
    {
      "epoch": 1.981208053691275,
      "grad_norm": 0.08217117190361023,
      "learning_rate": 0.00024048387096774193,
      "loss": 1.4049,
      "step": 738
    },
    {
      "epoch": 1.9838926174496643,
      "grad_norm": 0.0972566083073616,
      "learning_rate": 0.00024040322580645162,
      "loss": 1.5464,
      "step": 739
    },
    {
      "epoch": 1.9865771812080537,
      "grad_norm": 0.07287867367267609,
      "learning_rate": 0.00024032258064516125,
      "loss": 1.5131,
      "step": 740
    },
    {
      "epoch": 1.989261744966443,
      "grad_norm": 0.08458749949932098,
      "learning_rate": 0.00024024193548387093,
      "loss": 1.4777,
      "step": 741
    },
    {
      "epoch": 1.9919463087248324,
      "grad_norm": 0.09236721694469452,
      "learning_rate": 0.0002401612903225806,
      "loss": 1.4547,
      "step": 742
    },
    {
      "epoch": 1.9946308724832216,
      "grad_norm": 0.09430794417858124,
      "learning_rate": 0.0002400806451612903,
      "loss": 1.5615,
      "step": 743
    },
    {
      "epoch": 1.9973154362416108,
      "grad_norm": 0.0851052850484848,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.594,
      "step": 744
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.08206842094659805,
      "learning_rate": 0.00023991935483870964,
      "loss": 1.5433,
      "step": 745
    },
    {
      "epoch": 2.002684563758389,
      "grad_norm": 0.09030115604400635,
      "learning_rate": 0.00023983870967741932,
      "loss": 1.3979,
      "step": 746
    },
    {
      "epoch": 2.0053691275167784,
      "grad_norm": 0.10071368515491486,
      "learning_rate": 0.000239758064516129,
      "loss": 1.5133,
      "step": 747
    },
    {
      "epoch": 2.0080536912751676,
      "grad_norm": 0.0872480720281601,
      "learning_rate": 0.00023967741935483869,
      "loss": 1.5339,
      "step": 748
    },
    {
      "epoch": 2.010738255033557,
      "grad_norm": 0.08149947971105576,
      "learning_rate": 0.00023959677419354837,
      "loss": 1.4964,
      "step": 749
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 0.10770152509212494,
      "learning_rate": 0.00023951612903225802,
      "loss": 1.3998,
      "step": 750
    },
    {
      "epoch": 2.0161073825503357,
      "grad_norm": 0.08625124394893646,
      "learning_rate": 0.0002394354838709677,
      "loss": 1.4352,
      "step": 751
    },
    {
      "epoch": 2.018791946308725,
      "grad_norm": 0.08750861883163452,
      "learning_rate": 0.0002393548387096774,
      "loss": 1.4942,
      "step": 752
    },
    {
      "epoch": 2.021476510067114,
      "grad_norm": 0.09372452646493912,
      "learning_rate": 0.00023927419354838707,
      "loss": 1.4526,
      "step": 753
    },
    {
      "epoch": 2.0241610738255034,
      "grad_norm": 0.08307544887065887,
      "learning_rate": 0.00023919354838709676,
      "loss": 1.4572,
      "step": 754
    },
    {
      "epoch": 2.0268456375838926,
      "grad_norm": 0.08856522291898727,
      "learning_rate": 0.00023911290322580641,
      "loss": 1.4175,
      "step": 755
    },
    {
      "epoch": 2.029530201342282,
      "grad_norm": 0.0858641117811203,
      "learning_rate": 0.0002390322580645161,
      "loss": 1.4112,
      "step": 756
    },
    {
      "epoch": 2.032214765100671,
      "grad_norm": 0.08026354759931564,
      "learning_rate": 0.00023895161290322578,
      "loss": 1.4276,
      "step": 757
    },
    {
      "epoch": 2.03489932885906,
      "grad_norm": 0.09133680164813995,
      "learning_rate": 0.00023887096774193546,
      "loss": 1.4067,
      "step": 758
    },
    {
      "epoch": 2.03758389261745,
      "grad_norm": 0.08962598443031311,
      "learning_rate": 0.00023879032258064515,
      "loss": 1.4232,
      "step": 759
    },
    {
      "epoch": 2.040268456375839,
      "grad_norm": 0.09235604107379913,
      "learning_rate": 0.0002387096774193548,
      "loss": 1.4918,
      "step": 760
    },
    {
      "epoch": 2.0429530201342283,
      "grad_norm": 0.08633140474557877,
      "learning_rate": 0.0002386290322580645,
      "loss": 1.5052,
      "step": 761
    },
    {
      "epoch": 2.0456375838926175,
      "grad_norm": 0.10183794796466827,
      "learning_rate": 0.00023854838709677417,
      "loss": 1.4127,
      "step": 762
    },
    {
      "epoch": 2.0483221476510067,
      "grad_norm": 0.0903107300400734,
      "learning_rate": 0.00023846774193548385,
      "loss": 1.3788,
      "step": 763
    },
    {
      "epoch": 2.051006711409396,
      "grad_norm": 0.07833247631788254,
      "learning_rate": 0.00023838709677419354,
      "loss": 1.5087,
      "step": 764
    },
    {
      "epoch": 2.053691275167785,
      "grad_norm": 0.08620492368936539,
      "learning_rate": 0.0002383064516129032,
      "loss": 1.4412,
      "step": 765
    },
    {
      "epoch": 2.0563758389261744,
      "grad_norm": 0.0925217941403389,
      "learning_rate": 0.00023822580645161288,
      "loss": 1.4283,
      "step": 766
    },
    {
      "epoch": 2.0590604026845636,
      "grad_norm": 0.08546826988458633,
      "learning_rate": 0.00023814516129032256,
      "loss": 1.5632,
      "step": 767
    },
    {
      "epoch": 2.0617449664429532,
      "grad_norm": 0.07978231459856033,
      "learning_rate": 0.00023806451612903224,
      "loss": 1.3486,
      "step": 768
    },
    {
      "epoch": 2.0644295302013425,
      "grad_norm": 0.10509756207466125,
      "learning_rate": 0.00023798387096774193,
      "loss": 1.3451,
      "step": 769
    },
    {
      "epoch": 2.0671140939597317,
      "grad_norm": 0.08453941345214844,
      "learning_rate": 0.00023790322580645158,
      "loss": 1.5096,
      "step": 770
    },
    {
      "epoch": 2.069798657718121,
      "grad_norm": 0.08776485174894333,
      "learning_rate": 0.00023782258064516127,
      "loss": 1.4422,
      "step": 771
    },
    {
      "epoch": 2.07248322147651,
      "grad_norm": 0.0913689061999321,
      "learning_rate": 0.00023774193548387095,
      "loss": 1.5056,
      "step": 772
    },
    {
      "epoch": 2.0751677852348993,
      "grad_norm": 0.08568405359983444,
      "learning_rate": 0.00023766129032258063,
      "loss": 1.4501,
      "step": 773
    },
    {
      "epoch": 2.0778523489932885,
      "grad_norm": 0.09665822237730026,
      "learning_rate": 0.00023758064516129032,
      "loss": 1.3909,
      "step": 774
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 0.09810124337673187,
      "learning_rate": 0.00023749999999999997,
      "loss": 1.4226,
      "step": 775
    },
    {
      "epoch": 2.083221476510067,
      "grad_norm": 0.08137824386358261,
      "learning_rate": 0.00023741935483870966,
      "loss": 1.5988,
      "step": 776
    },
    {
      "epoch": 2.085906040268456,
      "grad_norm": 0.09620334953069687,
      "learning_rate": 0.00023733870967741934,
      "loss": 1.5285,
      "step": 777
    },
    {
      "epoch": 2.088590604026846,
      "grad_norm": 0.09626442939043045,
      "learning_rate": 0.00023725806451612902,
      "loss": 1.4241,
      "step": 778
    },
    {
      "epoch": 2.091275167785235,
      "grad_norm": 0.09302206337451935,
      "learning_rate": 0.0002371774193548387,
      "loss": 1.3462,
      "step": 779
    },
    {
      "epoch": 2.0939597315436242,
      "grad_norm": 0.08393984287977219,
      "learning_rate": 0.00023709677419354836,
      "loss": 1.53,
      "step": 780
    },
    {
      "epoch": 2.0966442953020135,
      "grad_norm": 0.08414103835821152,
      "learning_rate": 0.00023701612903225805,
      "loss": 1.5329,
      "step": 781
    },
    {
      "epoch": 2.0993288590604027,
      "grad_norm": 0.09157699346542358,
      "learning_rate": 0.00023693548387096773,
      "loss": 1.615,
      "step": 782
    },
    {
      "epoch": 2.102013422818792,
      "grad_norm": 0.10440468043088913,
      "learning_rate": 0.0002368548387096774,
      "loss": 1.3837,
      "step": 783
    },
    {
      "epoch": 2.104697986577181,
      "grad_norm": 0.08683697879314423,
      "learning_rate": 0.0002367741935483871,
      "loss": 1.5652,
      "step": 784
    },
    {
      "epoch": 2.1073825503355703,
      "grad_norm": 0.08382614701986313,
      "learning_rate": 0.00023669354838709675,
      "loss": 1.5073,
      "step": 785
    },
    {
      "epoch": 2.1100671140939595,
      "grad_norm": 0.09385236352682114,
      "learning_rate": 0.00023661290322580643,
      "loss": 1.4231,
      "step": 786
    },
    {
      "epoch": 2.112751677852349,
      "grad_norm": 0.09222894161939621,
      "learning_rate": 0.00023653225806451612,
      "loss": 1.543,
      "step": 787
    },
    {
      "epoch": 2.1154362416107384,
      "grad_norm": 0.08785124868154526,
      "learning_rate": 0.0002364516129032258,
      "loss": 1.4662,
      "step": 788
    },
    {
      "epoch": 2.1181208053691276,
      "grad_norm": 0.0888514295220375,
      "learning_rate": 0.00023637096774193548,
      "loss": 1.4477,
      "step": 789
    },
    {
      "epoch": 2.120805369127517,
      "grad_norm": 0.0836765319108963,
      "learning_rate": 0.00023629032258064514,
      "loss": 1.4225,
      "step": 790
    },
    {
      "epoch": 2.123489932885906,
      "grad_norm": 0.08801455050706863,
      "learning_rate": 0.00023620967741935482,
      "loss": 1.434,
      "step": 791
    },
    {
      "epoch": 2.1261744966442953,
      "grad_norm": 0.0840965211391449,
      "learning_rate": 0.0002361290322580645,
      "loss": 1.6648,
      "step": 792
    },
    {
      "epoch": 2.1288590604026845,
      "grad_norm": 0.09559910744428635,
      "learning_rate": 0.0002360483870967742,
      "loss": 1.5037,
      "step": 793
    },
    {
      "epoch": 2.1315436241610737,
      "grad_norm": 0.09730976819992065,
      "learning_rate": 0.00023596774193548385,
      "loss": 1.4282,
      "step": 794
    },
    {
      "epoch": 2.134228187919463,
      "grad_norm": 0.08785026520490646,
      "learning_rate": 0.00023588709677419353,
      "loss": 1.5448,
      "step": 795
    },
    {
      "epoch": 2.1369127516778526,
      "grad_norm": 0.07480332255363464,
      "learning_rate": 0.00023580645161290321,
      "loss": 1.5137,
      "step": 796
    },
    {
      "epoch": 2.1395973154362418,
      "grad_norm": 0.08709312230348587,
      "learning_rate": 0.0002357258064516129,
      "loss": 1.5067,
      "step": 797
    },
    {
      "epoch": 2.142281879194631,
      "grad_norm": 0.07742933183908463,
      "learning_rate": 0.00023564516129032258,
      "loss": 1.4492,
      "step": 798
    },
    {
      "epoch": 2.14496644295302,
      "grad_norm": 0.08931298553943634,
      "learning_rate": 0.0002355645161290322,
      "loss": 1.4204,
      "step": 799
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 0.08254784345626831,
      "learning_rate": 0.0002354838709677419,
      "loss": 1.4456,
      "step": 800
    },
    {
      "epoch": 2.1503355704697986,
      "grad_norm": 0.09419366717338562,
      "learning_rate": 0.00023540322580645158,
      "loss": 1.3841,
      "step": 801
    },
    {
      "epoch": 2.153020134228188,
      "grad_norm": 0.09360602498054504,
      "learning_rate": 0.0002353225806451613,
      "loss": 1.402,
      "step": 802
    },
    {
      "epoch": 2.155704697986577,
      "grad_norm": 0.0874118059873581,
      "learning_rate": 0.00023524193548387097,
      "loss": 1.507,
      "step": 803
    },
    {
      "epoch": 2.1583892617449663,
      "grad_norm": 0.08322526514530182,
      "learning_rate": 0.0002351612903225806,
      "loss": 1.5855,
      "step": 804
    },
    {
      "epoch": 2.1610738255033555,
      "grad_norm": 0.0936804935336113,
      "learning_rate": 0.00023508064516129028,
      "loss": 1.4445,
      "step": 805
    },
    {
      "epoch": 2.163758389261745,
      "grad_norm": 0.08741152286529541,
      "learning_rate": 0.00023499999999999997,
      "loss": 1.5038,
      "step": 806
    },
    {
      "epoch": 2.1664429530201343,
      "grad_norm": 0.08821528404951096,
      "learning_rate": 0.00023491935483870965,
      "loss": 1.4365,
      "step": 807
    },
    {
      "epoch": 2.1691275167785236,
      "grad_norm": 0.08736789226531982,
      "learning_rate": 0.00023483870967741933,
      "loss": 1.5384,
      "step": 808
    },
    {
      "epoch": 2.1718120805369128,
      "grad_norm": 0.08279349654912949,
      "learning_rate": 0.000234758064516129,
      "loss": 1.4009,
      "step": 809
    },
    {
      "epoch": 2.174496644295302,
      "grad_norm": 0.08148763328790665,
      "learning_rate": 0.00023467741935483867,
      "loss": 1.5698,
      "step": 810
    },
    {
      "epoch": 2.177181208053691,
      "grad_norm": 0.0842680111527443,
      "learning_rate": 0.00023459677419354836,
      "loss": 1.4695,
      "step": 811
    },
    {
      "epoch": 2.1798657718120804,
      "grad_norm": 0.0814262107014656,
      "learning_rate": 0.00023451612903225804,
      "loss": 1.4163,
      "step": 812
    },
    {
      "epoch": 2.1825503355704696,
      "grad_norm": 0.09116240590810776,
      "learning_rate": 0.00023443548387096772,
      "loss": 1.4294,
      "step": 813
    },
    {
      "epoch": 2.185234899328859,
      "grad_norm": 0.09636760503053665,
      "learning_rate": 0.00023435483870967738,
      "loss": 1.4725,
      "step": 814
    },
    {
      "epoch": 2.1879194630872485,
      "grad_norm": 0.09156286716461182,
      "learning_rate": 0.00023427419354838706,
      "loss": 1.6083,
      "step": 815
    },
    {
      "epoch": 2.1906040268456377,
      "grad_norm": 0.0851573795080185,
      "learning_rate": 0.00023419354838709674,
      "loss": 1.5113,
      "step": 816
    },
    {
      "epoch": 2.193288590604027,
      "grad_norm": 0.10651577264070511,
      "learning_rate": 0.00023411290322580643,
      "loss": 1.3971,
      "step": 817
    },
    {
      "epoch": 2.195973154362416,
      "grad_norm": 0.09606636315584183,
      "learning_rate": 0.0002340322580645161,
      "loss": 1.438,
      "step": 818
    },
    {
      "epoch": 2.1986577181208053,
      "grad_norm": 0.10030185431241989,
      "learning_rate": 0.00023395161290322577,
      "loss": 1.3895,
      "step": 819
    },
    {
      "epoch": 2.2013422818791946,
      "grad_norm": 0.09252124279737473,
      "learning_rate": 0.00023387096774193545,
      "loss": 1.4924,
      "step": 820
    },
    {
      "epoch": 2.2040268456375838,
      "grad_norm": 0.09607945382595062,
      "learning_rate": 0.00023379032258064513,
      "loss": 1.5175,
      "step": 821
    },
    {
      "epoch": 2.206711409395973,
      "grad_norm": 0.07927294075489044,
      "learning_rate": 0.00023370967741935482,
      "loss": 1.4973,
      "step": 822
    },
    {
      "epoch": 2.209395973154362,
      "grad_norm": 0.07718931138515472,
      "learning_rate": 0.0002336290322580645,
      "loss": 1.5111,
      "step": 823
    },
    {
      "epoch": 2.212080536912752,
      "grad_norm": 0.0960279032588005,
      "learning_rate": 0.00023354838709677416,
      "loss": 1.5288,
      "step": 824
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 0.08757106959819794,
      "learning_rate": 0.00023346774193548384,
      "loss": 1.4067,
      "step": 825
    },
    {
      "epoch": 2.2174496644295303,
      "grad_norm": 0.08062154054641724,
      "learning_rate": 0.00023338709677419352,
      "loss": 1.4591,
      "step": 826
    },
    {
      "epoch": 2.2201342281879195,
      "grad_norm": 0.08543175458908081,
      "learning_rate": 0.0002333064516129032,
      "loss": 1.4673,
      "step": 827
    },
    {
      "epoch": 2.2228187919463087,
      "grad_norm": 0.0992998406291008,
      "learning_rate": 0.0002332258064516129,
      "loss": 1.3014,
      "step": 828
    },
    {
      "epoch": 2.225503355704698,
      "grad_norm": 0.0908343642950058,
      "learning_rate": 0.00023314516129032255,
      "loss": 1.4373,
      "step": 829
    },
    {
      "epoch": 2.228187919463087,
      "grad_norm": 0.0802801176905632,
      "learning_rate": 0.00023306451612903223,
      "loss": 1.5304,
      "step": 830
    },
    {
      "epoch": 2.2308724832214764,
      "grad_norm": 0.08610609173774719,
      "learning_rate": 0.0002329838709677419,
      "loss": 1.5394,
      "step": 831
    },
    {
      "epoch": 2.2335570469798656,
      "grad_norm": 0.08002908527851105,
      "learning_rate": 0.0002329032258064516,
      "loss": 1.6249,
      "step": 832
    },
    {
      "epoch": 2.2362416107382552,
      "grad_norm": 0.0840684100985527,
      "learning_rate": 0.00023282258064516128,
      "loss": 1.4054,
      "step": 833
    },
    {
      "epoch": 2.2389261744966444,
      "grad_norm": 0.098082534968853,
      "learning_rate": 0.00023274193548387094,
      "loss": 1.3335,
      "step": 834
    },
    {
      "epoch": 2.2416107382550337,
      "grad_norm": 0.0890943631529808,
      "learning_rate": 0.00023266129032258062,
      "loss": 1.5596,
      "step": 835
    },
    {
      "epoch": 2.244295302013423,
      "grad_norm": 0.07721418142318726,
      "learning_rate": 0.0002325806451612903,
      "loss": 1.5122,
      "step": 836
    },
    {
      "epoch": 2.246979865771812,
      "grad_norm": 0.0869031697511673,
      "learning_rate": 0.00023249999999999999,
      "loss": 1.5016,
      "step": 837
    },
    {
      "epoch": 2.2496644295302013,
      "grad_norm": 0.09237247705459595,
      "learning_rate": 0.00023241935483870967,
      "loss": 1.5186,
      "step": 838
    },
    {
      "epoch": 2.2523489932885905,
      "grad_norm": 0.08876364678144455,
      "learning_rate": 0.00023233870967741933,
      "loss": 1.5284,
      "step": 839
    },
    {
      "epoch": 2.2550335570469797,
      "grad_norm": 0.0941171944141388,
      "learning_rate": 0.000232258064516129,
      "loss": 1.4302,
      "step": 840
    },
    {
      "epoch": 2.257718120805369,
      "grad_norm": 0.08314280211925507,
      "learning_rate": 0.0002321774193548387,
      "loss": 1.5491,
      "step": 841
    },
    {
      "epoch": 2.2604026845637586,
      "grad_norm": 0.09687700122594833,
      "learning_rate": 0.00023209677419354838,
      "loss": 1.5731,
      "step": 842
    },
    {
      "epoch": 2.263087248322148,
      "grad_norm": 0.08164570480585098,
      "learning_rate": 0.00023201612903225806,
      "loss": 1.5513,
      "step": 843
    },
    {
      "epoch": 2.265771812080537,
      "grad_norm": 0.07734667509794235,
      "learning_rate": 0.00023193548387096772,
      "loss": 1.4295,
      "step": 844
    },
    {
      "epoch": 2.2684563758389262,
      "grad_norm": 0.09513659030199051,
      "learning_rate": 0.0002318548387096774,
      "loss": 1.59,
      "step": 845
    },
    {
      "epoch": 2.2711409395973154,
      "grad_norm": 0.08959292620420456,
      "learning_rate": 0.00023177419354838708,
      "loss": 1.4764,
      "step": 846
    },
    {
      "epoch": 2.2738255033557047,
      "grad_norm": 0.08779247850179672,
      "learning_rate": 0.00023169354838709677,
      "loss": 1.3911,
      "step": 847
    },
    {
      "epoch": 2.276510067114094,
      "grad_norm": 0.07711796462535858,
      "learning_rate": 0.00023161290322580645,
      "loss": 1.6394,
      "step": 848
    },
    {
      "epoch": 2.279194630872483,
      "grad_norm": 0.0865427702665329,
      "learning_rate": 0.0002315322580645161,
      "loss": 1.5085,
      "step": 849
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 0.07653950154781342,
      "learning_rate": 0.0002314516129032258,
      "loss": 1.5412,
      "step": 850
    },
    {
      "epoch": 2.284563758389262,
      "grad_norm": 0.11203234642744064,
      "learning_rate": 0.00023137096774193547,
      "loss": 1.4732,
      "step": 851
    },
    {
      "epoch": 2.287248322147651,
      "grad_norm": 0.08334551006555557,
      "learning_rate": 0.00023129032258064516,
      "loss": 1.328,
      "step": 852
    },
    {
      "epoch": 2.2899328859060404,
      "grad_norm": 0.0790187194943428,
      "learning_rate": 0.00023120967741935484,
      "loss": 1.5718,
      "step": 853
    },
    {
      "epoch": 2.2926174496644296,
      "grad_norm": 0.07808245718479156,
      "learning_rate": 0.0002311290322580645,
      "loss": 1.5688,
      "step": 854
    },
    {
      "epoch": 2.295302013422819,
      "grad_norm": 0.09260307252407074,
      "learning_rate": 0.00023104838709677418,
      "loss": 1.4163,
      "step": 855
    },
    {
      "epoch": 2.297986577181208,
      "grad_norm": 0.07863125205039978,
      "learning_rate": 0.00023096774193548386,
      "loss": 1.5202,
      "step": 856
    },
    {
      "epoch": 2.3006711409395972,
      "grad_norm": 0.07845333963632584,
      "learning_rate": 0.00023088709677419354,
      "loss": 1.5221,
      "step": 857
    },
    {
      "epoch": 2.3033557046979865,
      "grad_norm": 0.0830414816737175,
      "learning_rate": 0.00023080645161290323,
      "loss": 1.482,
      "step": 858
    },
    {
      "epoch": 2.3060402684563757,
      "grad_norm": 0.10213404893875122,
      "learning_rate": 0.00023072580645161288,
      "loss": 1.5218,
      "step": 859
    },
    {
      "epoch": 2.3087248322147653,
      "grad_norm": 0.0966411828994751,
      "learning_rate": 0.00023064516129032257,
      "loss": 1.509,
      "step": 860
    },
    {
      "epoch": 2.3114093959731545,
      "grad_norm": 0.0864156186580658,
      "learning_rate": 0.00023056451612903225,
      "loss": 1.3567,
      "step": 861
    },
    {
      "epoch": 2.3140939597315437,
      "grad_norm": 0.08335927873849869,
      "learning_rate": 0.00023048387096774193,
      "loss": 1.5583,
      "step": 862
    },
    {
      "epoch": 2.316778523489933,
      "grad_norm": 0.08600480109453201,
      "learning_rate": 0.00023040322580645162,
      "loss": 1.4839,
      "step": 863
    },
    {
      "epoch": 2.319463087248322,
      "grad_norm": 0.08596469461917877,
      "learning_rate": 0.00023032258064516125,
      "loss": 1.5625,
      "step": 864
    },
    {
      "epoch": 2.3221476510067114,
      "grad_norm": 0.0921434611082077,
      "learning_rate": 0.00023024193548387093,
      "loss": 1.5129,
      "step": 865
    },
    {
      "epoch": 2.3248322147651006,
      "grad_norm": 0.07456234097480774,
      "learning_rate": 0.00023016129032258064,
      "loss": 1.6389,
      "step": 866
    },
    {
      "epoch": 2.32751677852349,
      "grad_norm": 0.09101565182209015,
      "learning_rate": 0.00023008064516129032,
      "loss": 1.4682,
      "step": 867
    },
    {
      "epoch": 2.330201342281879,
      "grad_norm": 0.08876325935125351,
      "learning_rate": 0.00023,
      "loss": 1.4433,
      "step": 868
    },
    {
      "epoch": 2.3328859060402687,
      "grad_norm": 0.08575129508972168,
      "learning_rate": 0.00022991935483870964,
      "loss": 1.4579,
      "step": 869
    },
    {
      "epoch": 2.335570469798658,
      "grad_norm": 0.09597673267126083,
      "learning_rate": 0.00022983870967741932,
      "loss": 1.4498,
      "step": 870
    },
    {
      "epoch": 2.338255033557047,
      "grad_norm": 0.08721085637807846,
      "learning_rate": 0.000229758064516129,
      "loss": 1.4649,
      "step": 871
    },
    {
      "epoch": 2.3409395973154363,
      "grad_norm": 0.0908501073718071,
      "learning_rate": 0.00022967741935483869,
      "loss": 1.4988,
      "step": 872
    },
    {
      "epoch": 2.3436241610738255,
      "grad_norm": 0.09127634763717651,
      "learning_rate": 0.00022959677419354837,
      "loss": 1.5803,
      "step": 873
    },
    {
      "epoch": 2.3463087248322148,
      "grad_norm": 0.08912672102451324,
      "learning_rate": 0.00022951612903225803,
      "loss": 1.5838,
      "step": 874
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 0.08232990652322769,
      "learning_rate": 0.0002294354838709677,
      "loss": 1.4904,
      "step": 875
    },
    {
      "epoch": 2.351677852348993,
      "grad_norm": 0.08339449763298035,
      "learning_rate": 0.0002293548387096774,
      "loss": 1.582,
      "step": 876
    },
    {
      "epoch": 2.3543624161073824,
      "grad_norm": 0.09932902455329895,
      "learning_rate": 0.00022927419354838708,
      "loss": 1.4595,
      "step": 877
    },
    {
      "epoch": 2.357046979865772,
      "grad_norm": 0.08258272707462311,
      "learning_rate": 0.00022919354838709676,
      "loss": 1.4726,
      "step": 878
    },
    {
      "epoch": 2.3597315436241613,
      "grad_norm": 0.09432249516248703,
      "learning_rate": 0.00022911290322580642,
      "loss": 1.4542,
      "step": 879
    },
    {
      "epoch": 2.3624161073825505,
      "grad_norm": 0.09245743602514267,
      "learning_rate": 0.0002290322580645161,
      "loss": 1.3525,
      "step": 880
    },
    {
      "epoch": 2.3651006711409397,
      "grad_norm": 0.09153271466493607,
      "learning_rate": 0.00022895161290322578,
      "loss": 1.349,
      "step": 881
    },
    {
      "epoch": 2.367785234899329,
      "grad_norm": 0.08509667217731476,
      "learning_rate": 0.00022887096774193547,
      "loss": 1.4926,
      "step": 882
    },
    {
      "epoch": 2.370469798657718,
      "grad_norm": 0.08394371718168259,
      "learning_rate": 0.00022879032258064515,
      "loss": 1.5956,
      "step": 883
    },
    {
      "epoch": 2.3731543624161073,
      "grad_norm": 0.09343361854553223,
      "learning_rate": 0.0002287096774193548,
      "loss": 1.5677,
      "step": 884
    },
    {
      "epoch": 2.3758389261744965,
      "grad_norm": 0.08109187334775925,
      "learning_rate": 0.0002286290322580645,
      "loss": 1.4324,
      "step": 885
    },
    {
      "epoch": 2.3785234899328858,
      "grad_norm": 0.08091995865106583,
      "learning_rate": 0.00022854838709677417,
      "loss": 1.6659,
      "step": 886
    },
    {
      "epoch": 2.3812080536912754,
      "grad_norm": 0.08532845228910446,
      "learning_rate": 0.00022846774193548385,
      "loss": 1.4064,
      "step": 887
    },
    {
      "epoch": 2.383892617449664,
      "grad_norm": 0.08435812592506409,
      "learning_rate": 0.0002283870967741935,
      "loss": 1.506,
      "step": 888
    },
    {
      "epoch": 2.386577181208054,
      "grad_norm": 0.09997724741697311,
      "learning_rate": 0.0002283064516129032,
      "loss": 1.506,
      "step": 889
    },
    {
      "epoch": 2.389261744966443,
      "grad_norm": 0.08553646504878998,
      "learning_rate": 0.00022822580645161288,
      "loss": 1.459,
      "step": 890
    },
    {
      "epoch": 2.3919463087248323,
      "grad_norm": 0.07783782482147217,
      "learning_rate": 0.00022814516129032256,
      "loss": 1.504,
      "step": 891
    },
    {
      "epoch": 2.3946308724832215,
      "grad_norm": 0.09455695003271103,
      "learning_rate": 0.00022806451612903224,
      "loss": 1.5132,
      "step": 892
    },
    {
      "epoch": 2.3973154362416107,
      "grad_norm": 0.08364982903003693,
      "learning_rate": 0.0002279838709677419,
      "loss": 1.4483,
      "step": 893
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.10448480397462845,
      "learning_rate": 0.00022790322580645158,
      "loss": 1.4104,
      "step": 894
    },
    {
      "epoch": 2.402684563758389,
      "grad_norm": 0.08845677971839905,
      "learning_rate": 0.00022782258064516127,
      "loss": 1.4085,
      "step": 895
    },
    {
      "epoch": 2.4053691275167783,
      "grad_norm": 0.093487948179245,
      "learning_rate": 0.00022774193548387095,
      "loss": 1.5121,
      "step": 896
    },
    {
      "epoch": 2.4080536912751676,
      "grad_norm": 0.08015599846839905,
      "learning_rate": 0.00022766129032258063,
      "loss": 1.4562,
      "step": 897
    },
    {
      "epoch": 2.410738255033557,
      "grad_norm": 0.08803407847881317,
      "learning_rate": 0.0002275806451612903,
      "loss": 1.4406,
      "step": 898
    },
    {
      "epoch": 2.4134228187919464,
      "grad_norm": 0.07748140394687653,
      "learning_rate": 0.00022749999999999997,
      "loss": 1.3827,
      "step": 899
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 0.0761907696723938,
      "learning_rate": 0.00022741935483870966,
      "loss": 1.4443,
      "step": 900
    },
    {
      "epoch": 2.418791946308725,
      "grad_norm": 0.09783027321100235,
      "learning_rate": 0.00022733870967741934,
      "loss": 1.5676,
      "step": 901
    },
    {
      "epoch": 2.421476510067114,
      "grad_norm": 0.08380766957998276,
      "learning_rate": 0.00022725806451612902,
      "loss": 1.4814,
      "step": 902
    },
    {
      "epoch": 2.4241610738255033,
      "grad_norm": 0.07874024659395218,
      "learning_rate": 0.00022717741935483868,
      "loss": 1.5176,
      "step": 903
    },
    {
      "epoch": 2.4268456375838925,
      "grad_norm": 0.08396494388580322,
      "learning_rate": 0.00022709677419354836,
      "loss": 1.5943,
      "step": 904
    },
    {
      "epoch": 2.4295302013422817,
      "grad_norm": 0.09837019443511963,
      "learning_rate": 0.00022701612903225805,
      "loss": 1.4257,
      "step": 905
    },
    {
      "epoch": 2.432214765100671,
      "grad_norm": 0.09299252182245255,
      "learning_rate": 0.00022693548387096773,
      "loss": 1.4416,
      "step": 906
    },
    {
      "epoch": 2.4348993288590606,
      "grad_norm": 0.113876111805439,
      "learning_rate": 0.0002268548387096774,
      "loss": 1.6458,
      "step": 907
    },
    {
      "epoch": 2.43758389261745,
      "grad_norm": 0.08710024505853653,
      "learning_rate": 0.00022677419354838707,
      "loss": 1.4744,
      "step": 908
    },
    {
      "epoch": 2.440268456375839,
      "grad_norm": 0.09245907515287399,
      "learning_rate": 0.00022669354838709675,
      "loss": 1.4785,
      "step": 909
    },
    {
      "epoch": 2.442953020134228,
      "grad_norm": 0.09541483968496323,
      "learning_rate": 0.00022661290322580644,
      "loss": 1.4942,
      "step": 910
    },
    {
      "epoch": 2.4456375838926174,
      "grad_norm": 0.08780492842197418,
      "learning_rate": 0.00022653225806451612,
      "loss": 1.5031,
      "step": 911
    },
    {
      "epoch": 2.4483221476510066,
      "grad_norm": 0.09420710802078247,
      "learning_rate": 0.0002264516129032258,
      "loss": 1.3978,
      "step": 912
    },
    {
      "epoch": 2.451006711409396,
      "grad_norm": 0.07613157480955124,
      "learning_rate": 0.00022637096774193546,
      "loss": 1.4509,
      "step": 913
    },
    {
      "epoch": 2.453691275167785,
      "grad_norm": 0.09066163748502731,
      "learning_rate": 0.00022629032258064514,
      "loss": 1.3948,
      "step": 914
    },
    {
      "epoch": 2.4563758389261743,
      "grad_norm": 0.0844423919916153,
      "learning_rate": 0.00022620967741935483,
      "loss": 1.5997,
      "step": 915
    },
    {
      "epoch": 2.459060402684564,
      "grad_norm": 0.10111954063177109,
      "learning_rate": 0.0002261290322580645,
      "loss": 1.4635,
      "step": 916
    },
    {
      "epoch": 2.461744966442953,
      "grad_norm": 0.08752544969320297,
      "learning_rate": 0.0002260483870967742,
      "loss": 1.4223,
      "step": 917
    },
    {
      "epoch": 2.4644295302013424,
      "grad_norm": 0.08137952536344528,
      "learning_rate": 0.00022596774193548385,
      "loss": 1.4777,
      "step": 918
    },
    {
      "epoch": 2.4671140939597316,
      "grad_norm": 0.0910765528678894,
      "learning_rate": 0.00022588709677419353,
      "loss": 1.5969,
      "step": 919
    },
    {
      "epoch": 2.469798657718121,
      "grad_norm": 0.092568539083004,
      "learning_rate": 0.00022580645161290321,
      "loss": 1.476,
      "step": 920
    },
    {
      "epoch": 2.47248322147651,
      "grad_norm": 0.08184246718883514,
      "learning_rate": 0.0002257258064516129,
      "loss": 1.5034,
      "step": 921
    },
    {
      "epoch": 2.475167785234899,
      "grad_norm": 0.08067397773265839,
      "learning_rate": 0.00022564516129032258,
      "loss": 1.4359,
      "step": 922
    },
    {
      "epoch": 2.4778523489932884,
      "grad_norm": 0.09017632901668549,
      "learning_rate": 0.00022556451612903224,
      "loss": 1.4523,
      "step": 923
    },
    {
      "epoch": 2.4805369127516776,
      "grad_norm": 0.0869731605052948,
      "learning_rate": 0.00022548387096774192,
      "loss": 1.6072,
      "step": 924
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 0.09450674802064896,
      "learning_rate": 0.0002254032258064516,
      "loss": 1.422,
      "step": 925
    },
    {
      "epoch": 2.4859060402684565,
      "grad_norm": 0.08716355264186859,
      "learning_rate": 0.0002253225806451613,
      "loss": 1.5514,
      "step": 926
    },
    {
      "epoch": 2.4885906040268457,
      "grad_norm": 0.08617899566888809,
      "learning_rate": 0.00022524193548387097,
      "loss": 1.5557,
      "step": 927
    },
    {
      "epoch": 2.491275167785235,
      "grad_norm": 0.09424548596143723,
      "learning_rate": 0.0002251612903225806,
      "loss": 1.3892,
      "step": 928
    },
    {
      "epoch": 2.493959731543624,
      "grad_norm": 0.09071370959281921,
      "learning_rate": 0.00022508064516129028,
      "loss": 1.6072,
      "step": 929
    },
    {
      "epoch": 2.4966442953020134,
      "grad_norm": 0.09200166910886765,
      "learning_rate": 0.000225,
      "loss": 1.5096,
      "step": 930
    },
    {
      "epoch": 2.4993288590604026,
      "grad_norm": 0.08605103194713593,
      "learning_rate": 0.00022491935483870968,
      "loss": 1.5028,
      "step": 931
    },
    {
      "epoch": 2.502013422818792,
      "grad_norm": 0.08926864713430405,
      "learning_rate": 0.00022483870967741936,
      "loss": 1.4707,
      "step": 932
    },
    {
      "epoch": 2.504697986577181,
      "grad_norm": 0.08176037669181824,
      "learning_rate": 0.000224758064516129,
      "loss": 1.6109,
      "step": 933
    },
    {
      "epoch": 2.5073825503355707,
      "grad_norm": 0.08624782413244247,
      "learning_rate": 0.00022467741935483867,
      "loss": 1.4495,
      "step": 934
    },
    {
      "epoch": 2.51006711409396,
      "grad_norm": 0.079141765832901,
      "learning_rate": 0.00022459677419354836,
      "loss": 1.6172,
      "step": 935
    },
    {
      "epoch": 2.512751677852349,
      "grad_norm": 0.08175493031740189,
      "learning_rate": 0.00022451612903225804,
      "loss": 1.4734,
      "step": 936
    },
    {
      "epoch": 2.5154362416107383,
      "grad_norm": 0.09074512124061584,
      "learning_rate": 0.00022443548387096772,
      "loss": 1.4466,
      "step": 937
    },
    {
      "epoch": 2.5181208053691275,
      "grad_norm": 0.09854232519865036,
      "learning_rate": 0.00022435483870967738,
      "loss": 1.452,
      "step": 938
    },
    {
      "epoch": 2.5208053691275167,
      "grad_norm": 0.07950346916913986,
      "learning_rate": 0.00022427419354838706,
      "loss": 1.4945,
      "step": 939
    },
    {
      "epoch": 2.523489932885906,
      "grad_norm": 0.08199293911457062,
      "learning_rate": 0.00022419354838709675,
      "loss": 1.5496,
      "step": 940
    },
    {
      "epoch": 2.526174496644295,
      "grad_norm": 0.09355813264846802,
      "learning_rate": 0.00022411290322580643,
      "loss": 1.396,
      "step": 941
    },
    {
      "epoch": 2.5288590604026844,
      "grad_norm": 0.0885724201798439,
      "learning_rate": 0.0002240322580645161,
      "loss": 1.4782,
      "step": 942
    },
    {
      "epoch": 2.531543624161074,
      "grad_norm": 0.08874034136533737,
      "learning_rate": 0.00022395161290322577,
      "loss": 1.4863,
      "step": 943
    },
    {
      "epoch": 2.5342281879194632,
      "grad_norm": 0.08123042434453964,
      "learning_rate": 0.00022387096774193545,
      "loss": 1.4563,
      "step": 944
    },
    {
      "epoch": 2.5369127516778525,
      "grad_norm": 0.08342399448156357,
      "learning_rate": 0.00022379032258064514,
      "loss": 1.4509,
      "step": 945
    },
    {
      "epoch": 2.5395973154362417,
      "grad_norm": 0.08849674463272095,
      "learning_rate": 0.00022370967741935482,
      "loss": 1.4255,
      "step": 946
    },
    {
      "epoch": 2.542281879194631,
      "grad_norm": 0.08800170570611954,
      "learning_rate": 0.0002236290322580645,
      "loss": 1.4237,
      "step": 947
    },
    {
      "epoch": 2.54496644295302,
      "grad_norm": 0.09199860692024231,
      "learning_rate": 0.00022354838709677416,
      "loss": 1.4226,
      "step": 948
    },
    {
      "epoch": 2.5476510067114093,
      "grad_norm": 0.08800637722015381,
      "learning_rate": 0.00022346774193548384,
      "loss": 1.5574,
      "step": 949
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 0.0791301429271698,
      "learning_rate": 0.00022338709677419352,
      "loss": 1.438,
      "step": 950
    },
    {
      "epoch": 2.5530201342281877,
      "grad_norm": 0.08577868342399597,
      "learning_rate": 0.0002233064516129032,
      "loss": 1.5964,
      "step": 951
    },
    {
      "epoch": 2.5557046979865774,
      "grad_norm": 0.09390134364366531,
      "learning_rate": 0.0002232258064516129,
      "loss": 1.4851,
      "step": 952
    },
    {
      "epoch": 2.558389261744966,
      "grad_norm": 0.09539094567298889,
      "learning_rate": 0.00022314516129032255,
      "loss": 1.5473,
      "step": 953
    },
    {
      "epoch": 2.561073825503356,
      "grad_norm": 0.12471882998943329,
      "learning_rate": 0.00022306451612903223,
      "loss": 1.4393,
      "step": 954
    },
    {
      "epoch": 2.563758389261745,
      "grad_norm": 0.08088933676481247,
      "learning_rate": 0.00022298387096774191,
      "loss": 1.5295,
      "step": 955
    },
    {
      "epoch": 2.5664429530201343,
      "grad_norm": 0.08776025474071503,
      "learning_rate": 0.0002229032258064516,
      "loss": 1.6034,
      "step": 956
    },
    {
      "epoch": 2.5691275167785235,
      "grad_norm": 0.08139346539974213,
      "learning_rate": 0.00022282258064516128,
      "loss": 1.4629,
      "step": 957
    },
    {
      "epoch": 2.5718120805369127,
      "grad_norm": 0.08988788723945618,
      "learning_rate": 0.00022274193548387094,
      "loss": 1.5171,
      "step": 958
    },
    {
      "epoch": 2.574496644295302,
      "grad_norm": 0.0800076276063919,
      "learning_rate": 0.00022266129032258062,
      "loss": 1.3463,
      "step": 959
    },
    {
      "epoch": 2.577181208053691,
      "grad_norm": 0.09367721527814865,
      "learning_rate": 0.0002225806451612903,
      "loss": 1.4643,
      "step": 960
    },
    {
      "epoch": 2.5798657718120808,
      "grad_norm": 0.10983839631080627,
      "learning_rate": 0.0002225,
      "loss": 1.4719,
      "step": 961
    },
    {
      "epoch": 2.5825503355704695,
      "grad_norm": 0.08419106155633926,
      "learning_rate": 0.00022241935483870967,
      "loss": 1.4755,
      "step": 962
    },
    {
      "epoch": 2.585234899328859,
      "grad_norm": 0.09300404787063599,
      "learning_rate": 0.00022233870967741933,
      "loss": 1.5736,
      "step": 963
    },
    {
      "epoch": 2.5879194630872484,
      "grad_norm": 0.08311194181442261,
      "learning_rate": 0.000222258064516129,
      "loss": 1.4678,
      "step": 964
    },
    {
      "epoch": 2.5906040268456376,
      "grad_norm": 0.09682239592075348,
      "learning_rate": 0.0002221774193548387,
      "loss": 1.6101,
      "step": 965
    },
    {
      "epoch": 2.593288590604027,
      "grad_norm": 0.08770478516817093,
      "learning_rate": 0.00022209677419354838,
      "loss": 1.3879,
      "step": 966
    },
    {
      "epoch": 2.595973154362416,
      "grad_norm": 0.0891122967004776,
      "learning_rate": 0.00022201612903225806,
      "loss": 1.4849,
      "step": 967
    },
    {
      "epoch": 2.5986577181208053,
      "grad_norm": 0.08567680418491364,
      "learning_rate": 0.00022193548387096772,
      "loss": 1.5691,
      "step": 968
    },
    {
      "epoch": 2.6013422818791945,
      "grad_norm": 0.08758200705051422,
      "learning_rate": 0.0002218548387096774,
      "loss": 1.4194,
      "step": 969
    },
    {
      "epoch": 2.604026845637584,
      "grad_norm": 0.08428730070590973,
      "learning_rate": 0.00022177419354838708,
      "loss": 1.4145,
      "step": 970
    },
    {
      "epoch": 2.606711409395973,
      "grad_norm": 0.08096092939376831,
      "learning_rate": 0.00022169354838709677,
      "loss": 1.4977,
      "step": 971
    },
    {
      "epoch": 2.6093959731543626,
      "grad_norm": 0.08560066670179367,
      "learning_rate": 0.00022161290322580645,
      "loss": 1.4657,
      "step": 972
    },
    {
      "epoch": 2.6120805369127518,
      "grad_norm": 0.08817654848098755,
      "learning_rate": 0.0002215322580645161,
      "loss": 1.438,
      "step": 973
    },
    {
      "epoch": 2.614765100671141,
      "grad_norm": 0.08631043881177902,
      "learning_rate": 0.0002214516129032258,
      "loss": 1.4758,
      "step": 974
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 0.08669862896203995,
      "learning_rate": 0.00022137096774193547,
      "loss": 1.4853,
      "step": 975
    },
    {
      "epoch": 2.6201342281879194,
      "grad_norm": 0.09562020003795624,
      "learning_rate": 0.00022129032258064516,
      "loss": 1.456,
      "step": 976
    },
    {
      "epoch": 2.6228187919463086,
      "grad_norm": 0.08842074126005173,
      "learning_rate": 0.0002212096774193548,
      "loss": 1.5244,
      "step": 977
    },
    {
      "epoch": 2.625503355704698,
      "grad_norm": 0.08614587783813477,
      "learning_rate": 0.0002211290322580645,
      "loss": 1.513,
      "step": 978
    },
    {
      "epoch": 2.6281879194630875,
      "grad_norm": 0.08791445940732956,
      "learning_rate": 0.00022104838709677418,
      "loss": 1.4566,
      "step": 979
    },
    {
      "epoch": 2.6308724832214763,
      "grad_norm": 0.08411787450313568,
      "learning_rate": 0.00022096774193548386,
      "loss": 1.4746,
      "step": 980
    },
    {
      "epoch": 2.633557046979866,
      "grad_norm": 0.08384998142719269,
      "learning_rate": 0.00022088709677419355,
      "loss": 1.5455,
      "step": 981
    },
    {
      "epoch": 2.636241610738255,
      "grad_norm": 0.0866149514913559,
      "learning_rate": 0.0002208064516129032,
      "loss": 1.5127,
      "step": 982
    },
    {
      "epoch": 2.6389261744966444,
      "grad_norm": 0.0849229171872139,
      "learning_rate": 0.00022072580645161288,
      "loss": 1.429,
      "step": 983
    },
    {
      "epoch": 2.6416107382550336,
      "grad_norm": 0.07954329252243042,
      "learning_rate": 0.00022064516129032257,
      "loss": 1.4733,
      "step": 984
    },
    {
      "epoch": 2.6442953020134228,
      "grad_norm": 0.09968812763690948,
      "learning_rate": 0.00022056451612903225,
      "loss": 1.4586,
      "step": 985
    },
    {
      "epoch": 2.646979865771812,
      "grad_norm": 0.08810890465974808,
      "learning_rate": 0.00022048387096774193,
      "loss": 1.6389,
      "step": 986
    },
    {
      "epoch": 2.649664429530201,
      "grad_norm": 0.0848773866891861,
      "learning_rate": 0.0002204032258064516,
      "loss": 1.4602,
      "step": 987
    },
    {
      "epoch": 2.652348993288591,
      "grad_norm": 0.08418278396129608,
      "learning_rate": 0.00022032258064516127,
      "loss": 1.4487,
      "step": 988
    },
    {
      "epoch": 2.6550335570469796,
      "grad_norm": 0.09299881756305695,
      "learning_rate": 0.00022024193548387096,
      "loss": 1.4506,
      "step": 989
    },
    {
      "epoch": 2.6577181208053693,
      "grad_norm": 0.09063652157783508,
      "learning_rate": 0.00022016129032258064,
      "loss": 1.4211,
      "step": 990
    },
    {
      "epoch": 2.6604026845637585,
      "grad_norm": 0.09206497669219971,
      "learning_rate": 0.00022008064516129032,
      "loss": 1.3962,
      "step": 991
    },
    {
      "epoch": 2.6630872483221477,
      "grad_norm": 0.08066455274820328,
      "learning_rate": 0.00021999999999999995,
      "loss": 1.7269,
      "step": 992
    },
    {
      "epoch": 2.665771812080537,
      "grad_norm": 0.09097977727651596,
      "learning_rate": 0.00021991935483870964,
      "loss": 1.4337,
      "step": 993
    },
    {
      "epoch": 2.668456375838926,
      "grad_norm": 0.08842434734106064,
      "learning_rate": 0.00021983870967741932,
      "loss": 1.5463,
      "step": 994
    },
    {
      "epoch": 2.6711409395973154,
      "grad_norm": 0.08019118756055832,
      "learning_rate": 0.00021975806451612903,
      "loss": 1.4676,
      "step": 995
    },
    {
      "epoch": 2.6738255033557046,
      "grad_norm": 0.09642674028873444,
      "learning_rate": 0.00021967741935483871,
      "loss": 1.3596,
      "step": 996
    },
    {
      "epoch": 2.6765100671140942,
      "grad_norm": 0.08530162274837494,
      "learning_rate": 0.00021959677419354834,
      "loss": 1.4756,
      "step": 997
    },
    {
      "epoch": 2.679194630872483,
      "grad_norm": 0.09195952862501144,
      "learning_rate": 0.00021951612903225803,
      "loss": 1.4014,
      "step": 998
    },
    {
      "epoch": 2.6818791946308727,
      "grad_norm": 0.09151583909988403,
      "learning_rate": 0.0002194354838709677,
      "loss": 1.5371,
      "step": 999
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 0.08751571178436279,
      "learning_rate": 0.0002193548387096774,
      "loss": 1.4949,
      "step": 1000
    },
    {
      "epoch": 2.687248322147651,
      "grad_norm": 0.09465526789426804,
      "learning_rate": 0.00021927419354838708,
      "loss": 1.4796,
      "step": 1001
    },
    {
      "epoch": 2.6899328859060403,
      "grad_norm": 0.09672568738460541,
      "learning_rate": 0.00021919354838709673,
      "loss": 1.561,
      "step": 1002
    },
    {
      "epoch": 2.6926174496644295,
      "grad_norm": 0.088022880256176,
      "learning_rate": 0.00021911290322580642,
      "loss": 1.5967,
      "step": 1003
    },
    {
      "epoch": 2.6953020134228187,
      "grad_norm": 0.08074358105659485,
      "learning_rate": 0.0002190322580645161,
      "loss": 1.5333,
      "step": 1004
    },
    {
      "epoch": 2.697986577181208,
      "grad_norm": 0.08706672489643097,
      "learning_rate": 0.00021895161290322578,
      "loss": 1.5069,
      "step": 1005
    },
    {
      "epoch": 2.7006711409395976,
      "grad_norm": 0.07656365633010864,
      "learning_rate": 0.00021887096774193547,
      "loss": 1.5546,
      "step": 1006
    },
    {
      "epoch": 2.7033557046979864,
      "grad_norm": 0.0887882187962532,
      "learning_rate": 0.00021879032258064512,
      "loss": 1.4158,
      "step": 1007
    },
    {
      "epoch": 2.706040268456376,
      "grad_norm": 0.09089923650026321,
      "learning_rate": 0.0002187096774193548,
      "loss": 1.4202,
      "step": 1008
    },
    {
      "epoch": 2.7087248322147652,
      "grad_norm": 0.09901118278503418,
      "learning_rate": 0.0002186290322580645,
      "loss": 1.3693,
      "step": 1009
    },
    {
      "epoch": 2.7114093959731544,
      "grad_norm": 0.08252952247858047,
      "learning_rate": 0.00021854838709677417,
      "loss": 1.4983,
      "step": 1010
    },
    {
      "epoch": 2.7140939597315437,
      "grad_norm": 0.09479959309101105,
      "learning_rate": 0.00021846774193548386,
      "loss": 1.4051,
      "step": 1011
    },
    {
      "epoch": 2.716778523489933,
      "grad_norm": 0.08286342769861221,
      "learning_rate": 0.0002183870967741935,
      "loss": 1.4303,
      "step": 1012
    },
    {
      "epoch": 2.719463087248322,
      "grad_norm": 0.0813990905880928,
      "learning_rate": 0.0002183064516129032,
      "loss": 1.4914,
      "step": 1013
    },
    {
      "epoch": 2.7221476510067113,
      "grad_norm": 0.08714582026004791,
      "learning_rate": 0.00021822580645161288,
      "loss": 1.5303,
      "step": 1014
    },
    {
      "epoch": 2.7248322147651005,
      "grad_norm": 0.08406171947717667,
      "learning_rate": 0.00021814516129032256,
      "loss": 1.4976,
      "step": 1015
    },
    {
      "epoch": 2.7275167785234897,
      "grad_norm": 0.084709532558918,
      "learning_rate": 0.00021806451612903225,
      "loss": 1.5472,
      "step": 1016
    },
    {
      "epoch": 2.7302013422818794,
      "grad_norm": 0.0894940048456192,
      "learning_rate": 0.0002179838709677419,
      "loss": 1.5009,
      "step": 1017
    },
    {
      "epoch": 2.7328859060402686,
      "grad_norm": 0.0850825160741806,
      "learning_rate": 0.00021790322580645158,
      "loss": 1.5679,
      "step": 1018
    },
    {
      "epoch": 2.735570469798658,
      "grad_norm": 0.09369508177042007,
      "learning_rate": 0.00021782258064516127,
      "loss": 1.4191,
      "step": 1019
    },
    {
      "epoch": 2.738255033557047,
      "grad_norm": 0.07520805299282074,
      "learning_rate": 0.00021774193548387095,
      "loss": 1.4621,
      "step": 1020
    },
    {
      "epoch": 2.7409395973154362,
      "grad_norm": 0.09909272193908691,
      "learning_rate": 0.00021766129032258063,
      "loss": 1.3896,
      "step": 1021
    },
    {
      "epoch": 2.7436241610738255,
      "grad_norm": 0.08274910598993301,
      "learning_rate": 0.0002175806451612903,
      "loss": 1.4933,
      "step": 1022
    },
    {
      "epoch": 2.7463087248322147,
      "grad_norm": 0.07455658912658691,
      "learning_rate": 0.00021749999999999997,
      "loss": 1.5818,
      "step": 1023
    },
    {
      "epoch": 2.748993288590604,
      "grad_norm": 0.09336749464273453,
      "learning_rate": 0.00021741935483870966,
      "loss": 1.5443,
      "step": 1024
    },
    {
      "epoch": 2.751677852348993,
      "grad_norm": 0.07448342442512512,
      "learning_rate": 0.00021733870967741934,
      "loss": 1.6174,
      "step": 1025
    },
    {
      "epoch": 2.7543624161073827,
      "grad_norm": 0.11131482571363449,
      "learning_rate": 0.00021725806451612902,
      "loss": 1.4596,
      "step": 1026
    },
    {
      "epoch": 2.757046979865772,
      "grad_norm": 0.10459954291582108,
      "learning_rate": 0.00021717741935483868,
      "loss": 1.3353,
      "step": 1027
    },
    {
      "epoch": 2.759731543624161,
      "grad_norm": 0.08989093452692032,
      "learning_rate": 0.00021709677419354836,
      "loss": 1.5427,
      "step": 1028
    },
    {
      "epoch": 2.7624161073825504,
      "grad_norm": 0.10287192463874817,
      "learning_rate": 0.00021701612903225805,
      "loss": 1.4891,
      "step": 1029
    },
    {
      "epoch": 2.7651006711409396,
      "grad_norm": 0.09426618367433548,
      "learning_rate": 0.00021693548387096773,
      "loss": 1.4357,
      "step": 1030
    },
    {
      "epoch": 2.767785234899329,
      "grad_norm": 0.08332294225692749,
      "learning_rate": 0.00021685483870967741,
      "loss": 1.5724,
      "step": 1031
    },
    {
      "epoch": 2.770469798657718,
      "grad_norm": 0.0854293629527092,
      "learning_rate": 0.00021677419354838707,
      "loss": 1.3899,
      "step": 1032
    },
    {
      "epoch": 2.7731543624161072,
      "grad_norm": 0.08642879873514175,
      "learning_rate": 0.00021669354838709675,
      "loss": 1.5006,
      "step": 1033
    },
    {
      "epoch": 2.7758389261744965,
      "grad_norm": 0.08680536597967148,
      "learning_rate": 0.00021661290322580644,
      "loss": 1.4923,
      "step": 1034
    },
    {
      "epoch": 2.778523489932886,
      "grad_norm": 0.08194131404161453,
      "learning_rate": 0.00021653225806451612,
      "loss": 1.4971,
      "step": 1035
    },
    {
      "epoch": 2.7812080536912753,
      "grad_norm": 0.09212791174650192,
      "learning_rate": 0.0002164516129032258,
      "loss": 1.458,
      "step": 1036
    },
    {
      "epoch": 2.7838926174496645,
      "grad_norm": 0.08777743577957153,
      "learning_rate": 0.00021637096774193546,
      "loss": 1.5278,
      "step": 1037
    },
    {
      "epoch": 2.7865771812080538,
      "grad_norm": 0.09254272282123566,
      "learning_rate": 0.00021629032258064514,
      "loss": 1.5535,
      "step": 1038
    },
    {
      "epoch": 2.789261744966443,
      "grad_norm": 0.0874578207731247,
      "learning_rate": 0.00021620967741935483,
      "loss": 1.4421,
      "step": 1039
    },
    {
      "epoch": 2.791946308724832,
      "grad_norm": 0.08910445123910904,
      "learning_rate": 0.0002161290322580645,
      "loss": 1.4683,
      "step": 1040
    },
    {
      "epoch": 2.7946308724832214,
      "grad_norm": 0.08582761883735657,
      "learning_rate": 0.0002160483870967742,
      "loss": 1.4681,
      "step": 1041
    },
    {
      "epoch": 2.7973154362416106,
      "grad_norm": 0.0881223976612091,
      "learning_rate": 0.00021596774193548385,
      "loss": 1.5116,
      "step": 1042
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.09145065397024155,
      "learning_rate": 0.00021588709677419353,
      "loss": 1.5556,
      "step": 1043
    },
    {
      "epoch": 2.8026845637583895,
      "grad_norm": 0.08595232665538788,
      "learning_rate": 0.00021580645161290322,
      "loss": 1.3529,
      "step": 1044
    },
    {
      "epoch": 2.8053691275167782,
      "grad_norm": 0.09890811890363693,
      "learning_rate": 0.0002157258064516129,
      "loss": 1.4381,
      "step": 1045
    },
    {
      "epoch": 2.808053691275168,
      "grad_norm": 0.09686987102031708,
      "learning_rate": 0.00021564516129032258,
      "loss": 1.3996,
      "step": 1046
    },
    {
      "epoch": 2.810738255033557,
      "grad_norm": 0.08387798070907593,
      "learning_rate": 0.00021556451612903224,
      "loss": 1.4296,
      "step": 1047
    },
    {
      "epoch": 2.8134228187919463,
      "grad_norm": 0.09517379105091095,
      "learning_rate": 0.00021548387096774192,
      "loss": 1.3719,
      "step": 1048
    },
    {
      "epoch": 2.8161073825503355,
      "grad_norm": 0.08090139925479889,
      "learning_rate": 0.0002154032258064516,
      "loss": 1.5478,
      "step": 1049
    },
    {
      "epoch": 2.8187919463087248,
      "grad_norm": 0.08705627918243408,
      "learning_rate": 0.0002153225806451613,
      "loss": 1.59,
      "step": 1050
    },
    {
      "epoch": 2.821476510067114,
      "grad_norm": 0.09089155495166779,
      "learning_rate": 0.00021524193548387097,
      "loss": 1.3649,
      "step": 1051
    },
    {
      "epoch": 2.824161073825503,
      "grad_norm": 0.09087590873241425,
      "learning_rate": 0.00021516129032258063,
      "loss": 1.4015,
      "step": 1052
    },
    {
      "epoch": 2.826845637583893,
      "grad_norm": 0.08393721282482147,
      "learning_rate": 0.0002150806451612903,
      "loss": 1.5014,
      "step": 1053
    },
    {
      "epoch": 2.8295302013422816,
      "grad_norm": 0.07930482178926468,
      "learning_rate": 0.000215,
      "loss": 1.451,
      "step": 1054
    },
    {
      "epoch": 2.8322147651006713,
      "grad_norm": 0.09295052289962769,
      "learning_rate": 0.00021491935483870968,
      "loss": 1.5043,
      "step": 1055
    },
    {
      "epoch": 2.8348993288590605,
      "grad_norm": 0.0799407884478569,
      "learning_rate": 0.00021483870967741936,
      "loss": 1.5844,
      "step": 1056
    },
    {
      "epoch": 2.8375838926174497,
      "grad_norm": 0.09679675847291946,
      "learning_rate": 0.000214758064516129,
      "loss": 1.4497,
      "step": 1057
    },
    {
      "epoch": 2.840268456375839,
      "grad_norm": 0.0907960906624794,
      "learning_rate": 0.00021467741935483867,
      "loss": 1.3633,
      "step": 1058
    },
    {
      "epoch": 2.842953020134228,
      "grad_norm": 0.09231247752904892,
      "learning_rate": 0.00021459677419354838,
      "loss": 1.5283,
      "step": 1059
    },
    {
      "epoch": 2.8456375838926173,
      "grad_norm": 0.088133305311203,
      "learning_rate": 0.00021451612903225807,
      "loss": 1.5266,
      "step": 1060
    },
    {
      "epoch": 2.8483221476510066,
      "grad_norm": 0.10444357246160507,
      "learning_rate": 0.00021443548387096775,
      "loss": 1.4311,
      "step": 1061
    },
    {
      "epoch": 2.851006711409396,
      "grad_norm": 0.08985665440559387,
      "learning_rate": 0.00021435483870967738,
      "loss": 1.4623,
      "step": 1062
    },
    {
      "epoch": 2.853691275167785,
      "grad_norm": 0.08948295563459396,
      "learning_rate": 0.00021427419354838706,
      "loss": 1.5099,
      "step": 1063
    },
    {
      "epoch": 2.8563758389261746,
      "grad_norm": 0.08746545761823654,
      "learning_rate": 0.00021419354838709675,
      "loss": 1.4712,
      "step": 1064
    },
    {
      "epoch": 2.859060402684564,
      "grad_norm": 0.09232393652200699,
      "learning_rate": 0.00021411290322580643,
      "loss": 1.4952,
      "step": 1065
    },
    {
      "epoch": 2.861744966442953,
      "grad_norm": 0.08752347528934479,
      "learning_rate": 0.0002140322580645161,
      "loss": 1.5477,
      "step": 1066
    },
    {
      "epoch": 2.8644295302013423,
      "grad_norm": 0.0858912318944931,
      "learning_rate": 0.00021395161290322577,
      "loss": 1.5006,
      "step": 1067
    },
    {
      "epoch": 2.8671140939597315,
      "grad_norm": 0.08389678597450256,
      "learning_rate": 0.00021387096774193545,
      "loss": 1.5118,
      "step": 1068
    },
    {
      "epoch": 2.8697986577181207,
      "grad_norm": 0.07851769030094147,
      "learning_rate": 0.00021379032258064514,
      "loss": 1.491,
      "step": 1069
    },
    {
      "epoch": 2.87248322147651,
      "grad_norm": 0.08612196892499924,
      "learning_rate": 0.00021370967741935482,
      "loss": 1.438,
      "step": 1070
    },
    {
      "epoch": 2.8751677852348996,
      "grad_norm": 0.08369900286197662,
      "learning_rate": 0.00021362903225806448,
      "loss": 1.3921,
      "step": 1071
    },
    {
      "epoch": 2.8778523489932883,
      "grad_norm": 0.09126251190900803,
      "learning_rate": 0.00021354838709677416,
      "loss": 1.5109,
      "step": 1072
    },
    {
      "epoch": 2.880536912751678,
      "grad_norm": 0.08930125832557678,
      "learning_rate": 0.00021346774193548384,
      "loss": 1.4722,
      "step": 1073
    },
    {
      "epoch": 2.883221476510067,
      "grad_norm": 0.08410652726888657,
      "learning_rate": 0.00021338709677419353,
      "loss": 1.5229,
      "step": 1074
    },
    {
      "epoch": 2.8859060402684564,
      "grad_norm": 0.09158303588628769,
      "learning_rate": 0.0002133064516129032,
      "loss": 1.458,
      "step": 1075
    },
    {
      "epoch": 2.8885906040268456,
      "grad_norm": 0.08155329525470734,
      "learning_rate": 0.00021322580645161287,
      "loss": 1.5267,
      "step": 1076
    },
    {
      "epoch": 2.891275167785235,
      "grad_norm": 0.09384742379188538,
      "learning_rate": 0.00021314516129032255,
      "loss": 1.5499,
      "step": 1077
    },
    {
      "epoch": 2.893959731543624,
      "grad_norm": 0.10411135852336884,
      "learning_rate": 0.00021306451612903223,
      "loss": 1.4624,
      "step": 1078
    },
    {
      "epoch": 2.8966442953020133,
      "grad_norm": 0.0896112397313118,
      "learning_rate": 0.00021298387096774192,
      "loss": 1.4274,
      "step": 1079
    },
    {
      "epoch": 2.899328859060403,
      "grad_norm": 0.08096062391996384,
      "learning_rate": 0.0002129032258064516,
      "loss": 1.518,
      "step": 1080
    },
    {
      "epoch": 2.9020134228187917,
      "grad_norm": 0.0918349102139473,
      "learning_rate": 0.00021282258064516125,
      "loss": 1.3453,
      "step": 1081
    },
    {
      "epoch": 2.9046979865771814,
      "grad_norm": 0.08580425381660461,
      "learning_rate": 0.00021274193548387094,
      "loss": 1.5603,
      "step": 1082
    },
    {
      "epoch": 2.9073825503355706,
      "grad_norm": 0.08702680468559265,
      "learning_rate": 0.00021266129032258062,
      "loss": 1.4541,
      "step": 1083
    },
    {
      "epoch": 2.91006711409396,
      "grad_norm": 0.09036656469106674,
      "learning_rate": 0.0002125806451612903,
      "loss": 1.5,
      "step": 1084
    },
    {
      "epoch": 2.912751677852349,
      "grad_norm": 0.09562766551971436,
      "learning_rate": 0.0002125,
      "loss": 1.4333,
      "step": 1085
    },
    {
      "epoch": 2.915436241610738,
      "grad_norm": 0.0825548991560936,
      "learning_rate": 0.00021241935483870964,
      "loss": 1.4998,
      "step": 1086
    },
    {
      "epoch": 2.9181208053691274,
      "grad_norm": 0.09396948665380478,
      "learning_rate": 0.00021233870967741933,
      "loss": 1.4314,
      "step": 1087
    },
    {
      "epoch": 2.9208053691275166,
      "grad_norm": 0.08902271836996078,
      "learning_rate": 0.000212258064516129,
      "loss": 1.5638,
      "step": 1088
    },
    {
      "epoch": 2.9234899328859063,
      "grad_norm": 0.10175494849681854,
      "learning_rate": 0.0002121774193548387,
      "loss": 1.5319,
      "step": 1089
    },
    {
      "epoch": 2.926174496644295,
      "grad_norm": 0.08344957232475281,
      "learning_rate": 0.00021209677419354838,
      "loss": 1.4512,
      "step": 1090
    },
    {
      "epoch": 2.9288590604026847,
      "grad_norm": 0.09292882680892944,
      "learning_rate": 0.00021201612903225803,
      "loss": 1.3998,
      "step": 1091
    },
    {
      "epoch": 2.931543624161074,
      "grad_norm": 0.08291146904230118,
      "learning_rate": 0.00021193548387096772,
      "loss": 1.5278,
      "step": 1092
    },
    {
      "epoch": 2.934228187919463,
      "grad_norm": 0.09894731640815735,
      "learning_rate": 0.0002118548387096774,
      "loss": 1.3605,
      "step": 1093
    },
    {
      "epoch": 2.9369127516778524,
      "grad_norm": 0.101814866065979,
      "learning_rate": 0.00021177419354838708,
      "loss": 1.4249,
      "step": 1094
    },
    {
      "epoch": 2.9395973154362416,
      "grad_norm": 0.09266690909862518,
      "learning_rate": 0.00021169354838709677,
      "loss": 1.372,
      "step": 1095
    },
    {
      "epoch": 2.942281879194631,
      "grad_norm": 0.079148069024086,
      "learning_rate": 0.00021161290322580642,
      "loss": 1.5185,
      "step": 1096
    },
    {
      "epoch": 2.94496644295302,
      "grad_norm": 0.07793305814266205,
      "learning_rate": 0.0002115322580645161,
      "loss": 1.6302,
      "step": 1097
    },
    {
      "epoch": 2.9476510067114097,
      "grad_norm": 0.08584263175725937,
      "learning_rate": 0.0002114516129032258,
      "loss": 1.4823,
      "step": 1098
    },
    {
      "epoch": 2.9503355704697984,
      "grad_norm": 0.10198832303285599,
      "learning_rate": 0.00021137096774193547,
      "loss": 1.502,
      "step": 1099
    },
    {
      "epoch": 2.953020134228188,
      "grad_norm": 0.08886633068323135,
      "learning_rate": 0.00021129032258064516,
      "loss": 1.4266,
      "step": 1100
    },
    {
      "epoch": 2.9557046979865773,
      "grad_norm": 0.08083264529705048,
      "learning_rate": 0.0002112096774193548,
      "loss": 1.4455,
      "step": 1101
    },
    {
      "epoch": 2.9583892617449665,
      "grad_norm": 0.08499789983034134,
      "learning_rate": 0.0002111290322580645,
      "loss": 1.4836,
      "step": 1102
    },
    {
      "epoch": 2.9610738255033557,
      "grad_norm": 0.09686906635761261,
      "learning_rate": 0.00021104838709677418,
      "loss": 1.4828,
      "step": 1103
    },
    {
      "epoch": 2.963758389261745,
      "grad_norm": 0.08271556347608566,
      "learning_rate": 0.00021096774193548386,
      "loss": 1.3462,
      "step": 1104
    },
    {
      "epoch": 2.966442953020134,
      "grad_norm": 0.08123259991407394,
      "learning_rate": 0.00021088709677419355,
      "loss": 1.546,
      "step": 1105
    },
    {
      "epoch": 2.9691275167785234,
      "grad_norm": 0.09340406954288483,
      "learning_rate": 0.0002108064516129032,
      "loss": 1.3833,
      "step": 1106
    },
    {
      "epoch": 2.9718120805369126,
      "grad_norm": 0.07718081027269363,
      "learning_rate": 0.00021072580645161289,
      "loss": 1.4724,
      "step": 1107
    },
    {
      "epoch": 2.974496644295302,
      "grad_norm": 0.09352670609951019,
      "learning_rate": 0.00021064516129032257,
      "loss": 1.4345,
      "step": 1108
    },
    {
      "epoch": 2.9771812080536915,
      "grad_norm": 0.08780615031719208,
      "learning_rate": 0.00021056451612903225,
      "loss": 1.5152,
      "step": 1109
    },
    {
      "epoch": 2.9798657718120807,
      "grad_norm": 0.09190084040164948,
      "learning_rate": 0.00021048387096774194,
      "loss": 1.487,
      "step": 1110
    },
    {
      "epoch": 2.98255033557047,
      "grad_norm": 0.09120511263608932,
      "learning_rate": 0.0002104032258064516,
      "loss": 1.5636,
      "step": 1111
    },
    {
      "epoch": 2.985234899328859,
      "grad_norm": 0.08430344611406326,
      "learning_rate": 0.00021032258064516128,
      "loss": 1.5401,
      "step": 1112
    },
    {
      "epoch": 2.9879194630872483,
      "grad_norm": 0.08010658621788025,
      "learning_rate": 0.00021024193548387096,
      "loss": 1.5098,
      "step": 1113
    },
    {
      "epoch": 2.9906040268456375,
      "grad_norm": 0.08301807194948196,
      "learning_rate": 0.00021016129032258064,
      "loss": 1.4917,
      "step": 1114
    },
    {
      "epoch": 2.9932885906040267,
      "grad_norm": 0.09033632278442383,
      "learning_rate": 0.00021008064516129033,
      "loss": 1.523,
      "step": 1115
    },
    {
      "epoch": 2.995973154362416,
      "grad_norm": 0.09479536861181259,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.4734,
      "step": 1116
    },
    {
      "epoch": 2.998657718120805,
      "grad_norm": 0.09203942865133286,
      "learning_rate": 0.00020991935483870966,
      "loss": 1.4219,
      "step": 1117
    },
    {
      "epoch": 3.001342281879195,
      "grad_norm": 0.0913827046751976,
      "learning_rate": 0.00020983870967741935,
      "loss": 1.4639,
      "step": 1118
    },
    {
      "epoch": 3.004026845637584,
      "grad_norm": 0.08041054010391235,
      "learning_rate": 0.00020975806451612903,
      "loss": 1.5347,
      "step": 1119
    },
    {
      "epoch": 3.0067114093959733,
      "grad_norm": 0.0820838212966919,
      "learning_rate": 0.00020967741935483871,
      "loss": 1.4784,
      "step": 1120
    },
    {
      "epoch": 3.0093959731543625,
      "grad_norm": 0.09662000834941864,
      "learning_rate": 0.00020959677419354834,
      "loss": 1.4342,
      "step": 1121
    },
    {
      "epoch": 3.0120805369127517,
      "grad_norm": 0.08472412824630737,
      "learning_rate": 0.00020951612903225803,
      "loss": 1.5523,
      "step": 1122
    },
    {
      "epoch": 3.014765100671141,
      "grad_norm": 0.08449013531208038,
      "learning_rate": 0.0002094354838709677,
      "loss": 1.4935,
      "step": 1123
    },
    {
      "epoch": 3.01744966442953,
      "grad_norm": 0.08196964859962463,
      "learning_rate": 0.00020935483870967742,
      "loss": 1.4309,
      "step": 1124
    },
    {
      "epoch": 3.0201342281879193,
      "grad_norm": 0.0902201309800148,
      "learning_rate": 0.0002092741935483871,
      "loss": 1.5682,
      "step": 1125
    },
    {
      "epoch": 3.0228187919463085,
      "grad_norm": 0.0933445617556572,
      "learning_rate": 0.00020919354838709673,
      "loss": 1.422,
      "step": 1126
    },
    {
      "epoch": 3.025503355704698,
      "grad_norm": 0.08318748325109482,
      "learning_rate": 0.00020911290322580642,
      "loss": 1.4746,
      "step": 1127
    },
    {
      "epoch": 3.0281879194630874,
      "grad_norm": 0.08315125852823257,
      "learning_rate": 0.0002090322580645161,
      "loss": 1.4512,
      "step": 1128
    },
    {
      "epoch": 3.0308724832214766,
      "grad_norm": 0.07831171154975891,
      "learning_rate": 0.00020895161290322578,
      "loss": 1.5521,
      "step": 1129
    },
    {
      "epoch": 3.033557046979866,
      "grad_norm": 0.07921863347291946,
      "learning_rate": 0.00020887096774193547,
      "loss": 1.5708,
      "step": 1130
    },
    {
      "epoch": 3.036241610738255,
      "grad_norm": 0.09462945908308029,
      "learning_rate": 0.00020879032258064512,
      "loss": 1.434,
      "step": 1131
    },
    {
      "epoch": 3.0389261744966443,
      "grad_norm": 0.08508829772472382,
      "learning_rate": 0.0002087096774193548,
      "loss": 1.4445,
      "step": 1132
    },
    {
      "epoch": 3.0416107382550335,
      "grad_norm": 0.09470890462398529,
      "learning_rate": 0.0002086290322580645,
      "loss": 1.4475,
      "step": 1133
    },
    {
      "epoch": 3.0442953020134227,
      "grad_norm": 0.10547057539224625,
      "learning_rate": 0.00020854838709677417,
      "loss": 1.4323,
      "step": 1134
    },
    {
      "epoch": 3.046979865771812,
      "grad_norm": 0.09041053056716919,
      "learning_rate": 0.00020846774193548386,
      "loss": 1.4459,
      "step": 1135
    },
    {
      "epoch": 3.0496644295302016,
      "grad_norm": 0.08590824902057648,
      "learning_rate": 0.0002083870967741935,
      "loss": 1.4595,
      "step": 1136
    },
    {
      "epoch": 3.0523489932885908,
      "grad_norm": 0.08308113366365433,
      "learning_rate": 0.0002083064516129032,
      "loss": 1.492,
      "step": 1137
    },
    {
      "epoch": 3.05503355704698,
      "grad_norm": 0.08334687352180481,
      "learning_rate": 0.00020822580645161288,
      "loss": 1.6049,
      "step": 1138
    },
    {
      "epoch": 3.057718120805369,
      "grad_norm": 0.09592831134796143,
      "learning_rate": 0.00020814516129032256,
      "loss": 1.4036,
      "step": 1139
    },
    {
      "epoch": 3.0604026845637584,
      "grad_norm": 0.09255056828260422,
      "learning_rate": 0.00020806451612903225,
      "loss": 1.423,
      "step": 1140
    },
    {
      "epoch": 3.0630872483221476,
      "grad_norm": 0.09058832377195358,
      "learning_rate": 0.0002079838709677419,
      "loss": 1.4437,
      "step": 1141
    },
    {
      "epoch": 3.065771812080537,
      "grad_norm": 0.07898084819316864,
      "learning_rate": 0.00020790322580645159,
      "loss": 1.479,
      "step": 1142
    },
    {
      "epoch": 3.068456375838926,
      "grad_norm": 0.10014183074235916,
      "learning_rate": 0.00020782258064516127,
      "loss": 1.4378,
      "step": 1143
    },
    {
      "epoch": 3.0711409395973153,
      "grad_norm": 0.09760739654302597,
      "learning_rate": 0.00020774193548387095,
      "loss": 1.3813,
      "step": 1144
    },
    {
      "epoch": 3.073825503355705,
      "grad_norm": 0.09257052838802338,
      "learning_rate": 0.00020766129032258064,
      "loss": 1.5312,
      "step": 1145
    },
    {
      "epoch": 3.076510067114094,
      "grad_norm": 0.09042999893426895,
      "learning_rate": 0.0002075806451612903,
      "loss": 1.4768,
      "step": 1146
    },
    {
      "epoch": 3.0791946308724834,
      "grad_norm": 0.09591919183731079,
      "learning_rate": 0.00020749999999999998,
      "loss": 1.5033,
      "step": 1147
    },
    {
      "epoch": 3.0818791946308726,
      "grad_norm": 0.07971009612083435,
      "learning_rate": 0.00020741935483870966,
      "loss": 1.5295,
      "step": 1148
    },
    {
      "epoch": 3.084563758389262,
      "grad_norm": 0.0840195044875145,
      "learning_rate": 0.00020733870967741934,
      "loss": 1.4144,
      "step": 1149
    },
    {
      "epoch": 3.087248322147651,
      "grad_norm": 0.08883994072675705,
      "learning_rate": 0.00020725806451612903,
      "loss": 1.521,
      "step": 1150
    },
    {
      "epoch": 3.08993288590604,
      "grad_norm": 0.08095361292362213,
      "learning_rate": 0.00020717741935483868,
      "loss": 1.5164,
      "step": 1151
    },
    {
      "epoch": 3.0926174496644294,
      "grad_norm": 0.07728655636310577,
      "learning_rate": 0.00020709677419354836,
      "loss": 1.5525,
      "step": 1152
    },
    {
      "epoch": 3.0953020134228186,
      "grad_norm": 0.09579513967037201,
      "learning_rate": 0.00020701612903225805,
      "loss": 1.348,
      "step": 1153
    },
    {
      "epoch": 3.097986577181208,
      "grad_norm": 0.0847342386841774,
      "learning_rate": 0.00020693548387096773,
      "loss": 1.4805,
      "step": 1154
    },
    {
      "epoch": 3.1006711409395975,
      "grad_norm": 0.0860285684466362,
      "learning_rate": 0.00020685483870967741,
      "loss": 1.4678,
      "step": 1155
    },
    {
      "epoch": 3.1033557046979867,
      "grad_norm": 0.08244044333696365,
      "learning_rate": 0.00020677419354838707,
      "loss": 1.464,
      "step": 1156
    },
    {
      "epoch": 3.106040268456376,
      "grad_norm": 0.08336605876684189,
      "learning_rate": 0.00020669354838709675,
      "loss": 1.5465,
      "step": 1157
    },
    {
      "epoch": 3.108724832214765,
      "grad_norm": 0.08116831630468369,
      "learning_rate": 0.00020661290322580644,
      "loss": 1.4707,
      "step": 1158
    },
    {
      "epoch": 3.1114093959731544,
      "grad_norm": 0.08888451009988785,
      "learning_rate": 0.00020653225806451612,
      "loss": 1.3858,
      "step": 1159
    },
    {
      "epoch": 3.1140939597315436,
      "grad_norm": 0.1162942424416542,
      "learning_rate": 0.0002064516129032258,
      "loss": 1.5042,
      "step": 1160
    },
    {
      "epoch": 3.116778523489933,
      "grad_norm": 0.08715564757585526,
      "learning_rate": 0.00020637096774193546,
      "loss": 1.3971,
      "step": 1161
    },
    {
      "epoch": 3.119463087248322,
      "grad_norm": 0.08210688829421997,
      "learning_rate": 0.00020629032258064514,
      "loss": 1.3824,
      "step": 1162
    },
    {
      "epoch": 3.122147651006711,
      "grad_norm": 0.08520251512527466,
      "learning_rate": 0.00020620967741935483,
      "loss": 1.4679,
      "step": 1163
    },
    {
      "epoch": 3.124832214765101,
      "grad_norm": 0.10011734813451767,
      "learning_rate": 0.0002061290322580645,
      "loss": 1.4057,
      "step": 1164
    },
    {
      "epoch": 3.12751677852349,
      "grad_norm": 0.10548757761716843,
      "learning_rate": 0.00020604838709677417,
      "loss": 1.2765,
      "step": 1165
    },
    {
      "epoch": 3.1302013422818793,
      "grad_norm": 0.11839723587036133,
      "learning_rate": 0.00020596774193548385,
      "loss": 1.5594,
      "step": 1166
    },
    {
      "epoch": 3.1328859060402685,
      "grad_norm": 0.09405611455440521,
      "learning_rate": 0.00020588709677419353,
      "loss": 1.4708,
      "step": 1167
    },
    {
      "epoch": 3.1355704697986577,
      "grad_norm": 0.08580121397972107,
      "learning_rate": 0.00020580645161290322,
      "loss": 1.5757,
      "step": 1168
    },
    {
      "epoch": 3.138255033557047,
      "grad_norm": 0.093572698533535,
      "learning_rate": 0.0002057258064516129,
      "loss": 1.5341,
      "step": 1169
    },
    {
      "epoch": 3.140939597315436,
      "grad_norm": 0.08510278910398483,
      "learning_rate": 0.00020564516129032256,
      "loss": 1.4904,
      "step": 1170
    },
    {
      "epoch": 3.1436241610738254,
      "grad_norm": 0.08211229741573334,
      "learning_rate": 0.00020556451612903224,
      "loss": 1.5363,
      "step": 1171
    },
    {
      "epoch": 3.1463087248322146,
      "grad_norm": 0.08742840588092804,
      "learning_rate": 0.00020548387096774192,
      "loss": 1.4954,
      "step": 1172
    },
    {
      "epoch": 3.1489932885906042,
      "grad_norm": 0.09751774370670319,
      "learning_rate": 0.0002054032258064516,
      "loss": 1.4206,
      "step": 1173
    },
    {
      "epoch": 3.1516778523489934,
      "grad_norm": 0.08539441972970963,
      "learning_rate": 0.0002053225806451613,
      "loss": 1.5401,
      "step": 1174
    },
    {
      "epoch": 3.1543624161073827,
      "grad_norm": 0.08596016466617584,
      "learning_rate": 0.00020524193548387095,
      "loss": 1.4819,
      "step": 1175
    },
    {
      "epoch": 3.157046979865772,
      "grad_norm": 0.10853252559900284,
      "learning_rate": 0.00020516129032258063,
      "loss": 1.4048,
      "step": 1176
    },
    {
      "epoch": 3.159731543624161,
      "grad_norm": 0.09095031023025513,
      "learning_rate": 0.0002050806451612903,
      "loss": 1.4703,
      "step": 1177
    },
    {
      "epoch": 3.1624161073825503,
      "grad_norm": 0.09470535069704056,
      "learning_rate": 0.000205,
      "loss": 1.4197,
      "step": 1178
    },
    {
      "epoch": 3.1651006711409395,
      "grad_norm": 0.10132336616516113,
      "learning_rate": 0.00020491935483870968,
      "loss": 1.3474,
      "step": 1179
    },
    {
      "epoch": 3.1677852348993287,
      "grad_norm": 0.09689547121524811,
      "learning_rate": 0.0002048387096774193,
      "loss": 1.4464,
      "step": 1180
    },
    {
      "epoch": 3.170469798657718,
      "grad_norm": 0.0846087858080864,
      "learning_rate": 0.00020475806451612902,
      "loss": 1.541,
      "step": 1181
    },
    {
      "epoch": 3.173154362416107,
      "grad_norm": 0.09371259063482285,
      "learning_rate": 0.0002046774193548387,
      "loss": 1.4251,
      "step": 1182
    },
    {
      "epoch": 3.175838926174497,
      "grad_norm": 0.09434367716312408,
      "learning_rate": 0.00020459677419354839,
      "loss": 1.4341,
      "step": 1183
    },
    {
      "epoch": 3.178523489932886,
      "grad_norm": 0.10364627093076706,
      "learning_rate": 0.00020451612903225807,
      "loss": 1.4943,
      "step": 1184
    },
    {
      "epoch": 3.1812080536912752,
      "grad_norm": 0.0934811383485794,
      "learning_rate": 0.0002044354838709677,
      "loss": 1.3835,
      "step": 1185
    },
    {
      "epoch": 3.1838926174496645,
      "grad_norm": 0.08076849579811096,
      "learning_rate": 0.00020435483870967738,
      "loss": 1.4756,
      "step": 1186
    },
    {
      "epoch": 3.1865771812080537,
      "grad_norm": 0.093205526471138,
      "learning_rate": 0.00020427419354838706,
      "loss": 1.4284,
      "step": 1187
    },
    {
      "epoch": 3.189261744966443,
      "grad_norm": 0.0937139168381691,
      "learning_rate": 0.00020419354838709677,
      "loss": 1.4908,
      "step": 1188
    },
    {
      "epoch": 3.191946308724832,
      "grad_norm": 0.10026077181100845,
      "learning_rate": 0.00020411290322580646,
      "loss": 1.3634,
      "step": 1189
    },
    {
      "epoch": 3.1946308724832213,
      "grad_norm": 0.10220025479793549,
      "learning_rate": 0.0002040322580645161,
      "loss": 1.3552,
      "step": 1190
    },
    {
      "epoch": 3.1973154362416105,
      "grad_norm": 0.08793102204799652,
      "learning_rate": 0.00020395161290322577,
      "loss": 1.4823,
      "step": 1191
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.0871439203619957,
      "learning_rate": 0.00020387096774193545,
      "loss": 1.3712,
      "step": 1192
    },
    {
      "epoch": 3.2026845637583894,
      "grad_norm": 0.08503450453281403,
      "learning_rate": 0.00020379032258064514,
      "loss": 1.4438,
      "step": 1193
    },
    {
      "epoch": 3.2053691275167786,
      "grad_norm": 0.09753423929214478,
      "learning_rate": 0.00020370967741935482,
      "loss": 1.3964,
      "step": 1194
    },
    {
      "epoch": 3.208053691275168,
      "grad_norm": 0.08313364535570145,
      "learning_rate": 0.00020362903225806448,
      "loss": 1.429,
      "step": 1195
    },
    {
      "epoch": 3.210738255033557,
      "grad_norm": 0.0947268009185791,
      "learning_rate": 0.00020354838709677416,
      "loss": 1.4025,
      "step": 1196
    },
    {
      "epoch": 3.2134228187919462,
      "grad_norm": 0.09559445083141327,
      "learning_rate": 0.00020346774193548384,
      "loss": 1.4932,
      "step": 1197
    },
    {
      "epoch": 3.2161073825503355,
      "grad_norm": 0.0841737613081932,
      "learning_rate": 0.00020338709677419353,
      "loss": 1.6031,
      "step": 1198
    },
    {
      "epoch": 3.2187919463087247,
      "grad_norm": 0.07784091681241989,
      "learning_rate": 0.0002033064516129032,
      "loss": 1.4071,
      "step": 1199
    },
    {
      "epoch": 3.221476510067114,
      "grad_norm": 0.08401767164468765,
      "learning_rate": 0.00020322580645161287,
      "loss": 1.413,
      "step": 1200
    },
    {
      "epoch": 3.2241610738255035,
      "grad_norm": 0.08308494836091995,
      "learning_rate": 0.00020314516129032255,
      "loss": 1.5555,
      "step": 1201
    },
    {
      "epoch": 3.2268456375838928,
      "grad_norm": 0.0892040878534317,
      "learning_rate": 0.00020306451612903223,
      "loss": 1.5359,
      "step": 1202
    },
    {
      "epoch": 3.229530201342282,
      "grad_norm": 0.0883682370185852,
      "learning_rate": 0.00020298387096774192,
      "loss": 1.4951,
      "step": 1203
    },
    {
      "epoch": 3.232214765100671,
      "grad_norm": 0.09529765695333481,
      "learning_rate": 0.0002029032258064516,
      "loss": 1.5526,
      "step": 1204
    },
    {
      "epoch": 3.2348993288590604,
      "grad_norm": 0.0834590271115303,
      "learning_rate": 0.00020282258064516126,
      "loss": 1.5212,
      "step": 1205
    },
    {
      "epoch": 3.2375838926174496,
      "grad_norm": 0.08927203714847565,
      "learning_rate": 0.00020274193548387094,
      "loss": 1.4069,
      "step": 1206
    },
    {
      "epoch": 3.240268456375839,
      "grad_norm": 0.08418301492929459,
      "learning_rate": 0.00020266129032258062,
      "loss": 1.4796,
      "step": 1207
    },
    {
      "epoch": 3.242953020134228,
      "grad_norm": 0.0788191556930542,
      "learning_rate": 0.0002025806451612903,
      "loss": 1.5036,
      "step": 1208
    },
    {
      "epoch": 3.2456375838926173,
      "grad_norm": 0.08319470286369324,
      "learning_rate": 0.0002025,
      "loss": 1.4043,
      "step": 1209
    },
    {
      "epoch": 3.248322147651007,
      "grad_norm": 0.09435109794139862,
      "learning_rate": 0.00020241935483870965,
      "loss": 1.4325,
      "step": 1210
    },
    {
      "epoch": 3.251006711409396,
      "grad_norm": 0.08895973861217499,
      "learning_rate": 0.00020233870967741933,
      "loss": 1.5046,
      "step": 1211
    },
    {
      "epoch": 3.2536912751677853,
      "grad_norm": 0.08895315229892731,
      "learning_rate": 0.000202258064516129,
      "loss": 1.6022,
      "step": 1212
    },
    {
      "epoch": 3.2563758389261745,
      "grad_norm": 0.08904525637626648,
      "learning_rate": 0.0002021774193548387,
      "loss": 1.4282,
      "step": 1213
    },
    {
      "epoch": 3.2590604026845638,
      "grad_norm": 0.10389036685228348,
      "learning_rate": 0.00020209677419354838,
      "loss": 1.4287,
      "step": 1214
    },
    {
      "epoch": 3.261744966442953,
      "grad_norm": 0.0870172381401062,
      "learning_rate": 0.00020201612903225803,
      "loss": 1.5519,
      "step": 1215
    },
    {
      "epoch": 3.264429530201342,
      "grad_norm": 0.08693983405828476,
      "learning_rate": 0.00020193548387096772,
      "loss": 1.4676,
      "step": 1216
    },
    {
      "epoch": 3.2671140939597314,
      "grad_norm": 0.09057294577360153,
      "learning_rate": 0.0002018548387096774,
      "loss": 1.6459,
      "step": 1217
    },
    {
      "epoch": 3.2697986577181206,
      "grad_norm": 0.08549599349498749,
      "learning_rate": 0.00020177419354838708,
      "loss": 1.4882,
      "step": 1218
    },
    {
      "epoch": 3.2724832214765103,
      "grad_norm": 0.08605577796697617,
      "learning_rate": 0.00020169354838709677,
      "loss": 1.5041,
      "step": 1219
    },
    {
      "epoch": 3.2751677852348995,
      "grad_norm": 0.09103740751743317,
      "learning_rate": 0.00020161290322580642,
      "loss": 1.5326,
      "step": 1220
    },
    {
      "epoch": 3.2778523489932887,
      "grad_norm": 0.08461079001426697,
      "learning_rate": 0.0002015322580645161,
      "loss": 1.5869,
      "step": 1221
    },
    {
      "epoch": 3.280536912751678,
      "grad_norm": 0.09755966067314148,
      "learning_rate": 0.0002014516129032258,
      "loss": 1.4597,
      "step": 1222
    },
    {
      "epoch": 3.283221476510067,
      "grad_norm": 0.08910945057868958,
      "learning_rate": 0.00020137096774193547,
      "loss": 1.5228,
      "step": 1223
    },
    {
      "epoch": 3.2859060402684563,
      "grad_norm": 0.08748261630535126,
      "learning_rate": 0.00020129032258064516,
      "loss": 1.5589,
      "step": 1224
    },
    {
      "epoch": 3.2885906040268456,
      "grad_norm": 0.09877554327249527,
      "learning_rate": 0.00020120967741935481,
      "loss": 1.4836,
      "step": 1225
    },
    {
      "epoch": 3.2912751677852348,
      "grad_norm": 0.09696979820728302,
      "learning_rate": 0.0002011290322580645,
      "loss": 1.3697,
      "step": 1226
    },
    {
      "epoch": 3.293959731543624,
      "grad_norm": 0.0987323597073555,
      "learning_rate": 0.00020104838709677418,
      "loss": 1.3588,
      "step": 1227
    },
    {
      "epoch": 3.2966442953020136,
      "grad_norm": 0.09550647437572479,
      "learning_rate": 0.00020096774193548386,
      "loss": 1.4712,
      "step": 1228
    },
    {
      "epoch": 3.299328859060403,
      "grad_norm": 0.08360958844423294,
      "learning_rate": 0.00020088709677419355,
      "loss": 1.4704,
      "step": 1229
    },
    {
      "epoch": 3.302013422818792,
      "grad_norm": 0.08489052951335907,
      "learning_rate": 0.0002008064516129032,
      "loss": 1.4938,
      "step": 1230
    },
    {
      "epoch": 3.3046979865771813,
      "grad_norm": 0.07772011309862137,
      "learning_rate": 0.0002007258064516129,
      "loss": 1.4583,
      "step": 1231
    },
    {
      "epoch": 3.3073825503355705,
      "grad_norm": 0.08540555834770203,
      "learning_rate": 0.00020064516129032257,
      "loss": 1.4667,
      "step": 1232
    },
    {
      "epoch": 3.3100671140939597,
      "grad_norm": 0.08597877621650696,
      "learning_rate": 0.00020056451612903225,
      "loss": 1.4369,
      "step": 1233
    },
    {
      "epoch": 3.312751677852349,
      "grad_norm": 0.08414201438426971,
      "learning_rate": 0.00020048387096774194,
      "loss": 1.471,
      "step": 1234
    },
    {
      "epoch": 3.315436241610738,
      "grad_norm": 0.0861777514219284,
      "learning_rate": 0.0002004032258064516,
      "loss": 1.4569,
      "step": 1235
    },
    {
      "epoch": 3.3181208053691273,
      "grad_norm": 0.11023314297199249,
      "learning_rate": 0.00020032258064516128,
      "loss": 1.3849,
      "step": 1236
    },
    {
      "epoch": 3.320805369127517,
      "grad_norm": 0.09321004897356033,
      "learning_rate": 0.00020024193548387096,
      "loss": 1.4748,
      "step": 1237
    },
    {
      "epoch": 3.323489932885906,
      "grad_norm": 0.08861880004405975,
      "learning_rate": 0.00020016129032258064,
      "loss": 1.4871,
      "step": 1238
    },
    {
      "epoch": 3.3261744966442954,
      "grad_norm": 0.0976971834897995,
      "learning_rate": 0.00020008064516129033,
      "loss": 1.4066,
      "step": 1239
    },
    {
      "epoch": 3.3288590604026846,
      "grad_norm": 0.10374632477760315,
      "learning_rate": 0.00019999999999999998,
      "loss": 1.4217,
      "step": 1240
    },
    {
      "epoch": 3.331543624161074,
      "grad_norm": 0.08639702200889587,
      "learning_rate": 0.00019991935483870967,
      "loss": 1.4937,
      "step": 1241
    },
    {
      "epoch": 3.334228187919463,
      "grad_norm": 0.0917854830622673,
      "learning_rate": 0.00019983870967741935,
      "loss": 1.321,
      "step": 1242
    },
    {
      "epoch": 3.3369127516778523,
      "grad_norm": 0.09828697890043259,
      "learning_rate": 0.00019975806451612903,
      "loss": 1.4608,
      "step": 1243
    },
    {
      "epoch": 3.3395973154362415,
      "grad_norm": 0.08657311648130417,
      "learning_rate": 0.00019967741935483872,
      "loss": 1.6014,
      "step": 1244
    },
    {
      "epoch": 3.3422818791946307,
      "grad_norm": 0.09143919497728348,
      "learning_rate": 0.00019959677419354837,
      "loss": 1.44,
      "step": 1245
    },
    {
      "epoch": 3.3449664429530204,
      "grad_norm": 0.08093944191932678,
      "learning_rate": 0.00019951612903225806,
      "loss": 1.622,
      "step": 1246
    },
    {
      "epoch": 3.3476510067114096,
      "grad_norm": 0.09343220293521881,
      "learning_rate": 0.00019943548387096774,
      "loss": 1.5432,
      "step": 1247
    },
    {
      "epoch": 3.350335570469799,
      "grad_norm": 0.1077568307518959,
      "learning_rate": 0.00019935483870967742,
      "loss": 1.3603,
      "step": 1248
    },
    {
      "epoch": 3.353020134228188,
      "grad_norm": 0.08083277195692062,
      "learning_rate": 0.0001992741935483871,
      "loss": 1.5827,
      "step": 1249
    },
    {
      "epoch": 3.3557046979865772,
      "grad_norm": 0.08872701972723007,
      "learning_rate": 0.00019919354838709673,
      "loss": 1.5025,
      "step": 1250
    },
    {
      "epoch": 3.3583892617449664,
      "grad_norm": 0.08436866104602814,
      "learning_rate": 0.00019911290322580642,
      "loss": 1.4767,
      "step": 1251
    },
    {
      "epoch": 3.3610738255033556,
      "grad_norm": 0.09037908166646957,
      "learning_rate": 0.00019903225806451613,
      "loss": 1.4427,
      "step": 1252
    },
    {
      "epoch": 3.363758389261745,
      "grad_norm": 0.08658172935247421,
      "learning_rate": 0.0001989516129032258,
      "loss": 1.477,
      "step": 1253
    },
    {
      "epoch": 3.366442953020134,
      "grad_norm": 0.08558151870965958,
      "learning_rate": 0.0001988709677419355,
      "loss": 1.6155,
      "step": 1254
    },
    {
      "epoch": 3.3691275167785237,
      "grad_norm": 0.08588145673274994,
      "learning_rate": 0.00019879032258064512,
      "loss": 1.5122,
      "step": 1255
    },
    {
      "epoch": 3.3718120805369125,
      "grad_norm": 0.09017109125852585,
      "learning_rate": 0.0001987096774193548,
      "loss": 1.6086,
      "step": 1256
    },
    {
      "epoch": 3.374496644295302,
      "grad_norm": 0.08731094002723694,
      "learning_rate": 0.0001986290322580645,
      "loss": 1.3976,
      "step": 1257
    },
    {
      "epoch": 3.3771812080536914,
      "grad_norm": 0.09593849629163742,
      "learning_rate": 0.00019854838709677417,
      "loss": 1.5226,
      "step": 1258
    },
    {
      "epoch": 3.3798657718120806,
      "grad_norm": 0.0858459621667862,
      "learning_rate": 0.00019846774193548383,
      "loss": 1.3798,
      "step": 1259
    },
    {
      "epoch": 3.38255033557047,
      "grad_norm": 0.08774993568658829,
      "learning_rate": 0.0001983870967741935,
      "loss": 1.5784,
      "step": 1260
    },
    {
      "epoch": 3.385234899328859,
      "grad_norm": 0.08454011380672455,
      "learning_rate": 0.0001983064516129032,
      "loss": 1.4367,
      "step": 1261
    },
    {
      "epoch": 3.3879194630872482,
      "grad_norm": 0.08061528950929642,
      "learning_rate": 0.00019822580645161288,
      "loss": 1.4413,
      "step": 1262
    },
    {
      "epoch": 3.3906040268456374,
      "grad_norm": 0.09012043476104736,
      "learning_rate": 0.00019814516129032256,
      "loss": 1.4092,
      "step": 1263
    },
    {
      "epoch": 3.3932885906040267,
      "grad_norm": 0.08795701712369919,
      "learning_rate": 0.00019806451612903222,
      "loss": 1.4248,
      "step": 1264
    },
    {
      "epoch": 3.395973154362416,
      "grad_norm": 0.09896314889192581,
      "learning_rate": 0.0001979838709677419,
      "loss": 1.3159,
      "step": 1265
    },
    {
      "epoch": 3.3986577181208055,
      "grad_norm": 0.09358787536621094,
      "learning_rate": 0.00019790322580645159,
      "loss": 1.5615,
      "step": 1266
    },
    {
      "epoch": 3.4013422818791947,
      "grad_norm": 0.07968058437108994,
      "learning_rate": 0.00019782258064516127,
      "loss": 1.4314,
      "step": 1267
    },
    {
      "epoch": 3.404026845637584,
      "grad_norm": 0.07996246218681335,
      "learning_rate": 0.00019774193548387095,
      "loss": 1.5315,
      "step": 1268
    },
    {
      "epoch": 3.406711409395973,
      "grad_norm": 0.08591428399085999,
      "learning_rate": 0.0001976612903225806,
      "loss": 1.4663,
      "step": 1269
    },
    {
      "epoch": 3.4093959731543624,
      "grad_norm": 0.09116784483194351,
      "learning_rate": 0.0001975806451612903,
      "loss": 1.505,
      "step": 1270
    },
    {
      "epoch": 3.4120805369127516,
      "grad_norm": 0.09024073928594589,
      "learning_rate": 0.00019749999999999998,
      "loss": 1.4348,
      "step": 1271
    },
    {
      "epoch": 3.414765100671141,
      "grad_norm": 0.08719303458929062,
      "learning_rate": 0.00019741935483870966,
      "loss": 1.4792,
      "step": 1272
    },
    {
      "epoch": 3.41744966442953,
      "grad_norm": 0.09892314672470093,
      "learning_rate": 0.00019733870967741934,
      "loss": 1.4172,
      "step": 1273
    },
    {
      "epoch": 3.4201342281879192,
      "grad_norm": 0.09221397340297699,
      "learning_rate": 0.000197258064516129,
      "loss": 1.4499,
      "step": 1274
    },
    {
      "epoch": 3.422818791946309,
      "grad_norm": 0.08591834455728531,
      "learning_rate": 0.00019717741935483868,
      "loss": 1.4721,
      "step": 1275
    },
    {
      "epoch": 3.425503355704698,
      "grad_norm": 0.09039494395256042,
      "learning_rate": 0.00019709677419354837,
      "loss": 1.3895,
      "step": 1276
    },
    {
      "epoch": 3.4281879194630873,
      "grad_norm": 0.09050033986568451,
      "learning_rate": 0.00019701612903225805,
      "loss": 1.4413,
      "step": 1277
    },
    {
      "epoch": 3.4308724832214765,
      "grad_norm": 0.08347725868225098,
      "learning_rate": 0.00019693548387096773,
      "loss": 1.5431,
      "step": 1278
    },
    {
      "epoch": 3.4335570469798657,
      "grad_norm": 0.09331115335226059,
      "learning_rate": 0.0001968548387096774,
      "loss": 1.4711,
      "step": 1279
    },
    {
      "epoch": 3.436241610738255,
      "grad_norm": 0.08914605528116226,
      "learning_rate": 0.00019677419354838707,
      "loss": 1.6389,
      "step": 1280
    },
    {
      "epoch": 3.438926174496644,
      "grad_norm": 0.08426384627819061,
      "learning_rate": 0.00019669354838709675,
      "loss": 1.4641,
      "step": 1281
    },
    {
      "epoch": 3.4416107382550334,
      "grad_norm": 0.09913717955350876,
      "learning_rate": 0.00019661290322580644,
      "loss": 1.4596,
      "step": 1282
    },
    {
      "epoch": 3.4442953020134226,
      "grad_norm": 0.09234728664159775,
      "learning_rate": 0.00019653225806451612,
      "loss": 1.5365,
      "step": 1283
    },
    {
      "epoch": 3.4469798657718123,
      "grad_norm": 0.10940226912498474,
      "learning_rate": 0.00019645161290322578,
      "loss": 1.3818,
      "step": 1284
    },
    {
      "epoch": 3.4496644295302015,
      "grad_norm": 0.08912894874811172,
      "learning_rate": 0.00019637096774193546,
      "loss": 1.4631,
      "step": 1285
    },
    {
      "epoch": 3.4523489932885907,
      "grad_norm": 0.08876575529575348,
      "learning_rate": 0.00019629032258064514,
      "loss": 1.4288,
      "step": 1286
    },
    {
      "epoch": 3.45503355704698,
      "grad_norm": 0.08576808869838715,
      "learning_rate": 0.00019620967741935483,
      "loss": 1.4273,
      "step": 1287
    },
    {
      "epoch": 3.457718120805369,
      "grad_norm": 0.07625452429056168,
      "learning_rate": 0.0001961290322580645,
      "loss": 1.5131,
      "step": 1288
    },
    {
      "epoch": 3.4604026845637583,
      "grad_norm": 0.0825960785150528,
      "learning_rate": 0.00019604838709677417,
      "loss": 1.449,
      "step": 1289
    },
    {
      "epoch": 3.4630872483221475,
      "grad_norm": 0.08292058855295181,
      "learning_rate": 0.00019596774193548385,
      "loss": 1.4809,
      "step": 1290
    },
    {
      "epoch": 3.4657718120805368,
      "grad_norm": 0.10199172794818878,
      "learning_rate": 0.00019588709677419353,
      "loss": 1.3547,
      "step": 1291
    },
    {
      "epoch": 3.468456375838926,
      "grad_norm": 0.08259493857622147,
      "learning_rate": 0.00019580645161290322,
      "loss": 1.4472,
      "step": 1292
    },
    {
      "epoch": 3.4711409395973156,
      "grad_norm": 0.09076210111379623,
      "learning_rate": 0.0001957258064516129,
      "loss": 1.4913,
      "step": 1293
    },
    {
      "epoch": 3.473825503355705,
      "grad_norm": 0.08898961544036865,
      "learning_rate": 0.00019564516129032256,
      "loss": 1.4239,
      "step": 1294
    },
    {
      "epoch": 3.476510067114094,
      "grad_norm": 0.08680308610200882,
      "learning_rate": 0.00019556451612903224,
      "loss": 1.4479,
      "step": 1295
    },
    {
      "epoch": 3.4791946308724833,
      "grad_norm": 0.09388457983732224,
      "learning_rate": 0.00019548387096774192,
      "loss": 1.4393,
      "step": 1296
    },
    {
      "epoch": 3.4818791946308725,
      "grad_norm": 0.10154390335083008,
      "learning_rate": 0.0001954032258064516,
      "loss": 1.4818,
      "step": 1297
    },
    {
      "epoch": 3.4845637583892617,
      "grad_norm": 0.09004577249288559,
      "learning_rate": 0.0001953225806451613,
      "loss": 1.4208,
      "step": 1298
    },
    {
      "epoch": 3.487248322147651,
      "grad_norm": 0.09422850608825684,
      "learning_rate": 0.00019524193548387095,
      "loss": 1.3949,
      "step": 1299
    },
    {
      "epoch": 3.48993288590604,
      "grad_norm": 0.0812436044216156,
      "learning_rate": 0.00019516129032258063,
      "loss": 1.5036,
      "step": 1300
    },
    {
      "epoch": 3.4926174496644293,
      "grad_norm": 0.09147173166275024,
      "learning_rate": 0.0001950806451612903,
      "loss": 1.5011,
      "step": 1301
    },
    {
      "epoch": 3.495302013422819,
      "grad_norm": 0.09724079817533493,
      "learning_rate": 0.000195,
      "loss": 1.444,
      "step": 1302
    },
    {
      "epoch": 3.497986577181208,
      "grad_norm": 0.08938506245613098,
      "learning_rate": 0.00019491935483870968,
      "loss": 1.4602,
      "step": 1303
    },
    {
      "epoch": 3.5006711409395974,
      "grad_norm": 0.09073850512504578,
      "learning_rate": 0.00019483870967741934,
      "loss": 1.3419,
      "step": 1304
    },
    {
      "epoch": 3.5033557046979866,
      "grad_norm": 0.09499990195035934,
      "learning_rate": 0.00019475806451612902,
      "loss": 1.4663,
      "step": 1305
    },
    {
      "epoch": 3.506040268456376,
      "grad_norm": 0.09257277101278305,
      "learning_rate": 0.0001946774193548387,
      "loss": 1.4038,
      "step": 1306
    },
    {
      "epoch": 3.508724832214765,
      "grad_norm": 0.0918470025062561,
      "learning_rate": 0.00019459677419354839,
      "loss": 1.4802,
      "step": 1307
    },
    {
      "epoch": 3.5114093959731543,
      "grad_norm": 0.09224782139062881,
      "learning_rate": 0.00019451612903225807,
      "loss": 1.4381,
      "step": 1308
    },
    {
      "epoch": 3.5140939597315435,
      "grad_norm": 0.07907454669475555,
      "learning_rate": 0.00019443548387096773,
      "loss": 1.5056,
      "step": 1309
    },
    {
      "epoch": 3.5167785234899327,
      "grad_norm": 0.08445057272911072,
      "learning_rate": 0.0001943548387096774,
      "loss": 1.4717,
      "step": 1310
    },
    {
      "epoch": 3.5194630872483224,
      "grad_norm": 0.08937137573957443,
      "learning_rate": 0.0001942741935483871,
      "loss": 1.5188,
      "step": 1311
    },
    {
      "epoch": 3.5221476510067116,
      "grad_norm": 0.09094620496034622,
      "learning_rate": 0.00019419354838709678,
      "loss": 1.4653,
      "step": 1312
    },
    {
      "epoch": 3.524832214765101,
      "grad_norm": 0.0862353965640068,
      "learning_rate": 0.00019411290322580646,
      "loss": 1.4037,
      "step": 1313
    },
    {
      "epoch": 3.52751677852349,
      "grad_norm": 0.0860271155834198,
      "learning_rate": 0.0001940322580645161,
      "loss": 1.4644,
      "step": 1314
    },
    {
      "epoch": 3.530201342281879,
      "grad_norm": 0.0855678990483284,
      "learning_rate": 0.00019395161290322577,
      "loss": 1.4639,
      "step": 1315
    },
    {
      "epoch": 3.5328859060402684,
      "grad_norm": 0.07842112332582474,
      "learning_rate": 0.00019387096774193545,
      "loss": 1.5801,
      "step": 1316
    },
    {
      "epoch": 3.5355704697986576,
      "grad_norm": 0.09631135314702988,
      "learning_rate": 0.00019379032258064517,
      "loss": 1.5164,
      "step": 1317
    },
    {
      "epoch": 3.538255033557047,
      "grad_norm": 0.10340023785829544,
      "learning_rate": 0.00019370967741935485,
      "loss": 1.4503,
      "step": 1318
    },
    {
      "epoch": 3.540939597315436,
      "grad_norm": 0.08846364170312881,
      "learning_rate": 0.00019362903225806448,
      "loss": 1.5533,
      "step": 1319
    },
    {
      "epoch": 3.5436241610738257,
      "grad_norm": 0.09786283224821091,
      "learning_rate": 0.00019354838709677416,
      "loss": 1.4433,
      "step": 1320
    },
    {
      "epoch": 3.546308724832215,
      "grad_norm": 0.08164532482624054,
      "learning_rate": 0.00019346774193548384,
      "loss": 1.5387,
      "step": 1321
    },
    {
      "epoch": 3.548993288590604,
      "grad_norm": 0.08514945954084396,
      "learning_rate": 0.00019338709677419353,
      "loss": 1.6368,
      "step": 1322
    },
    {
      "epoch": 3.5516778523489934,
      "grad_norm": 0.0913906842470169,
      "learning_rate": 0.0001933064516129032,
      "loss": 1.511,
      "step": 1323
    },
    {
      "epoch": 3.5543624161073826,
      "grad_norm": 0.09510308504104614,
      "learning_rate": 0.00019322580645161287,
      "loss": 1.437,
      "step": 1324
    },
    {
      "epoch": 3.557046979865772,
      "grad_norm": 0.08450628817081451,
      "learning_rate": 0.00019314516129032255,
      "loss": 1.436,
      "step": 1325
    },
    {
      "epoch": 3.559731543624161,
      "grad_norm": 0.09040293842554092,
      "learning_rate": 0.00019306451612903223,
      "loss": 1.6182,
      "step": 1326
    },
    {
      "epoch": 3.56241610738255,
      "grad_norm": 0.08563131093978882,
      "learning_rate": 0.00019298387096774192,
      "loss": 1.5152,
      "step": 1327
    },
    {
      "epoch": 3.5651006711409394,
      "grad_norm": 0.08263207226991653,
      "learning_rate": 0.0001929032258064516,
      "loss": 1.5493,
      "step": 1328
    },
    {
      "epoch": 3.567785234899329,
      "grad_norm": 0.09563025087118149,
      "learning_rate": 0.00019282258064516126,
      "loss": 1.5867,
      "step": 1329
    },
    {
      "epoch": 3.570469798657718,
      "grad_norm": 0.09481560438871384,
      "learning_rate": 0.00019274193548387094,
      "loss": 1.4218,
      "step": 1330
    },
    {
      "epoch": 3.5731543624161075,
      "grad_norm": 0.08156786859035492,
      "learning_rate": 0.00019266129032258062,
      "loss": 1.5487,
      "step": 1331
    },
    {
      "epoch": 3.5758389261744967,
      "grad_norm": 0.08584875613451004,
      "learning_rate": 0.0001925806451612903,
      "loss": 1.3366,
      "step": 1332
    },
    {
      "epoch": 3.578523489932886,
      "grad_norm": 0.083762526512146,
      "learning_rate": 0.0001925,
      "loss": 1.4789,
      "step": 1333
    },
    {
      "epoch": 3.581208053691275,
      "grad_norm": 0.09275995939970016,
      "learning_rate": 0.00019241935483870965,
      "loss": 1.4634,
      "step": 1334
    },
    {
      "epoch": 3.5838926174496644,
      "grad_norm": 0.09436023235321045,
      "learning_rate": 0.00019233870967741933,
      "loss": 1.5104,
      "step": 1335
    },
    {
      "epoch": 3.5865771812080536,
      "grad_norm": 0.08045459538698196,
      "learning_rate": 0.000192258064516129,
      "loss": 1.3918,
      "step": 1336
    },
    {
      "epoch": 3.589261744966443,
      "grad_norm": 0.0805194228887558,
      "learning_rate": 0.0001921774193548387,
      "loss": 1.5178,
      "step": 1337
    },
    {
      "epoch": 3.5919463087248324,
      "grad_norm": 0.08917094022035599,
      "learning_rate": 0.00019209677419354838,
      "loss": 1.4238,
      "step": 1338
    },
    {
      "epoch": 3.594630872483221,
      "grad_norm": 0.09870920330286026,
      "learning_rate": 0.00019201612903225804,
      "loss": 1.5093,
      "step": 1339
    },
    {
      "epoch": 3.597315436241611,
      "grad_norm": 0.09041721373796463,
      "learning_rate": 0.00019193548387096772,
      "loss": 1.4552,
      "step": 1340
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.07949332147836685,
      "learning_rate": 0.0001918548387096774,
      "loss": 1.4372,
      "step": 1341
    },
    {
      "epoch": 3.6026845637583893,
      "grad_norm": 0.09562645107507706,
      "learning_rate": 0.00019177419354838709,
      "loss": 1.5182,
      "step": 1342
    },
    {
      "epoch": 3.6053691275167785,
      "grad_norm": 0.08516251295804977,
      "learning_rate": 0.00019169354838709677,
      "loss": 1.5605,
      "step": 1343
    },
    {
      "epoch": 3.6080536912751677,
      "grad_norm": 0.08955243974924088,
      "learning_rate": 0.00019161290322580643,
      "loss": 1.5068,
      "step": 1344
    },
    {
      "epoch": 3.610738255033557,
      "grad_norm": 0.0890895277261734,
      "learning_rate": 0.0001915322580645161,
      "loss": 1.4504,
      "step": 1345
    },
    {
      "epoch": 3.613422818791946,
      "grad_norm": 0.08357280492782593,
      "learning_rate": 0.0001914516129032258,
      "loss": 1.418,
      "step": 1346
    },
    {
      "epoch": 3.616107382550336,
      "grad_norm": 0.11574358493089676,
      "learning_rate": 0.00019137096774193548,
      "loss": 1.4648,
      "step": 1347
    },
    {
      "epoch": 3.6187919463087246,
      "grad_norm": 0.08962966501712799,
      "learning_rate": 0.00019129032258064516,
      "loss": 1.5154,
      "step": 1348
    },
    {
      "epoch": 3.6214765100671142,
      "grad_norm": 0.07725860923528671,
      "learning_rate": 0.00019120967741935481,
      "loss": 1.4647,
      "step": 1349
    },
    {
      "epoch": 3.6241610738255035,
      "grad_norm": 0.08823820948600769,
      "learning_rate": 0.0001911290322580645,
      "loss": 1.4376,
      "step": 1350
    },
    {
      "epoch": 3.6268456375838927,
      "grad_norm": 0.0919634997844696,
      "learning_rate": 0.00019104838709677418,
      "loss": 1.5834,
      "step": 1351
    },
    {
      "epoch": 3.629530201342282,
      "grad_norm": 0.08108524233102798,
      "learning_rate": 0.00019096774193548386,
      "loss": 1.6566,
      "step": 1352
    },
    {
      "epoch": 3.632214765100671,
      "grad_norm": 0.08398422598838806,
      "learning_rate": 0.00019088709677419352,
      "loss": 1.4174,
      "step": 1353
    },
    {
      "epoch": 3.6348993288590603,
      "grad_norm": 0.08993156254291534,
      "learning_rate": 0.0001908064516129032,
      "loss": 1.3817,
      "step": 1354
    },
    {
      "epoch": 3.6375838926174495,
      "grad_norm": 0.10569418966770172,
      "learning_rate": 0.0001907258064516129,
      "loss": 1.3533,
      "step": 1355
    },
    {
      "epoch": 3.640268456375839,
      "grad_norm": 0.09505745768547058,
      "learning_rate": 0.00019064516129032257,
      "loss": 1.4624,
      "step": 1356
    },
    {
      "epoch": 3.642953020134228,
      "grad_norm": 0.09060019254684448,
      "learning_rate": 0.00019056451612903225,
      "loss": 1.4855,
      "step": 1357
    },
    {
      "epoch": 3.6456375838926176,
      "grad_norm": 0.09561420232057571,
      "learning_rate": 0.0001904838709677419,
      "loss": 1.4761,
      "step": 1358
    },
    {
      "epoch": 3.648322147651007,
      "grad_norm": 0.09308324754238129,
      "learning_rate": 0.0001904032258064516,
      "loss": 1.4252,
      "step": 1359
    },
    {
      "epoch": 3.651006711409396,
      "grad_norm": 0.08716175705194473,
      "learning_rate": 0.00019032258064516128,
      "loss": 1.4337,
      "step": 1360
    },
    {
      "epoch": 3.6536912751677852,
      "grad_norm": 0.09138240665197372,
      "learning_rate": 0.00019024193548387096,
      "loss": 1.5361,
      "step": 1361
    },
    {
      "epoch": 3.6563758389261745,
      "grad_norm": 0.08722995966672897,
      "learning_rate": 0.00019016129032258064,
      "loss": 1.4094,
      "step": 1362
    },
    {
      "epoch": 3.6590604026845637,
      "grad_norm": 0.08859998732805252,
      "learning_rate": 0.0001900806451612903,
      "loss": 1.5391,
      "step": 1363
    },
    {
      "epoch": 3.661744966442953,
      "grad_norm": 0.0835317000746727,
      "learning_rate": 0.00018999999999999998,
      "loss": 1.4638,
      "step": 1364
    },
    {
      "epoch": 3.6644295302013425,
      "grad_norm": 0.09512203931808472,
      "learning_rate": 0.00018991935483870967,
      "loss": 1.44,
      "step": 1365
    },
    {
      "epoch": 3.6671140939597313,
      "grad_norm": 0.09335560351610184,
      "learning_rate": 0.00018983870967741935,
      "loss": 1.5922,
      "step": 1366
    },
    {
      "epoch": 3.669798657718121,
      "grad_norm": 0.09918203204870224,
      "learning_rate": 0.00018975806451612903,
      "loss": 1.4802,
      "step": 1367
    },
    {
      "epoch": 3.67248322147651,
      "grad_norm": 0.0827609971165657,
      "learning_rate": 0.0001896774193548387,
      "loss": 1.4924,
      "step": 1368
    },
    {
      "epoch": 3.6751677852348994,
      "grad_norm": 0.08995568007230759,
      "learning_rate": 0.00018959677419354837,
      "loss": 1.4103,
      "step": 1369
    },
    {
      "epoch": 3.6778523489932886,
      "grad_norm": 0.08637847751379013,
      "learning_rate": 0.00018951612903225806,
      "loss": 1.4654,
      "step": 1370
    },
    {
      "epoch": 3.680536912751678,
      "grad_norm": 0.07612459361553192,
      "learning_rate": 0.00018943548387096774,
      "loss": 1.4911,
      "step": 1371
    },
    {
      "epoch": 3.683221476510067,
      "grad_norm": 0.08757325261831284,
      "learning_rate": 0.00018935483870967742,
      "loss": 1.4333,
      "step": 1372
    },
    {
      "epoch": 3.6859060402684563,
      "grad_norm": 0.08605517446994781,
      "learning_rate": 0.00018927419354838705,
      "loss": 1.5011,
      "step": 1373
    },
    {
      "epoch": 3.688590604026846,
      "grad_norm": 0.08272787183523178,
      "learning_rate": 0.00018919354838709676,
      "loss": 1.5158,
      "step": 1374
    },
    {
      "epoch": 3.6912751677852347,
      "grad_norm": 0.09311359375715256,
      "learning_rate": 0.00018911290322580645,
      "loss": 1.3589,
      "step": 1375
    },
    {
      "epoch": 3.6939597315436243,
      "grad_norm": 0.0873265489935875,
      "learning_rate": 0.00018903225806451613,
      "loss": 1.4432,
      "step": 1376
    },
    {
      "epoch": 3.6966442953020135,
      "grad_norm": 0.08710207045078278,
      "learning_rate": 0.0001889516129032258,
      "loss": 1.4189,
      "step": 1377
    },
    {
      "epoch": 3.6993288590604028,
      "grad_norm": 0.09478066861629486,
      "learning_rate": 0.00018887096774193544,
      "loss": 1.4352,
      "step": 1378
    },
    {
      "epoch": 3.702013422818792,
      "grad_norm": 0.0803355872631073,
      "learning_rate": 0.00018879032258064512,
      "loss": 1.4902,
      "step": 1379
    },
    {
      "epoch": 3.704697986577181,
      "grad_norm": 0.08480458706617355,
      "learning_rate": 0.0001887096774193548,
      "loss": 1.5555,
      "step": 1380
    },
    {
      "epoch": 3.7073825503355704,
      "grad_norm": 0.10744447261095047,
      "learning_rate": 0.00018862903225806452,
      "loss": 1.4639,
      "step": 1381
    },
    {
      "epoch": 3.7100671140939596,
      "grad_norm": 0.0972062423825264,
      "learning_rate": 0.0001885483870967742,
      "loss": 1.5254,
      "step": 1382
    },
    {
      "epoch": 3.712751677852349,
      "grad_norm": 0.08355914056301117,
      "learning_rate": 0.00018846774193548383,
      "loss": 1.6825,
      "step": 1383
    },
    {
      "epoch": 3.715436241610738,
      "grad_norm": 0.07639114558696747,
      "learning_rate": 0.00018838709677419351,
      "loss": 1.4568,
      "step": 1384
    },
    {
      "epoch": 3.7181208053691277,
      "grad_norm": 0.08744034171104431,
      "learning_rate": 0.0001883064516129032,
      "loss": 1.5642,
      "step": 1385
    },
    {
      "epoch": 3.720805369127517,
      "grad_norm": 0.07956042885780334,
      "learning_rate": 0.00018822580645161288,
      "loss": 1.3175,
      "step": 1386
    },
    {
      "epoch": 3.723489932885906,
      "grad_norm": 0.08372361212968826,
      "learning_rate": 0.00018814516129032256,
      "loss": 1.4774,
      "step": 1387
    },
    {
      "epoch": 3.7261744966442953,
      "grad_norm": 0.08847317844629288,
      "learning_rate": 0.00018806451612903222,
      "loss": 1.6054,
      "step": 1388
    },
    {
      "epoch": 3.7288590604026846,
      "grad_norm": 0.10094708949327469,
      "learning_rate": 0.0001879838709677419,
      "loss": 1.4077,
      "step": 1389
    },
    {
      "epoch": 3.7315436241610738,
      "grad_norm": 0.09392957389354706,
      "learning_rate": 0.0001879032258064516,
      "loss": 1.5241,
      "step": 1390
    },
    {
      "epoch": 3.734228187919463,
      "grad_norm": 0.0775735080242157,
      "learning_rate": 0.00018782258064516127,
      "loss": 1.5928,
      "step": 1391
    },
    {
      "epoch": 3.736912751677852,
      "grad_norm": 0.08585906773805618,
      "learning_rate": 0.00018774193548387095,
      "loss": 1.5352,
      "step": 1392
    },
    {
      "epoch": 3.7395973154362414,
      "grad_norm": 0.08898156881332397,
      "learning_rate": 0.0001876612903225806,
      "loss": 1.4697,
      "step": 1393
    },
    {
      "epoch": 3.742281879194631,
      "grad_norm": 0.09811985492706299,
      "learning_rate": 0.0001875806451612903,
      "loss": 1.4159,
      "step": 1394
    },
    {
      "epoch": 3.7449664429530203,
      "grad_norm": 0.09692229330539703,
      "learning_rate": 0.00018749999999999998,
      "loss": 1.4195,
      "step": 1395
    },
    {
      "epoch": 3.7476510067114095,
      "grad_norm": 0.09382855892181396,
      "learning_rate": 0.00018741935483870966,
      "loss": 1.3834,
      "step": 1396
    },
    {
      "epoch": 3.7503355704697987,
      "grad_norm": 0.0931670144200325,
      "learning_rate": 0.00018733870967741934,
      "loss": 1.4933,
      "step": 1397
    },
    {
      "epoch": 3.753020134228188,
      "grad_norm": 0.09364566951990128,
      "learning_rate": 0.000187258064516129,
      "loss": 1.6159,
      "step": 1398
    },
    {
      "epoch": 3.755704697986577,
      "grad_norm": 0.07814498990774155,
      "learning_rate": 0.00018717741935483868,
      "loss": 1.5434,
      "step": 1399
    },
    {
      "epoch": 3.7583892617449663,
      "grad_norm": 0.07833440601825714,
      "learning_rate": 0.00018709677419354837,
      "loss": 1.5156,
      "step": 1400
    },
    {
      "epoch": 3.7610738255033556,
      "grad_norm": 0.08280273526906967,
      "learning_rate": 0.00018701612903225805,
      "loss": 1.501,
      "step": 1401
    },
    {
      "epoch": 3.7637583892617448,
      "grad_norm": 0.08071088045835495,
      "learning_rate": 0.00018693548387096773,
      "loss": 1.4712,
      "step": 1402
    },
    {
      "epoch": 3.7664429530201344,
      "grad_norm": 0.07933926582336426,
      "learning_rate": 0.0001868548387096774,
      "loss": 1.5147,
      "step": 1403
    },
    {
      "epoch": 3.7691275167785236,
      "grad_norm": 0.08349589258432388,
      "learning_rate": 0.00018677419354838707,
      "loss": 1.4192,
      "step": 1404
    },
    {
      "epoch": 3.771812080536913,
      "grad_norm": 0.0981646403670311,
      "learning_rate": 0.00018669354838709676,
      "loss": 1.4703,
      "step": 1405
    },
    {
      "epoch": 3.774496644295302,
      "grad_norm": 0.08826761692762375,
      "learning_rate": 0.00018661290322580644,
      "loss": 1.4777,
      "step": 1406
    },
    {
      "epoch": 3.7771812080536913,
      "grad_norm": 0.08856076747179031,
      "learning_rate": 0.00018653225806451612,
      "loss": 1.3943,
      "step": 1407
    },
    {
      "epoch": 3.7798657718120805,
      "grad_norm": 0.08890832960605621,
      "learning_rate": 0.00018645161290322578,
      "loss": 1.4869,
      "step": 1408
    },
    {
      "epoch": 3.7825503355704697,
      "grad_norm": 0.07268444448709488,
      "learning_rate": 0.00018637096774193546,
      "loss": 1.5781,
      "step": 1409
    },
    {
      "epoch": 3.785234899328859,
      "grad_norm": 0.08420027047395706,
      "learning_rate": 0.00018629032258064515,
      "loss": 1.4464,
      "step": 1410
    },
    {
      "epoch": 3.787919463087248,
      "grad_norm": 0.09196563065052032,
      "learning_rate": 0.00018620967741935483,
      "loss": 1.4052,
      "step": 1411
    },
    {
      "epoch": 3.790604026845638,
      "grad_norm": 0.0961800217628479,
      "learning_rate": 0.0001861290322580645,
      "loss": 1.4014,
      "step": 1412
    },
    {
      "epoch": 3.793288590604027,
      "grad_norm": 0.08722516149282455,
      "learning_rate": 0.00018604838709677417,
      "loss": 1.4839,
      "step": 1413
    },
    {
      "epoch": 3.7959731543624162,
      "grad_norm": 0.11345908045768738,
      "learning_rate": 0.00018596774193548385,
      "loss": 1.2934,
      "step": 1414
    },
    {
      "epoch": 3.7986577181208054,
      "grad_norm": 0.09799838066101074,
      "learning_rate": 0.00018588709677419353,
      "loss": 1.3806,
      "step": 1415
    },
    {
      "epoch": 3.8013422818791947,
      "grad_norm": 0.08375374972820282,
      "learning_rate": 0.00018580645161290322,
      "loss": 1.5452,
      "step": 1416
    },
    {
      "epoch": 3.804026845637584,
      "grad_norm": 0.08759572356939316,
      "learning_rate": 0.0001857258064516129,
      "loss": 1.4401,
      "step": 1417
    },
    {
      "epoch": 3.806711409395973,
      "grad_norm": 0.08297641575336456,
      "learning_rate": 0.00018564516129032256,
      "loss": 1.3688,
      "step": 1418
    },
    {
      "epoch": 3.8093959731543623,
      "grad_norm": 0.09173183143138885,
      "learning_rate": 0.00018556451612903224,
      "loss": 1.3797,
      "step": 1419
    },
    {
      "epoch": 3.8120805369127515,
      "grad_norm": 0.08489927649497986,
      "learning_rate": 0.00018548387096774192,
      "loss": 1.5781,
      "step": 1420
    },
    {
      "epoch": 3.814765100671141,
      "grad_norm": 0.0870487242937088,
      "learning_rate": 0.0001854032258064516,
      "loss": 1.4103,
      "step": 1421
    },
    {
      "epoch": 3.81744966442953,
      "grad_norm": 0.0857723280787468,
      "learning_rate": 0.0001853225806451613,
      "loss": 1.4478,
      "step": 1422
    },
    {
      "epoch": 3.8201342281879196,
      "grad_norm": 0.0938258245587349,
      "learning_rate": 0.00018524193548387095,
      "loss": 1.359,
      "step": 1423
    },
    {
      "epoch": 3.822818791946309,
      "grad_norm": 0.08697718381881714,
      "learning_rate": 0.00018516129032258063,
      "loss": 1.3832,
      "step": 1424
    },
    {
      "epoch": 3.825503355704698,
      "grad_norm": 0.07843165844678879,
      "learning_rate": 0.00018508064516129031,
      "loss": 1.6996,
      "step": 1425
    },
    {
      "epoch": 3.8281879194630872,
      "grad_norm": 0.09468839317560196,
      "learning_rate": 0.000185,
      "loss": 1.3765,
      "step": 1426
    },
    {
      "epoch": 3.8308724832214764,
      "grad_norm": 0.08893487602472305,
      "learning_rate": 0.00018491935483870968,
      "loss": 1.4567,
      "step": 1427
    },
    {
      "epoch": 3.8335570469798657,
      "grad_norm": 0.07995465397834778,
      "learning_rate": 0.00018483870967741934,
      "loss": 1.5263,
      "step": 1428
    },
    {
      "epoch": 3.836241610738255,
      "grad_norm": 0.09177960455417633,
      "learning_rate": 0.00018475806451612902,
      "loss": 1.4757,
      "step": 1429
    },
    {
      "epoch": 3.8389261744966445,
      "grad_norm": 0.09662647545337677,
      "learning_rate": 0.0001846774193548387,
      "loss": 1.3129,
      "step": 1430
    },
    {
      "epoch": 3.8416107382550333,
      "grad_norm": 0.09646054357290268,
      "learning_rate": 0.0001845967741935484,
      "loss": 1.5559,
      "step": 1431
    },
    {
      "epoch": 3.844295302013423,
      "grad_norm": 0.09208470582962036,
      "learning_rate": 0.00018451612903225807,
      "loss": 1.5599,
      "step": 1432
    },
    {
      "epoch": 3.846979865771812,
      "grad_norm": 0.09980012476444244,
      "learning_rate": 0.00018443548387096773,
      "loss": 1.6352,
      "step": 1433
    },
    {
      "epoch": 3.8496644295302014,
      "grad_norm": 0.09709254652261734,
      "learning_rate": 0.0001843548387096774,
      "loss": 1.3855,
      "step": 1434
    },
    {
      "epoch": 3.8523489932885906,
      "grad_norm": 0.10129788517951965,
      "learning_rate": 0.0001842741935483871,
      "loss": 1.4827,
      "step": 1435
    },
    {
      "epoch": 3.85503355704698,
      "grad_norm": 0.08648108690977097,
      "learning_rate": 0.00018419354838709678,
      "loss": 1.5377,
      "step": 1436
    },
    {
      "epoch": 3.857718120805369,
      "grad_norm": 0.07876140624284744,
      "learning_rate": 0.00018411290322580646,
      "loss": 1.5054,
      "step": 1437
    },
    {
      "epoch": 3.8604026845637582,
      "grad_norm": 0.09056749194860458,
      "learning_rate": 0.00018403225806451612,
      "loss": 1.4698,
      "step": 1438
    },
    {
      "epoch": 3.863087248322148,
      "grad_norm": 0.08492676913738251,
      "learning_rate": 0.0001839516129032258,
      "loss": 1.6141,
      "step": 1439
    },
    {
      "epoch": 3.8657718120805367,
      "grad_norm": 0.08331742882728577,
      "learning_rate": 0.00018387096774193548,
      "loss": 1.5481,
      "step": 1440
    },
    {
      "epoch": 3.8684563758389263,
      "grad_norm": 0.08318706601858139,
      "learning_rate": 0.00018379032258064517,
      "loss": 1.5118,
      "step": 1441
    },
    {
      "epoch": 3.8711409395973155,
      "grad_norm": 0.08626407384872437,
      "learning_rate": 0.0001837096774193548,
      "loss": 1.5146,
      "step": 1442
    },
    {
      "epoch": 3.8738255033557047,
      "grad_norm": 0.08658436685800552,
      "learning_rate": 0.00018362903225806448,
      "loss": 1.5253,
      "step": 1443
    },
    {
      "epoch": 3.876510067114094,
      "grad_norm": 0.0883958488702774,
      "learning_rate": 0.00018354838709677416,
      "loss": 1.4738,
      "step": 1444
    },
    {
      "epoch": 3.879194630872483,
      "grad_norm": 0.09184018522500992,
      "learning_rate": 0.00018346774193548385,
      "loss": 1.5201,
      "step": 1445
    },
    {
      "epoch": 3.8818791946308724,
      "grad_norm": 0.09031729400157928,
      "learning_rate": 0.00018338709677419356,
      "loss": 1.3513,
      "step": 1446
    },
    {
      "epoch": 3.8845637583892616,
      "grad_norm": 0.09611786901950836,
      "learning_rate": 0.00018330645161290318,
      "loss": 1.4974,
      "step": 1447
    },
    {
      "epoch": 3.8872483221476513,
      "grad_norm": 0.08690258860588074,
      "learning_rate": 0.00018322580645161287,
      "loss": 1.3711,
      "step": 1448
    },
    {
      "epoch": 3.88993288590604,
      "grad_norm": 0.08269494026899338,
      "learning_rate": 0.00018314516129032255,
      "loss": 1.5651,
      "step": 1449
    },
    {
      "epoch": 3.8926174496644297,
      "grad_norm": 0.08422695845365524,
      "learning_rate": 0.00018306451612903223,
      "loss": 1.4402,
      "step": 1450
    },
    {
      "epoch": 3.895302013422819,
      "grad_norm": 0.0772521048784256,
      "learning_rate": 0.00018298387096774192,
      "loss": 1.446,
      "step": 1451
    },
    {
      "epoch": 3.897986577181208,
      "grad_norm": 0.0884644016623497,
      "learning_rate": 0.00018290322580645157,
      "loss": 1.4578,
      "step": 1452
    },
    {
      "epoch": 3.9006711409395973,
      "grad_norm": 0.08848585188388824,
      "learning_rate": 0.00018282258064516126,
      "loss": 1.4925,
      "step": 1453
    },
    {
      "epoch": 3.9033557046979865,
      "grad_norm": 0.09158150106668472,
      "learning_rate": 0.00018274193548387094,
      "loss": 1.3817,
      "step": 1454
    },
    {
      "epoch": 3.9060402684563758,
      "grad_norm": 0.08941861242055893,
      "learning_rate": 0.00018266129032258062,
      "loss": 1.4691,
      "step": 1455
    },
    {
      "epoch": 3.908724832214765,
      "grad_norm": 0.09168027341365814,
      "learning_rate": 0.0001825806451612903,
      "loss": 1.5132,
      "step": 1456
    },
    {
      "epoch": 3.9114093959731546,
      "grad_norm": 0.09071172028779984,
      "learning_rate": 0.00018249999999999996,
      "loss": 1.5089,
      "step": 1457
    },
    {
      "epoch": 3.9140939597315434,
      "grad_norm": 0.08627181500196457,
      "learning_rate": 0.00018241935483870965,
      "loss": 1.5625,
      "step": 1458
    },
    {
      "epoch": 3.916778523489933,
      "grad_norm": 0.08542993664741516,
      "learning_rate": 0.00018233870967741933,
      "loss": 1.4465,
      "step": 1459
    },
    {
      "epoch": 3.9194630872483223,
      "grad_norm": 0.0967249721288681,
      "learning_rate": 0.00018225806451612901,
      "loss": 1.3772,
      "step": 1460
    },
    {
      "epoch": 3.9221476510067115,
      "grad_norm": 0.0844162106513977,
      "learning_rate": 0.0001821774193548387,
      "loss": 1.5731,
      "step": 1461
    },
    {
      "epoch": 3.9248322147651007,
      "grad_norm": 0.0906507670879364,
      "learning_rate": 0.00018209677419354835,
      "loss": 1.4207,
      "step": 1462
    },
    {
      "epoch": 3.92751677852349,
      "grad_norm": 0.0910281166434288,
      "learning_rate": 0.00018201612903225804,
      "loss": 1.5671,
      "step": 1463
    },
    {
      "epoch": 3.930201342281879,
      "grad_norm": 0.08889240771532059,
      "learning_rate": 0.00018193548387096772,
      "loss": 1.4969,
      "step": 1464
    },
    {
      "epoch": 3.9328859060402683,
      "grad_norm": 0.08200394362211227,
      "learning_rate": 0.0001818548387096774,
      "loss": 1.5358,
      "step": 1465
    },
    {
      "epoch": 3.935570469798658,
      "grad_norm": 0.08400129526853561,
      "learning_rate": 0.0001817741935483871,
      "loss": 1.4867,
      "step": 1466
    },
    {
      "epoch": 3.9382550335570468,
      "grad_norm": 0.0995764434337616,
      "learning_rate": 0.00018169354838709674,
      "loss": 1.3634,
      "step": 1467
    },
    {
      "epoch": 3.9409395973154364,
      "grad_norm": 0.093739815056324,
      "learning_rate": 0.00018161290322580643,
      "loss": 1.502,
      "step": 1468
    },
    {
      "epoch": 3.9436241610738256,
      "grad_norm": 0.0863904282450676,
      "learning_rate": 0.0001815322580645161,
      "loss": 1.4775,
      "step": 1469
    },
    {
      "epoch": 3.946308724832215,
      "grad_norm": 0.09084276854991913,
      "learning_rate": 0.0001814516129032258,
      "loss": 1.345,
      "step": 1470
    },
    {
      "epoch": 3.948993288590604,
      "grad_norm": 0.0829220563173294,
      "learning_rate": 0.00018137096774193548,
      "loss": 1.4686,
      "step": 1471
    },
    {
      "epoch": 3.9516778523489933,
      "grad_norm": 0.09177819639444351,
      "learning_rate": 0.00018129032258064513,
      "loss": 1.4417,
      "step": 1472
    },
    {
      "epoch": 3.9543624161073825,
      "grad_norm": 0.08517049252986908,
      "learning_rate": 0.00018120967741935482,
      "loss": 1.3994,
      "step": 1473
    },
    {
      "epoch": 3.9570469798657717,
      "grad_norm": 0.09547650814056396,
      "learning_rate": 0.0001811290322580645,
      "loss": 1.5057,
      "step": 1474
    },
    {
      "epoch": 3.959731543624161,
      "grad_norm": 0.07851018756628036,
      "learning_rate": 0.00018104838709677418,
      "loss": 1.4414,
      "step": 1475
    },
    {
      "epoch": 3.96241610738255,
      "grad_norm": 0.08504722267389297,
      "learning_rate": 0.00018096774193548387,
      "loss": 1.5118,
      "step": 1476
    },
    {
      "epoch": 3.96510067114094,
      "grad_norm": 0.0903695747256279,
      "learning_rate": 0.00018088709677419352,
      "loss": 1.4331,
      "step": 1477
    },
    {
      "epoch": 3.967785234899329,
      "grad_norm": 0.0911685973405838,
      "learning_rate": 0.0001808064516129032,
      "loss": 1.5733,
      "step": 1478
    },
    {
      "epoch": 3.970469798657718,
      "grad_norm": 0.08451711386442184,
      "learning_rate": 0.0001807258064516129,
      "loss": 1.5404,
      "step": 1479
    },
    {
      "epoch": 3.9731543624161074,
      "grad_norm": 0.08886577188968658,
      "learning_rate": 0.00018064516129032257,
      "loss": 1.4704,
      "step": 1480
    },
    {
      "epoch": 3.9758389261744966,
      "grad_norm": 0.082906574010849,
      "learning_rate": 0.00018056451612903226,
      "loss": 1.4779,
      "step": 1481
    },
    {
      "epoch": 3.978523489932886,
      "grad_norm": 0.08339408785104752,
      "learning_rate": 0.0001804838709677419,
      "loss": 1.4585,
      "step": 1482
    },
    {
      "epoch": 3.981208053691275,
      "grad_norm": 0.08893697708845139,
      "learning_rate": 0.0001804032258064516,
      "loss": 1.4936,
      "step": 1483
    },
    {
      "epoch": 3.9838926174496643,
      "grad_norm": 0.09435944259166718,
      "learning_rate": 0.00018032258064516128,
      "loss": 1.4468,
      "step": 1484
    },
    {
      "epoch": 3.9865771812080535,
      "grad_norm": 0.09824756532907486,
      "learning_rate": 0.00018024193548387096,
      "loss": 1.474,
      "step": 1485
    },
    {
      "epoch": 3.989261744966443,
      "grad_norm": 0.08924201130867004,
      "learning_rate": 0.00018016129032258064,
      "loss": 1.4297,
      "step": 1486
    },
    {
      "epoch": 3.9919463087248324,
      "grad_norm": 0.1026274710893631,
      "learning_rate": 0.0001800806451612903,
      "loss": 1.4379,
      "step": 1487
    },
    {
      "epoch": 3.9946308724832216,
      "grad_norm": 0.08068455010652542,
      "learning_rate": 0.00017999999999999998,
      "loss": 1.4048,
      "step": 1488
    },
    {
      "epoch": 3.997315436241611,
      "grad_norm": 0.08598116040229797,
      "learning_rate": 0.00017991935483870967,
      "loss": 1.4581,
      "step": 1489
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.09042791277170181,
      "learning_rate": 0.00017983870967741935,
      "loss": 1.5465,
      "step": 1490
    },
    {
      "epoch": 4.00268456375839,
      "grad_norm": 0.08353465795516968,
      "learning_rate": 0.00017975806451612903,
      "loss": 1.4701,
      "step": 1491
    },
    {
      "epoch": 4.005369127516778,
      "grad_norm": 0.0832185298204422,
      "learning_rate": 0.0001796774193548387,
      "loss": 1.4459,
      "step": 1492
    },
    {
      "epoch": 4.008053691275168,
      "grad_norm": 0.09188055992126465,
      "learning_rate": 0.00017959677419354837,
      "loss": 1.4757,
      "step": 1493
    },
    {
      "epoch": 4.010738255033557,
      "grad_norm": 0.08533879369497299,
      "learning_rate": 0.00017951612903225806,
      "loss": 1.4028,
      "step": 1494
    },
    {
      "epoch": 4.0134228187919465,
      "grad_norm": 0.09248896688222885,
      "learning_rate": 0.00017943548387096774,
      "loss": 1.5916,
      "step": 1495
    },
    {
      "epoch": 4.016107382550335,
      "grad_norm": 0.08668967336416245,
      "learning_rate": 0.00017935483870967742,
      "loss": 1.5031,
      "step": 1496
    },
    {
      "epoch": 4.018791946308725,
      "grad_norm": 0.08222559839487076,
      "learning_rate": 0.00017927419354838708,
      "loss": 1.4833,
      "step": 1497
    },
    {
      "epoch": 4.021476510067114,
      "grad_norm": 0.09361616522073746,
      "learning_rate": 0.00017919354838709676,
      "loss": 1.4106,
      "step": 1498
    },
    {
      "epoch": 4.024161073825503,
      "grad_norm": 0.09167680889368057,
      "learning_rate": 0.00017911290322580645,
      "loss": 1.5617,
      "step": 1499
    },
    {
      "epoch": 4.026845637583893,
      "grad_norm": 0.09435399621725082,
      "learning_rate": 0.00017903225806451613,
      "loss": 1.4878,
      "step": 1500
    },
    {
      "epoch": 4.029530201342282,
      "grad_norm": 0.08513528108596802,
      "learning_rate": 0.0001789516129032258,
      "loss": 1.4251,
      "step": 1501
    },
    {
      "epoch": 4.0322147651006714,
      "grad_norm": 0.08601792901754379,
      "learning_rate": 0.00017887096774193544,
      "loss": 1.5277,
      "step": 1502
    },
    {
      "epoch": 4.03489932885906,
      "grad_norm": 0.09118276834487915,
      "learning_rate": 0.00017879032258064515,
      "loss": 1.3747,
      "step": 1503
    },
    {
      "epoch": 4.03758389261745,
      "grad_norm": 0.08737479150295258,
      "learning_rate": 0.00017870967741935484,
      "loss": 1.4463,
      "step": 1504
    },
    {
      "epoch": 4.040268456375839,
      "grad_norm": 0.0870261862874031,
      "learning_rate": 0.00017862903225806452,
      "loss": 1.5547,
      "step": 1505
    },
    {
      "epoch": 4.042953020134228,
      "grad_norm": 0.08728665113449097,
      "learning_rate": 0.0001785483870967742,
      "loss": 1.3872,
      "step": 1506
    },
    {
      "epoch": 4.045637583892617,
      "grad_norm": 0.08719915896654129,
      "learning_rate": 0.00017846774193548383,
      "loss": 1.5371,
      "step": 1507
    },
    {
      "epoch": 4.048322147651007,
      "grad_norm": 0.08444516360759735,
      "learning_rate": 0.00017838709677419352,
      "loss": 1.5887,
      "step": 1508
    },
    {
      "epoch": 4.051006711409396,
      "grad_norm": 0.08575133979320526,
      "learning_rate": 0.0001783064516129032,
      "loss": 1.4118,
      "step": 1509
    },
    {
      "epoch": 4.053691275167785,
      "grad_norm": 0.08589904010295868,
      "learning_rate": 0.0001782258064516129,
      "loss": 1.4408,
      "step": 1510
    },
    {
      "epoch": 4.056375838926175,
      "grad_norm": 0.08567202091217041,
      "learning_rate": 0.0001781451612903226,
      "loss": 1.4863,
      "step": 1511
    },
    {
      "epoch": 4.059060402684564,
      "grad_norm": 0.09018702805042267,
      "learning_rate": 0.00017806451612903222,
      "loss": 1.4041,
      "step": 1512
    },
    {
      "epoch": 4.061744966442953,
      "grad_norm": 0.09212110191583633,
      "learning_rate": 0.0001779838709677419,
      "loss": 1.5011,
      "step": 1513
    },
    {
      "epoch": 4.064429530201342,
      "grad_norm": 0.09563595801591873,
      "learning_rate": 0.0001779032258064516,
      "loss": 1.399,
      "step": 1514
    },
    {
      "epoch": 4.067114093959732,
      "grad_norm": 0.07315144687891006,
      "learning_rate": 0.00017782258064516127,
      "loss": 1.5378,
      "step": 1515
    },
    {
      "epoch": 4.06979865771812,
      "grad_norm": 0.08736751973628998,
      "learning_rate": 0.00017774193548387095,
      "loss": 1.4347,
      "step": 1516
    },
    {
      "epoch": 4.07248322147651,
      "grad_norm": 0.08744935691356659,
      "learning_rate": 0.0001776612903225806,
      "loss": 1.4114,
      "step": 1517
    },
    {
      "epoch": 4.0751677852349,
      "grad_norm": 0.0857372134923935,
      "learning_rate": 0.0001775806451612903,
      "loss": 1.4794,
      "step": 1518
    },
    {
      "epoch": 4.0778523489932885,
      "grad_norm": 0.08728300034999847,
      "learning_rate": 0.00017749999999999998,
      "loss": 1.4703,
      "step": 1519
    },
    {
      "epoch": 4.080536912751678,
      "grad_norm": 0.08644450455904007,
      "learning_rate": 0.00017741935483870966,
      "loss": 1.4283,
      "step": 1520
    },
    {
      "epoch": 4.083221476510067,
      "grad_norm": 0.09273224323987961,
      "learning_rate": 0.00017733870967741934,
      "loss": 1.5572,
      "step": 1521
    },
    {
      "epoch": 4.085906040268457,
      "grad_norm": 0.08894568681716919,
      "learning_rate": 0.000177258064516129,
      "loss": 1.4217,
      "step": 1522
    },
    {
      "epoch": 4.088590604026845,
      "grad_norm": 0.09147366136312485,
      "learning_rate": 0.00017717741935483868,
      "loss": 1.4121,
      "step": 1523
    },
    {
      "epoch": 4.091275167785235,
      "grad_norm": 0.09797993302345276,
      "learning_rate": 0.00017709677419354837,
      "loss": 1.3494,
      "step": 1524
    },
    {
      "epoch": 4.093959731543624,
      "grad_norm": 0.0968843400478363,
      "learning_rate": 0.00017701612903225805,
      "loss": 1.3608,
      "step": 1525
    },
    {
      "epoch": 4.0966442953020135,
      "grad_norm": 0.0931602269411087,
      "learning_rate": 0.00017693548387096773,
      "loss": 1.4416,
      "step": 1526
    },
    {
      "epoch": 4.099328859060403,
      "grad_norm": 0.09616092592477798,
      "learning_rate": 0.0001768548387096774,
      "loss": 1.4292,
      "step": 1527
    },
    {
      "epoch": 4.102013422818792,
      "grad_norm": 0.09495245665311813,
      "learning_rate": 0.00017677419354838707,
      "loss": 1.407,
      "step": 1528
    },
    {
      "epoch": 4.1046979865771815,
      "grad_norm": 0.0997631624341011,
      "learning_rate": 0.00017669354838709676,
      "loss": 1.3622,
      "step": 1529
    },
    {
      "epoch": 4.10738255033557,
      "grad_norm": 0.08707037568092346,
      "learning_rate": 0.00017661290322580644,
      "loss": 1.4721,
      "step": 1530
    },
    {
      "epoch": 4.11006711409396,
      "grad_norm": 0.09218420833349228,
      "learning_rate": 0.00017653225806451612,
      "loss": 1.4228,
      "step": 1531
    },
    {
      "epoch": 4.112751677852349,
      "grad_norm": 0.09142537415027618,
      "learning_rate": 0.00017645161290322578,
      "loss": 1.4318,
      "step": 1532
    },
    {
      "epoch": 4.115436241610738,
      "grad_norm": 0.10041626542806625,
      "learning_rate": 0.00017637096774193546,
      "loss": 1.2871,
      "step": 1533
    },
    {
      "epoch": 4.118120805369127,
      "grad_norm": 0.09634897857904434,
      "learning_rate": 0.00017629032258064515,
      "loss": 1.4986,
      "step": 1534
    },
    {
      "epoch": 4.120805369127517,
      "grad_norm": 0.09097161889076233,
      "learning_rate": 0.00017620967741935483,
      "loss": 1.5275,
      "step": 1535
    },
    {
      "epoch": 4.1234899328859065,
      "grad_norm": 0.08574388921260834,
      "learning_rate": 0.00017612903225806449,
      "loss": 1.3472,
      "step": 1536
    },
    {
      "epoch": 4.126174496644295,
      "grad_norm": 0.09160105139017105,
      "learning_rate": 0.00017604838709677417,
      "loss": 1.5309,
      "step": 1537
    },
    {
      "epoch": 4.128859060402685,
      "grad_norm": 0.09460984915494919,
      "learning_rate": 0.00017596774193548385,
      "loss": 1.5786,
      "step": 1538
    },
    {
      "epoch": 4.131543624161074,
      "grad_norm": 0.08290573209524155,
      "learning_rate": 0.00017588709677419354,
      "loss": 1.5265,
      "step": 1539
    },
    {
      "epoch": 4.134228187919463,
      "grad_norm": 0.07957281917333603,
      "learning_rate": 0.00017580645161290322,
      "loss": 1.5682,
      "step": 1540
    },
    {
      "epoch": 4.136912751677852,
      "grad_norm": 0.09165401011705399,
      "learning_rate": 0.00017572580645161288,
      "loss": 1.4772,
      "step": 1541
    },
    {
      "epoch": 4.139597315436242,
      "grad_norm": 0.09961459785699844,
      "learning_rate": 0.00017564516129032256,
      "loss": 1.5053,
      "step": 1542
    },
    {
      "epoch": 4.1422818791946305,
      "grad_norm": 0.09646600484848022,
      "learning_rate": 0.00017556451612903224,
      "loss": 1.4442,
      "step": 1543
    },
    {
      "epoch": 4.14496644295302,
      "grad_norm": 0.09490130096673965,
      "learning_rate": 0.00017548387096774193,
      "loss": 1.4863,
      "step": 1544
    },
    {
      "epoch": 4.14765100671141,
      "grad_norm": 0.08245490491390228,
      "learning_rate": 0.0001754032258064516,
      "loss": 1.4605,
      "step": 1545
    },
    {
      "epoch": 4.150335570469799,
      "grad_norm": 0.09332932531833649,
      "learning_rate": 0.00017532258064516126,
      "loss": 1.509,
      "step": 1546
    },
    {
      "epoch": 4.153020134228188,
      "grad_norm": 0.08039798587560654,
      "learning_rate": 0.00017524193548387095,
      "loss": 1.5842,
      "step": 1547
    },
    {
      "epoch": 4.155704697986577,
      "grad_norm": 0.08983441442251205,
      "learning_rate": 0.00017516129032258063,
      "loss": 1.4183,
      "step": 1548
    },
    {
      "epoch": 4.158389261744967,
      "grad_norm": 0.08361785858869553,
      "learning_rate": 0.00017508064516129031,
      "loss": 1.497,
      "step": 1549
    },
    {
      "epoch": 4.1610738255033555,
      "grad_norm": 0.09092840552330017,
      "learning_rate": 0.000175,
      "loss": 1.4657,
      "step": 1550
    },
    {
      "epoch": 4.163758389261745,
      "grad_norm": 0.09442120045423508,
      "learning_rate": 0.00017491935483870965,
      "loss": 1.4068,
      "step": 1551
    },
    {
      "epoch": 4.166442953020134,
      "grad_norm": 0.08725211024284363,
      "learning_rate": 0.00017483870967741934,
      "loss": 1.4186,
      "step": 1552
    },
    {
      "epoch": 4.169127516778524,
      "grad_norm": 0.08686526864767075,
      "learning_rate": 0.00017475806451612902,
      "loss": 1.4586,
      "step": 1553
    },
    {
      "epoch": 4.171812080536912,
      "grad_norm": 0.08188356459140778,
      "learning_rate": 0.0001746774193548387,
      "loss": 1.5189,
      "step": 1554
    },
    {
      "epoch": 4.174496644295302,
      "grad_norm": 0.08745419234037399,
      "learning_rate": 0.0001745967741935484,
      "loss": 1.5037,
      "step": 1555
    },
    {
      "epoch": 4.177181208053692,
      "grad_norm": 0.08153602480888367,
      "learning_rate": 0.00017451612903225804,
      "loss": 1.4026,
      "step": 1556
    },
    {
      "epoch": 4.17986577181208,
      "grad_norm": 0.08533434569835663,
      "learning_rate": 0.00017443548387096773,
      "loss": 1.5003,
      "step": 1557
    },
    {
      "epoch": 4.18255033557047,
      "grad_norm": 0.09428197145462036,
      "learning_rate": 0.0001743548387096774,
      "loss": 1.5219,
      "step": 1558
    },
    {
      "epoch": 4.185234899328859,
      "grad_norm": 0.10064629465341568,
      "learning_rate": 0.0001742741935483871,
      "loss": 1.3927,
      "step": 1559
    },
    {
      "epoch": 4.1879194630872485,
      "grad_norm": 0.0859447494149208,
      "learning_rate": 0.00017419354838709678,
      "loss": 1.4425,
      "step": 1560
    },
    {
      "epoch": 4.190604026845637,
      "grad_norm": 0.08755148947238922,
      "learning_rate": 0.00017411290322580643,
      "loss": 1.5445,
      "step": 1561
    },
    {
      "epoch": 4.193288590604027,
      "grad_norm": 0.08346788585186005,
      "learning_rate": 0.00017403225806451612,
      "loss": 1.5964,
      "step": 1562
    },
    {
      "epoch": 4.195973154362416,
      "grad_norm": 0.08291427791118622,
      "learning_rate": 0.0001739516129032258,
      "loss": 1.5831,
      "step": 1563
    },
    {
      "epoch": 4.198657718120805,
      "grad_norm": 0.08445248007774353,
      "learning_rate": 0.00017387096774193548,
      "loss": 1.5634,
      "step": 1564
    },
    {
      "epoch": 4.201342281879195,
      "grad_norm": 0.08411108702421188,
      "learning_rate": 0.00017379032258064517,
      "loss": 1.5043,
      "step": 1565
    },
    {
      "epoch": 4.204026845637584,
      "grad_norm": 0.08828777819871902,
      "learning_rate": 0.0001737096774193548,
      "loss": 1.408,
      "step": 1566
    },
    {
      "epoch": 4.206711409395973,
      "grad_norm": 0.09524684399366379,
      "learning_rate": 0.0001736290322580645,
      "loss": 1.6071,
      "step": 1567
    },
    {
      "epoch": 4.209395973154362,
      "grad_norm": 0.1121957078576088,
      "learning_rate": 0.0001735483870967742,
      "loss": 1.4603,
      "step": 1568
    },
    {
      "epoch": 4.212080536912752,
      "grad_norm": 0.09044550359249115,
      "learning_rate": 0.00017346774193548387,
      "loss": 1.4987,
      "step": 1569
    },
    {
      "epoch": 4.214765100671141,
      "grad_norm": 0.09141036868095398,
      "learning_rate": 0.00017338709677419356,
      "loss": 1.4443,
      "step": 1570
    },
    {
      "epoch": 4.21744966442953,
      "grad_norm": 0.077492855489254,
      "learning_rate": 0.00017330645161290319,
      "loss": 1.5377,
      "step": 1571
    },
    {
      "epoch": 4.220134228187919,
      "grad_norm": 0.08881278336048126,
      "learning_rate": 0.00017322580645161287,
      "loss": 1.4539,
      "step": 1572
    },
    {
      "epoch": 4.222818791946309,
      "grad_norm": 0.08086640387773514,
      "learning_rate": 0.00017314516129032255,
      "loss": 1.439,
      "step": 1573
    },
    {
      "epoch": 4.225503355704698,
      "grad_norm": 0.08965431898832321,
      "learning_rate": 0.00017306451612903226,
      "loss": 1.5534,
      "step": 1574
    },
    {
      "epoch": 4.228187919463087,
      "grad_norm": 0.08478548377752304,
      "learning_rate": 0.00017298387096774195,
      "loss": 1.4831,
      "step": 1575
    },
    {
      "epoch": 4.230872483221477,
      "grad_norm": 0.08814643323421478,
      "learning_rate": 0.00017290322580645158,
      "loss": 1.4267,
      "step": 1576
    },
    {
      "epoch": 4.233557046979866,
      "grad_norm": 0.10833539813756943,
      "learning_rate": 0.00017282258064516126,
      "loss": 1.521,
      "step": 1577
    },
    {
      "epoch": 4.236241610738255,
      "grad_norm": 0.09247971326112747,
      "learning_rate": 0.00017274193548387094,
      "loss": 1.4809,
      "step": 1578
    },
    {
      "epoch": 4.238926174496644,
      "grad_norm": 0.08608805388212204,
      "learning_rate": 0.00017266129032258063,
      "loss": 1.2971,
      "step": 1579
    },
    {
      "epoch": 4.241610738255034,
      "grad_norm": 0.09378187358379364,
      "learning_rate": 0.0001725806451612903,
      "loss": 1.5662,
      "step": 1580
    },
    {
      "epoch": 4.244295302013422,
      "grad_norm": 0.09747917205095291,
      "learning_rate": 0.00017249999999999996,
      "loss": 1.4277,
      "step": 1581
    },
    {
      "epoch": 4.246979865771812,
      "grad_norm": 0.09043912589550018,
      "learning_rate": 0.00017241935483870965,
      "loss": 1.3706,
      "step": 1582
    },
    {
      "epoch": 4.249664429530202,
      "grad_norm": 0.0870993360877037,
      "learning_rate": 0.00017233870967741933,
      "loss": 1.4868,
      "step": 1583
    },
    {
      "epoch": 4.2523489932885905,
      "grad_norm": 0.09692072123289108,
      "learning_rate": 0.00017225806451612901,
      "loss": 1.3669,
      "step": 1584
    },
    {
      "epoch": 4.25503355704698,
      "grad_norm": 0.08918743580579758,
      "learning_rate": 0.0001721774193548387,
      "loss": 1.5521,
      "step": 1585
    },
    {
      "epoch": 4.257718120805369,
      "grad_norm": 0.07956361770629883,
      "learning_rate": 0.00017209677419354835,
      "loss": 1.587,
      "step": 1586
    },
    {
      "epoch": 4.260402684563759,
      "grad_norm": 0.09507279098033905,
      "learning_rate": 0.00017201612903225804,
      "loss": 1.2661,
      "step": 1587
    },
    {
      "epoch": 4.263087248322147,
      "grad_norm": 0.0783337950706482,
      "learning_rate": 0.00017193548387096772,
      "loss": 1.3862,
      "step": 1588
    },
    {
      "epoch": 4.265771812080537,
      "grad_norm": 0.07895395904779434,
      "learning_rate": 0.0001718548387096774,
      "loss": 1.4447,
      "step": 1589
    },
    {
      "epoch": 4.268456375838926,
      "grad_norm": 0.09103555977344513,
      "learning_rate": 0.0001717741935483871,
      "loss": 1.5475,
      "step": 1590
    },
    {
      "epoch": 4.2711409395973154,
      "grad_norm": 0.0846828892827034,
      "learning_rate": 0.00017169354838709674,
      "loss": 1.5389,
      "step": 1591
    },
    {
      "epoch": 4.273825503355705,
      "grad_norm": 0.09562323242425919,
      "learning_rate": 0.00017161290322580643,
      "loss": 1.4812,
      "step": 1592
    },
    {
      "epoch": 4.276510067114094,
      "grad_norm": 0.09224989265203476,
      "learning_rate": 0.0001715322580645161,
      "loss": 1.5512,
      "step": 1593
    },
    {
      "epoch": 4.2791946308724835,
      "grad_norm": 0.09879390895366669,
      "learning_rate": 0.0001714516129032258,
      "loss": 1.5164,
      "step": 1594
    },
    {
      "epoch": 4.281879194630872,
      "grad_norm": 0.09226960688829422,
      "learning_rate": 0.00017137096774193548,
      "loss": 1.4941,
      "step": 1595
    },
    {
      "epoch": 4.284563758389262,
      "grad_norm": 0.08971453458070755,
      "learning_rate": 0.00017129032258064513,
      "loss": 1.4523,
      "step": 1596
    },
    {
      "epoch": 4.287248322147651,
      "grad_norm": 0.08356489986181259,
      "learning_rate": 0.00017120967741935482,
      "loss": 1.5249,
      "step": 1597
    },
    {
      "epoch": 4.28993288590604,
      "grad_norm": 0.08980464190244675,
      "learning_rate": 0.0001711290322580645,
      "loss": 1.4795,
      "step": 1598
    },
    {
      "epoch": 4.292617449664429,
      "grad_norm": 0.0868944376707077,
      "learning_rate": 0.00017104838709677418,
      "loss": 1.499,
      "step": 1599
    },
    {
      "epoch": 4.295302013422819,
      "grad_norm": 0.09354442358016968,
      "learning_rate": 0.00017096774193548387,
      "loss": 1.4908,
      "step": 1600
    },
    {
      "epoch": 4.2979865771812085,
      "grad_norm": 0.09207067638635635,
      "learning_rate": 0.00017088709677419352,
      "loss": 1.4178,
      "step": 1601
    },
    {
      "epoch": 4.300671140939597,
      "grad_norm": 0.09718969464302063,
      "learning_rate": 0.0001708064516129032,
      "loss": 1.3405,
      "step": 1602
    },
    {
      "epoch": 4.303355704697987,
      "grad_norm": 0.09420950710773468,
      "learning_rate": 0.0001707258064516129,
      "loss": 1.4394,
      "step": 1603
    },
    {
      "epoch": 4.306040268456376,
      "grad_norm": 0.07857875525951385,
      "learning_rate": 0.00017064516129032257,
      "loss": 1.4566,
      "step": 1604
    },
    {
      "epoch": 4.308724832214765,
      "grad_norm": 0.08988720923662186,
      "learning_rate": 0.00017056451612903226,
      "loss": 1.4947,
      "step": 1605
    },
    {
      "epoch": 4.311409395973154,
      "grad_norm": 0.09111515432596207,
      "learning_rate": 0.0001704838709677419,
      "loss": 1.4626,
      "step": 1606
    },
    {
      "epoch": 4.314093959731544,
      "grad_norm": 0.09884937107563019,
      "learning_rate": 0.0001704032258064516,
      "loss": 1.3378,
      "step": 1607
    },
    {
      "epoch": 4.3167785234899325,
      "grad_norm": 0.09028080850839615,
      "learning_rate": 0.00017032258064516128,
      "loss": 1.4074,
      "step": 1608
    },
    {
      "epoch": 4.319463087248322,
      "grad_norm": 0.08347216993570328,
      "learning_rate": 0.00017024193548387096,
      "loss": 1.4418,
      "step": 1609
    },
    {
      "epoch": 4.322147651006711,
      "grad_norm": 0.09860824793577194,
      "learning_rate": 0.00017016129032258065,
      "loss": 1.3676,
      "step": 1610
    },
    {
      "epoch": 4.324832214765101,
      "grad_norm": 0.08490824699401855,
      "learning_rate": 0.0001700806451612903,
      "loss": 1.4099,
      "step": 1611
    },
    {
      "epoch": 4.32751677852349,
      "grad_norm": 0.09191624820232391,
      "learning_rate": 0.00016999999999999999,
      "loss": 1.3931,
      "step": 1612
    },
    {
      "epoch": 4.330201342281879,
      "grad_norm": 0.09421646595001221,
      "learning_rate": 0.00016991935483870967,
      "loss": 1.4438,
      "step": 1613
    },
    {
      "epoch": 4.332885906040269,
      "grad_norm": 0.08939371258020401,
      "learning_rate": 0.00016983870967741935,
      "loss": 1.4267,
      "step": 1614
    },
    {
      "epoch": 4.3355704697986575,
      "grad_norm": 0.0864587053656578,
      "learning_rate": 0.00016975806451612904,
      "loss": 1.3964,
      "step": 1615
    },
    {
      "epoch": 4.338255033557047,
      "grad_norm": 0.08092769980430603,
      "learning_rate": 0.0001696774193548387,
      "loss": 1.5735,
      "step": 1616
    },
    {
      "epoch": 4.340939597315436,
      "grad_norm": 0.10093654692173004,
      "learning_rate": 0.00016959677419354837,
      "loss": 1.4723,
      "step": 1617
    },
    {
      "epoch": 4.3436241610738255,
      "grad_norm": 0.09521189332008362,
      "learning_rate": 0.00016951612903225806,
      "loss": 1.496,
      "step": 1618
    },
    {
      "epoch": 4.346308724832214,
      "grad_norm": 0.09498865902423859,
      "learning_rate": 0.00016943548387096774,
      "loss": 1.4178,
      "step": 1619
    },
    {
      "epoch": 4.348993288590604,
      "grad_norm": 0.0920422300696373,
      "learning_rate": 0.00016935483870967742,
      "loss": 1.5826,
      "step": 1620
    },
    {
      "epoch": 4.351677852348994,
      "grad_norm": 0.09386877715587616,
      "learning_rate": 0.00016927419354838708,
      "loss": 1.4894,
      "step": 1621
    },
    {
      "epoch": 4.354362416107382,
      "grad_norm": 0.09397751837968826,
      "learning_rate": 0.00016919354838709676,
      "loss": 1.5052,
      "step": 1622
    },
    {
      "epoch": 4.357046979865772,
      "grad_norm": 0.07992912083864212,
      "learning_rate": 0.00016911290322580645,
      "loss": 1.4722,
      "step": 1623
    },
    {
      "epoch": 4.359731543624161,
      "grad_norm": 0.08808733522891998,
      "learning_rate": 0.00016903225806451613,
      "loss": 1.2958,
      "step": 1624
    },
    {
      "epoch": 4.3624161073825505,
      "grad_norm": 0.09409112483263016,
      "learning_rate": 0.00016895161290322581,
      "loss": 1.4546,
      "step": 1625
    },
    {
      "epoch": 4.365100671140939,
      "grad_norm": 0.0808272585272789,
      "learning_rate": 0.00016887096774193547,
      "loss": 1.4236,
      "step": 1626
    },
    {
      "epoch": 4.367785234899329,
      "grad_norm": 0.0934864729642868,
      "learning_rate": 0.00016879032258064515,
      "loss": 1.4353,
      "step": 1627
    },
    {
      "epoch": 4.370469798657718,
      "grad_norm": 0.08521832525730133,
      "learning_rate": 0.00016870967741935484,
      "loss": 1.4764,
      "step": 1628
    },
    {
      "epoch": 4.373154362416107,
      "grad_norm": 0.0928611010313034,
      "learning_rate": 0.00016862903225806452,
      "loss": 1.3392,
      "step": 1629
    },
    {
      "epoch": 4.375838926174497,
      "grad_norm": 0.10006631910800934,
      "learning_rate": 0.00016854838709677415,
      "loss": 1.3994,
      "step": 1630
    },
    {
      "epoch": 4.378523489932886,
      "grad_norm": 0.08192753791809082,
      "learning_rate": 0.00016846774193548386,
      "loss": 1.5169,
      "step": 1631
    },
    {
      "epoch": 4.381208053691275,
      "grad_norm": 0.08957365900278091,
      "learning_rate": 0.00016838709677419354,
      "loss": 1.5246,
      "step": 1632
    },
    {
      "epoch": 4.383892617449664,
      "grad_norm": 0.08927281200885773,
      "learning_rate": 0.00016830645161290323,
      "loss": 1.4138,
      "step": 1633
    },
    {
      "epoch": 4.386577181208054,
      "grad_norm": 0.09699387848377228,
      "learning_rate": 0.0001682258064516129,
      "loss": 1.507,
      "step": 1634
    },
    {
      "epoch": 4.389261744966443,
      "grad_norm": 0.09977105259895325,
      "learning_rate": 0.00016814516129032254,
      "loss": 1.3901,
      "step": 1635
    },
    {
      "epoch": 4.391946308724832,
      "grad_norm": 0.08977212756872177,
      "learning_rate": 0.00016806451612903222,
      "loss": 1.5685,
      "step": 1636
    },
    {
      "epoch": 4.394630872483221,
      "grad_norm": 0.0883372351527214,
      "learning_rate": 0.0001679838709677419,
      "loss": 1.4932,
      "step": 1637
    },
    {
      "epoch": 4.397315436241611,
      "grad_norm": 0.10078459978103638,
      "learning_rate": 0.0001679032258064516,
      "loss": 1.5785,
      "step": 1638
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.08346571773290634,
      "learning_rate": 0.0001678225806451613,
      "loss": 1.4978,
      "step": 1639
    },
    {
      "epoch": 4.402684563758389,
      "grad_norm": 0.09341903030872345,
      "learning_rate": 0.00016774193548387093,
      "loss": 1.4077,
      "step": 1640
    },
    {
      "epoch": 4.405369127516779,
      "grad_norm": 0.08818146586418152,
      "learning_rate": 0.0001676612903225806,
      "loss": 1.564,
      "step": 1641
    },
    {
      "epoch": 4.4080536912751676,
      "grad_norm": 0.09295938163995743,
      "learning_rate": 0.0001675806451612903,
      "loss": 1.375,
      "step": 1642
    },
    {
      "epoch": 4.410738255033557,
      "grad_norm": 0.09704402089118958,
      "learning_rate": 0.00016749999999999998,
      "loss": 1.4417,
      "step": 1643
    },
    {
      "epoch": 4.413422818791946,
      "grad_norm": 0.08586273342370987,
      "learning_rate": 0.00016741935483870966,
      "loss": 1.5143,
      "step": 1644
    },
    {
      "epoch": 4.416107382550336,
      "grad_norm": 0.08105304092168808,
      "learning_rate": 0.00016733870967741932,
      "loss": 1.3595,
      "step": 1645
    },
    {
      "epoch": 4.418791946308724,
      "grad_norm": 0.09275428205728531,
      "learning_rate": 0.000167258064516129,
      "loss": 1.3822,
      "step": 1646
    },
    {
      "epoch": 4.421476510067114,
      "grad_norm": 0.08784166723489761,
      "learning_rate": 0.00016717741935483868,
      "loss": 1.5817,
      "step": 1647
    },
    {
      "epoch": 4.424161073825504,
      "grad_norm": 0.08434177935123444,
      "learning_rate": 0.00016709677419354837,
      "loss": 1.4482,
      "step": 1648
    },
    {
      "epoch": 4.4268456375838925,
      "grad_norm": 0.10090363025665283,
      "learning_rate": 0.00016701612903225805,
      "loss": 1.4241,
      "step": 1649
    },
    {
      "epoch": 4.429530201342282,
      "grad_norm": 0.07856607437133789,
      "learning_rate": 0.0001669354838709677,
      "loss": 1.5204,
      "step": 1650
    },
    {
      "epoch": 4.432214765100671,
      "grad_norm": 0.08962712436914444,
      "learning_rate": 0.0001668548387096774,
      "loss": 1.5226,
      "step": 1651
    },
    {
      "epoch": 4.434899328859061,
      "grad_norm": 0.09326930344104767,
      "learning_rate": 0.00016677419354838707,
      "loss": 1.4801,
      "step": 1652
    },
    {
      "epoch": 4.437583892617449,
      "grad_norm": 0.08635997027158737,
      "learning_rate": 0.00016669354838709676,
      "loss": 1.3994,
      "step": 1653
    },
    {
      "epoch": 4.440268456375839,
      "grad_norm": 0.08538396656513214,
      "learning_rate": 0.00016661290322580644,
      "loss": 1.4144,
      "step": 1654
    },
    {
      "epoch": 4.442953020134228,
      "grad_norm": 0.09881379455327988,
      "learning_rate": 0.0001665322580645161,
      "loss": 1.47,
      "step": 1655
    },
    {
      "epoch": 4.445637583892617,
      "grad_norm": 0.08666769415140152,
      "learning_rate": 0.00016645161290322578,
      "loss": 1.5157,
      "step": 1656
    },
    {
      "epoch": 4.448322147651007,
      "grad_norm": 0.09703580290079117,
      "learning_rate": 0.00016637096774193546,
      "loss": 1.415,
      "step": 1657
    },
    {
      "epoch": 4.451006711409396,
      "grad_norm": 0.09117064625024796,
      "learning_rate": 0.00016629032258064515,
      "loss": 1.5654,
      "step": 1658
    },
    {
      "epoch": 4.4536912751677855,
      "grad_norm": 0.08574184775352478,
      "learning_rate": 0.00016620967741935483,
      "loss": 1.625,
      "step": 1659
    },
    {
      "epoch": 4.456375838926174,
      "grad_norm": 0.08174504339694977,
      "learning_rate": 0.0001661290322580645,
      "loss": 1.4499,
      "step": 1660
    },
    {
      "epoch": 4.459060402684564,
      "grad_norm": 0.08930187672376633,
      "learning_rate": 0.00016604838709677417,
      "loss": 1.4807,
      "step": 1661
    },
    {
      "epoch": 4.461744966442953,
      "grad_norm": 0.09944450855255127,
      "learning_rate": 0.00016596774193548385,
      "loss": 1.4324,
      "step": 1662
    },
    {
      "epoch": 4.464429530201342,
      "grad_norm": 0.09589742124080658,
      "learning_rate": 0.00016588709677419354,
      "loss": 1.3847,
      "step": 1663
    },
    {
      "epoch": 4.467114093959731,
      "grad_norm": 0.09226372092962265,
      "learning_rate": 0.00016580645161290322,
      "loss": 1.4714,
      "step": 1664
    },
    {
      "epoch": 4.469798657718121,
      "grad_norm": 0.08328087627887726,
      "learning_rate": 0.00016572580645161288,
      "loss": 1.5665,
      "step": 1665
    },
    {
      "epoch": 4.4724832214765105,
      "grad_norm": 0.09939960390329361,
      "learning_rate": 0.00016564516129032256,
      "loss": 1.4658,
      "step": 1666
    },
    {
      "epoch": 4.475167785234899,
      "grad_norm": 0.09564100205898285,
      "learning_rate": 0.00016556451612903224,
      "loss": 1.5412,
      "step": 1667
    },
    {
      "epoch": 4.477852348993289,
      "grad_norm": 0.09306609630584717,
      "learning_rate": 0.00016548387096774193,
      "loss": 1.3495,
      "step": 1668
    },
    {
      "epoch": 4.480536912751678,
      "grad_norm": 0.09841583669185638,
      "learning_rate": 0.0001654032258064516,
      "loss": 1.4713,
      "step": 1669
    },
    {
      "epoch": 4.483221476510067,
      "grad_norm": 0.08190396428108215,
      "learning_rate": 0.00016532258064516127,
      "loss": 1.5552,
      "step": 1670
    },
    {
      "epoch": 4.485906040268456,
      "grad_norm": 0.11105728149414062,
      "learning_rate": 0.00016524193548387095,
      "loss": 1.4313,
      "step": 1671
    },
    {
      "epoch": 4.488590604026846,
      "grad_norm": 0.09042442589998245,
      "learning_rate": 0.00016516129032258063,
      "loss": 1.4771,
      "step": 1672
    },
    {
      "epoch": 4.4912751677852345,
      "grad_norm": 0.09492912888526917,
      "learning_rate": 0.00016508064516129032,
      "loss": 1.4905,
      "step": 1673
    },
    {
      "epoch": 4.493959731543624,
      "grad_norm": 0.08869117498397827,
      "learning_rate": 0.000165,
      "loss": 1.5976,
      "step": 1674
    },
    {
      "epoch": 4.496644295302014,
      "grad_norm": 0.10526762157678604,
      "learning_rate": 0.00016491935483870966,
      "loss": 1.4076,
      "step": 1675
    },
    {
      "epoch": 4.499328859060403,
      "grad_norm": 0.08896087855100632,
      "learning_rate": 0.00016483870967741934,
      "loss": 1.6378,
      "step": 1676
    },
    {
      "epoch": 4.502013422818792,
      "grad_norm": 0.08556176722049713,
      "learning_rate": 0.00016475806451612902,
      "loss": 1.4418,
      "step": 1677
    },
    {
      "epoch": 4.504697986577181,
      "grad_norm": 0.09086668491363525,
      "learning_rate": 0.0001646774193548387,
      "loss": 1.4532,
      "step": 1678
    },
    {
      "epoch": 4.507382550335571,
      "grad_norm": 0.08473929017782211,
      "learning_rate": 0.0001645967741935484,
      "loss": 1.507,
      "step": 1679
    },
    {
      "epoch": 4.510067114093959,
      "grad_norm": 0.09909448027610779,
      "learning_rate": 0.00016451612903225804,
      "loss": 1.4521,
      "step": 1680
    },
    {
      "epoch": 4.512751677852349,
      "grad_norm": 0.10555703938007355,
      "learning_rate": 0.00016443548387096773,
      "loss": 1.3098,
      "step": 1681
    },
    {
      "epoch": 4.515436241610738,
      "grad_norm": 0.08444730937480927,
      "learning_rate": 0.0001643548387096774,
      "loss": 1.4534,
      "step": 1682
    },
    {
      "epoch": 4.5181208053691275,
      "grad_norm": 0.09582316875457764,
      "learning_rate": 0.0001642741935483871,
      "loss": 1.5482,
      "step": 1683
    },
    {
      "epoch": 4.520805369127517,
      "grad_norm": 0.10140573978424072,
      "learning_rate": 0.00016419354838709678,
      "loss": 1.3956,
      "step": 1684
    },
    {
      "epoch": 4.523489932885906,
      "grad_norm": 0.08305499702692032,
      "learning_rate": 0.00016411290322580643,
      "loss": 1.5069,
      "step": 1685
    },
    {
      "epoch": 4.526174496644296,
      "grad_norm": 0.09961302578449249,
      "learning_rate": 0.00016403225806451612,
      "loss": 1.48,
      "step": 1686
    },
    {
      "epoch": 4.528859060402684,
      "grad_norm": 0.08673836290836334,
      "learning_rate": 0.0001639516129032258,
      "loss": 1.4767,
      "step": 1687
    },
    {
      "epoch": 4.531543624161074,
      "grad_norm": 0.0897315964102745,
      "learning_rate": 0.00016387096774193548,
      "loss": 1.3307,
      "step": 1688
    },
    {
      "epoch": 4.534228187919463,
      "grad_norm": 0.08307460695505142,
      "learning_rate": 0.00016379032258064517,
      "loss": 1.5088,
      "step": 1689
    },
    {
      "epoch": 4.5369127516778525,
      "grad_norm": 0.08586672693490982,
      "learning_rate": 0.00016370967741935482,
      "loss": 1.4499,
      "step": 1690
    },
    {
      "epoch": 4.539597315436241,
      "grad_norm": 0.09719084948301315,
      "learning_rate": 0.0001636290322580645,
      "loss": 1.323,
      "step": 1691
    },
    {
      "epoch": 4.542281879194631,
      "grad_norm": 0.10310852527618408,
      "learning_rate": 0.0001635483870967742,
      "loss": 1.4324,
      "step": 1692
    },
    {
      "epoch": 4.5449664429530205,
      "grad_norm": 0.08445235341787338,
      "learning_rate": 0.00016346774193548387,
      "loss": 1.3905,
      "step": 1693
    },
    {
      "epoch": 4.547651006711409,
      "grad_norm": 0.08806836605072021,
      "learning_rate": 0.00016338709677419356,
      "loss": 1.5123,
      "step": 1694
    },
    {
      "epoch": 4.550335570469799,
      "grad_norm": 0.08805688470602036,
      "learning_rate": 0.00016330645161290319,
      "loss": 1.5039,
      "step": 1695
    },
    {
      "epoch": 4.553020134228188,
      "grad_norm": 0.1143844723701477,
      "learning_rate": 0.0001632258064516129,
      "loss": 1.4715,
      "step": 1696
    },
    {
      "epoch": 4.555704697986577,
      "grad_norm": 0.08467786014080048,
      "learning_rate": 0.00016314516129032258,
      "loss": 1.4636,
      "step": 1697
    },
    {
      "epoch": 4.558389261744966,
      "grad_norm": 0.07635296881198883,
      "learning_rate": 0.00016306451612903226,
      "loss": 1.5289,
      "step": 1698
    },
    {
      "epoch": 4.561073825503356,
      "grad_norm": 0.09620046615600586,
      "learning_rate": 0.00016298387096774195,
      "loss": 1.4186,
      "step": 1699
    },
    {
      "epoch": 4.563758389261745,
      "grad_norm": 0.08962428569793701,
      "learning_rate": 0.00016290322580645158,
      "loss": 1.527,
      "step": 1700
    },
    {
      "epoch": 4.566442953020134,
      "grad_norm": 0.09655176103115082,
      "learning_rate": 0.00016282258064516126,
      "loss": 1.5255,
      "step": 1701
    },
    {
      "epoch": 4.569127516778524,
      "grad_norm": 0.07904262095689774,
      "learning_rate": 0.00016274193548387094,
      "loss": 1.4839,
      "step": 1702
    },
    {
      "epoch": 4.571812080536913,
      "grad_norm": 0.09074006974697113,
      "learning_rate": 0.00016266129032258065,
      "loss": 1.3945,
      "step": 1703
    },
    {
      "epoch": 4.574496644295302,
      "grad_norm": 0.08242641389369965,
      "learning_rate": 0.00016258064516129034,
      "loss": 1.4586,
      "step": 1704
    },
    {
      "epoch": 4.577181208053691,
      "grad_norm": 0.10102138668298721,
      "learning_rate": 0.00016249999999999997,
      "loss": 1.3715,
      "step": 1705
    },
    {
      "epoch": 4.579865771812081,
      "grad_norm": 0.08694885671138763,
      "learning_rate": 0.00016241935483870965,
      "loss": 1.4665,
      "step": 1706
    },
    {
      "epoch": 4.5825503355704695,
      "grad_norm": 0.08777405321598053,
      "learning_rate": 0.00016233870967741933,
      "loss": 1.5005,
      "step": 1707
    },
    {
      "epoch": 4.585234899328859,
      "grad_norm": 0.09735431522130966,
      "learning_rate": 0.00016225806451612902,
      "loss": 1.3669,
      "step": 1708
    },
    {
      "epoch": 4.587919463087248,
      "grad_norm": 0.08312689512968063,
      "learning_rate": 0.0001621774193548387,
      "loss": 1.4563,
      "step": 1709
    },
    {
      "epoch": 4.590604026845638,
      "grad_norm": 0.0882369875907898,
      "learning_rate": 0.00016209677419354835,
      "loss": 1.447,
      "step": 1710
    },
    {
      "epoch": 4.593288590604027,
      "grad_norm": 0.09625831991434097,
      "learning_rate": 0.00016201612903225804,
      "loss": 1.4024,
      "step": 1711
    },
    {
      "epoch": 4.595973154362416,
      "grad_norm": 0.08624164015054703,
      "learning_rate": 0.00016193548387096772,
      "loss": 1.5414,
      "step": 1712
    },
    {
      "epoch": 4.598657718120806,
      "grad_norm": 0.09712323546409607,
      "learning_rate": 0.0001618548387096774,
      "loss": 1.528,
      "step": 1713
    },
    {
      "epoch": 4.6013422818791945,
      "grad_norm": 0.09203555434942245,
      "learning_rate": 0.0001617741935483871,
      "loss": 1.5117,
      "step": 1714
    },
    {
      "epoch": 4.604026845637584,
      "grad_norm": 0.08890065550804138,
      "learning_rate": 0.00016169354838709674,
      "loss": 1.4951,
      "step": 1715
    },
    {
      "epoch": 4.606711409395973,
      "grad_norm": 0.0907084047794342,
      "learning_rate": 0.00016161290322580643,
      "loss": 1.3781,
      "step": 1716
    },
    {
      "epoch": 4.609395973154363,
      "grad_norm": 0.08606257289648056,
      "learning_rate": 0.0001615322580645161,
      "loss": 1.3734,
      "step": 1717
    },
    {
      "epoch": 4.612080536912751,
      "grad_norm": 0.09403300285339355,
      "learning_rate": 0.0001614516129032258,
      "loss": 1.5251,
      "step": 1718
    },
    {
      "epoch": 4.614765100671141,
      "grad_norm": 0.09094634652137756,
      "learning_rate": 0.00016137096774193548,
      "loss": 1.4756,
      "step": 1719
    },
    {
      "epoch": 4.617449664429531,
      "grad_norm": 0.09403177350759506,
      "learning_rate": 0.00016129032258064513,
      "loss": 1.357,
      "step": 1720
    },
    {
      "epoch": 4.620134228187919,
      "grad_norm": 0.0906657874584198,
      "learning_rate": 0.00016120967741935482,
      "loss": 1.4424,
      "step": 1721
    },
    {
      "epoch": 4.622818791946309,
      "grad_norm": 0.090057872235775,
      "learning_rate": 0.0001611290322580645,
      "loss": 1.6401,
      "step": 1722
    },
    {
      "epoch": 4.625503355704698,
      "grad_norm": 0.08935105800628662,
      "learning_rate": 0.00016104838709677418,
      "loss": 1.3848,
      "step": 1723
    },
    {
      "epoch": 4.6281879194630875,
      "grad_norm": 0.09275355935096741,
      "learning_rate": 0.00016096774193548384,
      "loss": 1.4982,
      "step": 1724
    },
    {
      "epoch": 4.630872483221476,
      "grad_norm": 0.09198068827390671,
      "learning_rate": 0.00016088709677419352,
      "loss": 1.4406,
      "step": 1725
    },
    {
      "epoch": 4.633557046979866,
      "grad_norm": 0.09875066578388214,
      "learning_rate": 0.0001608064516129032,
      "loss": 1.3124,
      "step": 1726
    },
    {
      "epoch": 4.636241610738255,
      "grad_norm": 0.08781816065311432,
      "learning_rate": 0.0001607258064516129,
      "loss": 1.473,
      "step": 1727
    },
    {
      "epoch": 4.638926174496644,
      "grad_norm": 0.09002451598644257,
      "learning_rate": 0.00016064516129032257,
      "loss": 1.4645,
      "step": 1728
    },
    {
      "epoch": 4.641610738255034,
      "grad_norm": 0.08651193231344223,
      "learning_rate": 0.00016056451612903223,
      "loss": 1.4181,
      "step": 1729
    },
    {
      "epoch": 4.644295302013423,
      "grad_norm": 0.10338947176933289,
      "learning_rate": 0.0001604838709677419,
      "loss": 1.4361,
      "step": 1730
    },
    {
      "epoch": 4.646979865771812,
      "grad_norm": 0.08889702707529068,
      "learning_rate": 0.0001604032258064516,
      "loss": 1.5177,
      "step": 1731
    },
    {
      "epoch": 4.649664429530201,
      "grad_norm": 0.0936727523803711,
      "learning_rate": 0.00016032258064516128,
      "loss": 1.5017,
      "step": 1732
    },
    {
      "epoch": 4.652348993288591,
      "grad_norm": 0.08735477179288864,
      "learning_rate": 0.00016024193548387096,
      "loss": 1.4753,
      "step": 1733
    },
    {
      "epoch": 4.65503355704698,
      "grad_norm": 0.08602925390005112,
      "learning_rate": 0.00016016129032258062,
      "loss": 1.5762,
      "step": 1734
    },
    {
      "epoch": 4.657718120805369,
      "grad_norm": 0.09025431424379349,
      "learning_rate": 0.0001600806451612903,
      "loss": 1.4384,
      "step": 1735
    },
    {
      "epoch": 4.660402684563758,
      "grad_norm": 0.08307591080665588,
      "learning_rate": 0.00015999999999999999,
      "loss": 1.5189,
      "step": 1736
    },
    {
      "epoch": 4.663087248322148,
      "grad_norm": 0.09068825095891953,
      "learning_rate": 0.00015991935483870967,
      "loss": 1.4041,
      "step": 1737
    },
    {
      "epoch": 4.665771812080537,
      "grad_norm": 0.08810869604349136,
      "learning_rate": 0.00015983870967741935,
      "loss": 1.4247,
      "step": 1738
    },
    {
      "epoch": 4.668456375838926,
      "grad_norm": 0.08504477143287659,
      "learning_rate": 0.000159758064516129,
      "loss": 1.5609,
      "step": 1739
    },
    {
      "epoch": 4.671140939597316,
      "grad_norm": 0.08133217692375183,
      "learning_rate": 0.0001596774193548387,
      "loss": 1.4423,
      "step": 1740
    },
    {
      "epoch": 4.673825503355705,
      "grad_norm": 0.0864759236574173,
      "learning_rate": 0.00015959677419354838,
      "loss": 1.4501,
      "step": 1741
    },
    {
      "epoch": 4.676510067114094,
      "grad_norm": 0.0730246752500534,
      "learning_rate": 0.00015951612903225806,
      "loss": 1.4353,
      "step": 1742
    },
    {
      "epoch": 4.679194630872483,
      "grad_norm": 0.08465975522994995,
      "learning_rate": 0.00015943548387096774,
      "loss": 1.5312,
      "step": 1743
    },
    {
      "epoch": 4.681879194630873,
      "grad_norm": 0.08784782886505127,
      "learning_rate": 0.0001593548387096774,
      "loss": 1.4532,
      "step": 1744
    },
    {
      "epoch": 4.684563758389261,
      "grad_norm": 0.09161145985126495,
      "learning_rate": 0.00015927419354838708,
      "loss": 1.3806,
      "step": 1745
    },
    {
      "epoch": 4.687248322147651,
      "grad_norm": 0.08907680213451385,
      "learning_rate": 0.00015919354838709677,
      "loss": 1.5316,
      "step": 1746
    },
    {
      "epoch": 4.689932885906041,
      "grad_norm": 0.08771484345197678,
      "learning_rate": 0.00015911290322580645,
      "loss": 1.4533,
      "step": 1747
    },
    {
      "epoch": 4.6926174496644295,
      "grad_norm": 0.09218467772006989,
      "learning_rate": 0.00015903225806451613,
      "loss": 1.4615,
      "step": 1748
    },
    {
      "epoch": 4.695302013422819,
      "grad_norm": 0.08003393560647964,
      "learning_rate": 0.0001589516129032258,
      "loss": 1.5223,
      "step": 1749
    },
    {
      "epoch": 4.697986577181208,
      "grad_norm": 0.09318331629037857,
      "learning_rate": 0.00015887096774193547,
      "loss": 1.4112,
      "step": 1750
    },
    {
      "epoch": 4.700671140939598,
      "grad_norm": 0.09858916699886322,
      "learning_rate": 0.00015879032258064515,
      "loss": 1.3797,
      "step": 1751
    },
    {
      "epoch": 4.703355704697986,
      "grad_norm": 0.08482155948877335,
      "learning_rate": 0.00015870967741935484,
      "loss": 1.4365,
      "step": 1752
    },
    {
      "epoch": 4.706040268456376,
      "grad_norm": 0.0973583236336708,
      "learning_rate": 0.00015862903225806452,
      "loss": 1.386,
      "step": 1753
    },
    {
      "epoch": 4.708724832214765,
      "grad_norm": 0.07985218614339828,
      "learning_rate": 0.00015854838709677418,
      "loss": 1.4833,
      "step": 1754
    },
    {
      "epoch": 4.7114093959731544,
      "grad_norm": 0.09107325971126556,
      "learning_rate": 0.00015846774193548386,
      "loss": 1.4241,
      "step": 1755
    },
    {
      "epoch": 4.714093959731544,
      "grad_norm": 0.08052359521389008,
      "learning_rate": 0.00015838709677419354,
      "loss": 1.4836,
      "step": 1756
    },
    {
      "epoch": 4.716778523489933,
      "grad_norm": 0.0959504023194313,
      "learning_rate": 0.00015830645161290323,
      "loss": 1.4073,
      "step": 1757
    },
    {
      "epoch": 4.7194630872483225,
      "grad_norm": 0.09785863012075424,
      "learning_rate": 0.0001582258064516129,
      "loss": 1.4973,
      "step": 1758
    },
    {
      "epoch": 4.722147651006711,
      "grad_norm": 0.08404918015003204,
      "learning_rate": 0.00015814516129032254,
      "loss": 1.5921,
      "step": 1759
    },
    {
      "epoch": 4.724832214765101,
      "grad_norm": 0.10990481823682785,
      "learning_rate": 0.00015806451612903225,
      "loss": 1.3229,
      "step": 1760
    },
    {
      "epoch": 4.72751677852349,
      "grad_norm": 0.08081167191267014,
      "learning_rate": 0.00015798387096774193,
      "loss": 1.4648,
      "step": 1761
    },
    {
      "epoch": 4.730201342281879,
      "grad_norm": 0.09043794870376587,
      "learning_rate": 0.00015790322580645162,
      "loss": 1.464,
      "step": 1762
    },
    {
      "epoch": 4.732885906040268,
      "grad_norm": 0.0874335989356041,
      "learning_rate": 0.0001578225806451613,
      "loss": 1.4895,
      "step": 1763
    },
    {
      "epoch": 4.735570469798658,
      "grad_norm": 0.09840399026870728,
      "learning_rate": 0.00015774193548387093,
      "loss": 1.4863,
      "step": 1764
    },
    {
      "epoch": 4.7382550335570475,
      "grad_norm": 0.08802910894155502,
      "learning_rate": 0.0001576612903225806,
      "loss": 1.5147,
      "step": 1765
    },
    {
      "epoch": 4.740939597315436,
      "grad_norm": 0.08789766579866409,
      "learning_rate": 0.0001575806451612903,
      "loss": 1.597,
      "step": 1766
    },
    {
      "epoch": 4.743624161073825,
      "grad_norm": 0.0867193341255188,
      "learning_rate": 0.00015749999999999998,
      "loss": 1.5111,
      "step": 1767
    },
    {
      "epoch": 4.746308724832215,
      "grad_norm": 0.0857621505856514,
      "learning_rate": 0.0001574193548387097,
      "loss": 1.4468,
      "step": 1768
    },
    {
      "epoch": 4.748993288590604,
      "grad_norm": 0.08732380717992783,
      "learning_rate": 0.00015733870967741932,
      "loss": 1.5453,
      "step": 1769
    },
    {
      "epoch": 4.751677852348993,
      "grad_norm": 0.08212802559137344,
      "learning_rate": 0.000157258064516129,
      "loss": 1.3276,
      "step": 1770
    },
    {
      "epoch": 4.754362416107383,
      "grad_norm": 0.0924188569188118,
      "learning_rate": 0.00015717741935483869,
      "loss": 1.5434,
      "step": 1771
    },
    {
      "epoch": 4.7570469798657715,
      "grad_norm": 0.08528252691030502,
      "learning_rate": 0.00015709677419354837,
      "loss": 1.4985,
      "step": 1772
    },
    {
      "epoch": 4.759731543624161,
      "grad_norm": 0.09934823960065842,
      "learning_rate": 0.00015701612903225805,
      "loss": 1.4672,
      "step": 1773
    },
    {
      "epoch": 4.762416107382551,
      "grad_norm": 0.09185268729925156,
      "learning_rate": 0.0001569354838709677,
      "loss": 1.5419,
      "step": 1774
    },
    {
      "epoch": 4.76510067114094,
      "grad_norm": 0.09113045781850815,
      "learning_rate": 0.0001568548387096774,
      "loss": 1.3926,
      "step": 1775
    },
    {
      "epoch": 4.767785234899328,
      "grad_norm": 0.08925765752792358,
      "learning_rate": 0.00015677419354838708,
      "loss": 1.5131,
      "step": 1776
    },
    {
      "epoch": 4.770469798657718,
      "grad_norm": 0.11106345057487488,
      "learning_rate": 0.00015669354838709676,
      "loss": 1.3529,
      "step": 1777
    },
    {
      "epoch": 4.773154362416108,
      "grad_norm": 0.08505565673112869,
      "learning_rate": 0.00015661290322580644,
      "loss": 1.488,
      "step": 1778
    },
    {
      "epoch": 4.7758389261744965,
      "grad_norm": 0.0883050337433815,
      "learning_rate": 0.0001565322580645161,
      "loss": 1.4047,
      "step": 1779
    },
    {
      "epoch": 4.778523489932886,
      "grad_norm": 0.09289876371622086,
      "learning_rate": 0.00015645161290322578,
      "loss": 1.3394,
      "step": 1780
    },
    {
      "epoch": 4.781208053691275,
      "grad_norm": 0.084518201649189,
      "learning_rate": 0.00015637096774193546,
      "loss": 1.4672,
      "step": 1781
    },
    {
      "epoch": 4.7838926174496645,
      "grad_norm": 0.09174949675798416,
      "learning_rate": 0.00015629032258064515,
      "loss": 1.4554,
      "step": 1782
    },
    {
      "epoch": 4.786577181208053,
      "grad_norm": 0.08474229276180267,
      "learning_rate": 0.00015620967741935483,
      "loss": 1.349,
      "step": 1783
    },
    {
      "epoch": 4.789261744966443,
      "grad_norm": 0.08981122821569443,
      "learning_rate": 0.0001561290322580645,
      "loss": 1.4499,
      "step": 1784
    },
    {
      "epoch": 4.791946308724832,
      "grad_norm": 0.08270243555307388,
      "learning_rate": 0.00015604838709677417,
      "loss": 1.3747,
      "step": 1785
    },
    {
      "epoch": 4.794630872483221,
      "grad_norm": 0.09813166409730911,
      "learning_rate": 0.00015596774193548385,
      "loss": 1.5082,
      "step": 1786
    },
    {
      "epoch": 4.797315436241611,
      "grad_norm": 0.08638586103916168,
      "learning_rate": 0.00015588709677419354,
      "loss": 1.5719,
      "step": 1787
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.0951232984662056,
      "learning_rate": 0.00015580645161290322,
      "loss": 1.437,
      "step": 1788
    },
    {
      "epoch": 4.8026845637583895,
      "grad_norm": 0.09400947391986847,
      "learning_rate": 0.00015572580645161288,
      "loss": 1.4472,
      "step": 1789
    },
    {
      "epoch": 4.805369127516778,
      "grad_norm": 0.08377483487129211,
      "learning_rate": 0.00015564516129032256,
      "loss": 1.5734,
      "step": 1790
    },
    {
      "epoch": 4.808053691275168,
      "grad_norm": 0.0788736566901207,
      "learning_rate": 0.00015556451612903224,
      "loss": 1.4193,
      "step": 1791
    },
    {
      "epoch": 4.810738255033557,
      "grad_norm": 0.09040632843971252,
      "learning_rate": 0.00015548387096774193,
      "loss": 1.5397,
      "step": 1792
    },
    {
      "epoch": 4.813422818791946,
      "grad_norm": 0.08310158550739288,
      "learning_rate": 0.0001554032258064516,
      "loss": 1.3927,
      "step": 1793
    },
    {
      "epoch": 4.816107382550335,
      "grad_norm": 0.08520032465457916,
      "learning_rate": 0.00015532258064516127,
      "loss": 1.5084,
      "step": 1794
    },
    {
      "epoch": 4.818791946308725,
      "grad_norm": 0.08106093853712082,
      "learning_rate": 0.00015524193548387095,
      "loss": 1.4817,
      "step": 1795
    },
    {
      "epoch": 4.821476510067114,
      "grad_norm": 0.09504979848861694,
      "learning_rate": 0.00015516129032258063,
      "loss": 1.4495,
      "step": 1796
    },
    {
      "epoch": 4.824161073825503,
      "grad_norm": 0.0831766426563263,
      "learning_rate": 0.00015508064516129032,
      "loss": 1.5448,
      "step": 1797
    },
    {
      "epoch": 4.826845637583893,
      "grad_norm": 0.09624287486076355,
      "learning_rate": 0.000155,
      "loss": 1.4581,
      "step": 1798
    },
    {
      "epoch": 4.829530201342282,
      "grad_norm": 0.09303948283195496,
      "learning_rate": 0.00015491935483870966,
      "loss": 1.3768,
      "step": 1799
    },
    {
      "epoch": 4.832214765100671,
      "grad_norm": 0.09670521318912506,
      "learning_rate": 0.00015483870967741934,
      "loss": 1.475,
      "step": 1800
    },
    {
      "epoch": 4.83489932885906,
      "grad_norm": 0.08613911271095276,
      "learning_rate": 0.00015475806451612902,
      "loss": 1.5275,
      "step": 1801
    },
    {
      "epoch": 4.83758389261745,
      "grad_norm": 0.10123389959335327,
      "learning_rate": 0.0001546774193548387,
      "loss": 1.5691,
      "step": 1802
    },
    {
      "epoch": 4.8402684563758385,
      "grad_norm": 0.08470548689365387,
      "learning_rate": 0.0001545967741935484,
      "loss": 1.568,
      "step": 1803
    },
    {
      "epoch": 4.842953020134228,
      "grad_norm": 0.10398636758327484,
      "learning_rate": 0.00015451612903225805,
      "loss": 1.3621,
      "step": 1804
    },
    {
      "epoch": 4.845637583892618,
      "grad_norm": 0.09519270062446594,
      "learning_rate": 0.00015443548387096773,
      "loss": 1.3373,
      "step": 1805
    },
    {
      "epoch": 4.8483221476510066,
      "grad_norm": 0.08268669247627258,
      "learning_rate": 0.0001543548387096774,
      "loss": 1.506,
      "step": 1806
    },
    {
      "epoch": 4.851006711409396,
      "grad_norm": 0.09157054126262665,
      "learning_rate": 0.0001542741935483871,
      "loss": 1.5392,
      "step": 1807
    },
    {
      "epoch": 4.853691275167785,
      "grad_norm": 0.09722758829593658,
      "learning_rate": 0.00015419354838709678,
      "loss": 1.3678,
      "step": 1808
    },
    {
      "epoch": 4.856375838926175,
      "grad_norm": 0.08984353393316269,
      "learning_rate": 0.00015411290322580644,
      "loss": 1.5025,
      "step": 1809
    },
    {
      "epoch": 4.859060402684563,
      "grad_norm": 0.09427297115325928,
      "learning_rate": 0.00015403225806451612,
      "loss": 1.3607,
      "step": 1810
    },
    {
      "epoch": 4.861744966442953,
      "grad_norm": 0.09870471805334091,
      "learning_rate": 0.0001539516129032258,
      "loss": 1.4471,
      "step": 1811
    },
    {
      "epoch": 4.864429530201342,
      "grad_norm": 0.08907514065504074,
      "learning_rate": 0.00015387096774193549,
      "loss": 1.5667,
      "step": 1812
    },
    {
      "epoch": 4.8671140939597315,
      "grad_norm": 0.08603448420763016,
      "learning_rate": 0.00015379032258064517,
      "loss": 1.5774,
      "step": 1813
    },
    {
      "epoch": 4.869798657718121,
      "grad_norm": 0.08783560991287231,
      "learning_rate": 0.00015370967741935482,
      "loss": 1.4753,
      "step": 1814
    },
    {
      "epoch": 4.87248322147651,
      "grad_norm": 0.08423832058906555,
      "learning_rate": 0.0001536290322580645,
      "loss": 1.562,
      "step": 1815
    },
    {
      "epoch": 4.8751677852349,
      "grad_norm": 0.09076584875583649,
      "learning_rate": 0.0001535483870967742,
      "loss": 1.5195,
      "step": 1816
    },
    {
      "epoch": 4.877852348993288,
      "grad_norm": 0.08895472437143326,
      "learning_rate": 0.00015346774193548387,
      "loss": 1.5716,
      "step": 1817
    },
    {
      "epoch": 4.880536912751678,
      "grad_norm": 0.09373622387647629,
      "learning_rate": 0.00015338709677419353,
      "loss": 1.454,
      "step": 1818
    },
    {
      "epoch": 4.883221476510067,
      "grad_norm": 0.0956500768661499,
      "learning_rate": 0.00015330645161290321,
      "loss": 1.4938,
      "step": 1819
    },
    {
      "epoch": 4.885906040268456,
      "grad_norm": 0.09972838312387466,
      "learning_rate": 0.0001532258064516129,
      "loss": 1.3643,
      "step": 1820
    },
    {
      "epoch": 4.888590604026845,
      "grad_norm": 0.08670388907194138,
      "learning_rate": 0.00015314516129032258,
      "loss": 1.4778,
      "step": 1821
    },
    {
      "epoch": 4.891275167785235,
      "grad_norm": 0.08489973098039627,
      "learning_rate": 0.00015306451612903226,
      "loss": 1.6277,
      "step": 1822
    },
    {
      "epoch": 4.8939597315436245,
      "grad_norm": 0.07858224213123322,
      "learning_rate": 0.0001529838709677419,
      "loss": 1.5781,
      "step": 1823
    },
    {
      "epoch": 4.896644295302013,
      "grad_norm": 0.09092467278242111,
      "learning_rate": 0.00015290322580645158,
      "loss": 1.5266,
      "step": 1824
    },
    {
      "epoch": 4.899328859060403,
      "grad_norm": 0.08849858492612839,
      "learning_rate": 0.0001528225806451613,
      "loss": 1.4667,
      "step": 1825
    },
    {
      "epoch": 4.902013422818792,
      "grad_norm": 0.09761252254247665,
      "learning_rate": 0.00015274193548387097,
      "loss": 1.4712,
      "step": 1826
    },
    {
      "epoch": 4.904697986577181,
      "grad_norm": 0.08826103806495667,
      "learning_rate": 0.00015266129032258065,
      "loss": 1.4839,
      "step": 1827
    },
    {
      "epoch": 4.90738255033557,
      "grad_norm": 0.09305662661790848,
      "learning_rate": 0.00015258064516129028,
      "loss": 1.4119,
      "step": 1828
    },
    {
      "epoch": 4.91006711409396,
      "grad_norm": 0.09109767526388168,
      "learning_rate": 0.00015249999999999997,
      "loss": 1.5862,
      "step": 1829
    },
    {
      "epoch": 4.912751677852349,
      "grad_norm": 0.08058576285839081,
      "learning_rate": 0.00015241935483870965,
      "loss": 1.4361,
      "step": 1830
    },
    {
      "epoch": 4.915436241610738,
      "grad_norm": 0.09336627274751663,
      "learning_rate": 0.00015233870967741933,
      "loss": 1.5012,
      "step": 1831
    },
    {
      "epoch": 4.918120805369128,
      "grad_norm": 0.08849839866161346,
      "learning_rate": 0.00015225806451612904,
      "loss": 1.5544,
      "step": 1832
    },
    {
      "epoch": 4.920805369127517,
      "grad_norm": 0.09511701762676239,
      "learning_rate": 0.00015217741935483867,
      "loss": 1.4684,
      "step": 1833
    },
    {
      "epoch": 4.923489932885906,
      "grad_norm": 0.08641494810581207,
      "learning_rate": 0.00015209677419354836,
      "loss": 1.6222,
      "step": 1834
    },
    {
      "epoch": 4.926174496644295,
      "grad_norm": 0.0901893600821495,
      "learning_rate": 0.00015201612903225804,
      "loss": 1.4204,
      "step": 1835
    },
    {
      "epoch": 4.928859060402685,
      "grad_norm": 0.08953744173049927,
      "learning_rate": 0.00015193548387096772,
      "loss": 1.4965,
      "step": 1836
    },
    {
      "epoch": 4.9315436241610735,
      "grad_norm": 0.08561777323484421,
      "learning_rate": 0.0001518548387096774,
      "loss": 1.522,
      "step": 1837
    },
    {
      "epoch": 4.934228187919463,
      "grad_norm": 0.10117343813180923,
      "learning_rate": 0.00015177419354838706,
      "loss": 1.3294,
      "step": 1838
    },
    {
      "epoch": 4.936912751677852,
      "grad_norm": 0.08191994577646255,
      "learning_rate": 0.00015169354838709675,
      "loss": 1.6224,
      "step": 1839
    },
    {
      "epoch": 4.939597315436242,
      "grad_norm": 0.08117905259132385,
      "learning_rate": 0.00015161290322580643,
      "loss": 1.4945,
      "step": 1840
    },
    {
      "epoch": 4.942281879194631,
      "grad_norm": 0.08855967968702316,
      "learning_rate": 0.0001515322580645161,
      "loss": 1.5833,
      "step": 1841
    },
    {
      "epoch": 4.94496644295302,
      "grad_norm": 0.08549754321575165,
      "learning_rate": 0.0001514516129032258,
      "loss": 1.4673,
      "step": 1842
    },
    {
      "epoch": 4.94765100671141,
      "grad_norm": 0.08728085458278656,
      "learning_rate": 0.00015137096774193545,
      "loss": 1.5291,
      "step": 1843
    },
    {
      "epoch": 4.950335570469798,
      "grad_norm": 0.09371617436408997,
      "learning_rate": 0.00015129032258064513,
      "loss": 1.4099,
      "step": 1844
    },
    {
      "epoch": 4.953020134228188,
      "grad_norm": 0.08537914603948593,
      "learning_rate": 0.00015120967741935482,
      "loss": 1.4957,
      "step": 1845
    },
    {
      "epoch": 4.955704697986577,
      "grad_norm": 0.08554099500179291,
      "learning_rate": 0.0001511290322580645,
      "loss": 1.5905,
      "step": 1846
    },
    {
      "epoch": 4.9583892617449665,
      "grad_norm": 0.09097113460302353,
      "learning_rate": 0.00015104838709677418,
      "loss": 1.3373,
      "step": 1847
    },
    {
      "epoch": 4.961073825503355,
      "grad_norm": 0.10059747099876404,
      "learning_rate": 0.00015096774193548384,
      "loss": 1.3725,
      "step": 1848
    },
    {
      "epoch": 4.963758389261745,
      "grad_norm": 0.09197837114334106,
      "learning_rate": 0.00015088709677419352,
      "loss": 1.3983,
      "step": 1849
    },
    {
      "epoch": 4.966442953020135,
      "grad_norm": 0.08948327600955963,
      "learning_rate": 0.0001508064516129032,
      "loss": 1.3977,
      "step": 1850
    },
    {
      "epoch": 4.969127516778523,
      "grad_norm": 0.08728551119565964,
      "learning_rate": 0.0001507258064516129,
      "loss": 1.4336,
      "step": 1851
    },
    {
      "epoch": 4.971812080536913,
      "grad_norm": 0.09000427275896072,
      "learning_rate": 0.00015064516129032257,
      "loss": 1.6207,
      "step": 1852
    },
    {
      "epoch": 4.974496644295302,
      "grad_norm": 0.07866636663675308,
      "learning_rate": 0.00015056451612903223,
      "loss": 1.4969,
      "step": 1853
    },
    {
      "epoch": 4.9771812080536915,
      "grad_norm": 0.08975151926279068,
      "learning_rate": 0.00015048387096774191,
      "loss": 1.4519,
      "step": 1854
    },
    {
      "epoch": 4.97986577181208,
      "grad_norm": 0.09047628194093704,
      "learning_rate": 0.0001504032258064516,
      "loss": 1.4721,
      "step": 1855
    },
    {
      "epoch": 4.98255033557047,
      "grad_norm": 0.08927107602357864,
      "learning_rate": 0.00015032258064516128,
      "loss": 1.6055,
      "step": 1856
    },
    {
      "epoch": 4.985234899328859,
      "grad_norm": 0.09854255616664886,
      "learning_rate": 0.00015024193548387096,
      "loss": 1.4805,
      "step": 1857
    },
    {
      "epoch": 4.987919463087248,
      "grad_norm": 0.0855712965130806,
      "learning_rate": 0.00015016129032258062,
      "loss": 1.4755,
      "step": 1858
    },
    {
      "epoch": 4.990604026845638,
      "grad_norm": 0.07740779966115952,
      "learning_rate": 0.0001500806451612903,
      "loss": 1.5015,
      "step": 1859
    },
    {
      "epoch": 4.993288590604027,
      "grad_norm": 0.10154948383569717,
      "learning_rate": 0.00015,
      "loss": 1.3263,
      "step": 1860
    },
    {
      "epoch": 4.995973154362416,
      "grad_norm": 0.09653989970684052,
      "learning_rate": 0.00014991935483870967,
      "loss": 1.4894,
      "step": 1861
    },
    {
      "epoch": 4.998657718120805,
      "grad_norm": 0.10069016367197037,
      "learning_rate": 0.00014983870967741933,
      "loss": 1.324,
      "step": 1862
    },
    {
      "epoch": 5.001342281879195,
      "grad_norm": 0.08574189245700836,
      "learning_rate": 0.000149758064516129,
      "loss": 1.4319,
      "step": 1863
    },
    {
      "epoch": 5.004026845637584,
      "grad_norm": 0.08862929046154022,
      "learning_rate": 0.0001496774193548387,
      "loss": 1.4111,
      "step": 1864
    },
    {
      "epoch": 5.006711409395973,
      "grad_norm": 0.07693370431661606,
      "learning_rate": 0.00014959677419354838,
      "loss": 1.4008,
      "step": 1865
    },
    {
      "epoch": 5.009395973154362,
      "grad_norm": 0.08747280389070511,
      "learning_rate": 0.00014951612903225806,
      "loss": 1.4104,
      "step": 1866
    },
    {
      "epoch": 5.012080536912752,
      "grad_norm": 0.11258958280086517,
      "learning_rate": 0.00014943548387096772,
      "loss": 1.4984,
      "step": 1867
    },
    {
      "epoch": 5.014765100671141,
      "grad_norm": 0.11498003453016281,
      "learning_rate": 0.0001493548387096774,
      "loss": 1.5075,
      "step": 1868
    },
    {
      "epoch": 5.01744966442953,
      "grad_norm": 0.0956629067659378,
      "learning_rate": 0.00014927419354838708,
      "loss": 1.4273,
      "step": 1869
    },
    {
      "epoch": 5.02013422818792,
      "grad_norm": 0.09335719048976898,
      "learning_rate": 0.00014919354838709677,
      "loss": 1.4884,
      "step": 1870
    },
    {
      "epoch": 5.0228187919463085,
      "grad_norm": 0.0892653614282608,
      "learning_rate": 0.00014911290322580645,
      "loss": 1.4017,
      "step": 1871
    },
    {
      "epoch": 5.025503355704698,
      "grad_norm": 0.1016111746430397,
      "learning_rate": 0.0001490322580645161,
      "loss": 1.4565,
      "step": 1872
    },
    {
      "epoch": 5.028187919463087,
      "grad_norm": 0.08930910378694534,
      "learning_rate": 0.0001489516129032258,
      "loss": 1.3749,
      "step": 1873
    },
    {
      "epoch": 5.030872483221477,
      "grad_norm": 0.08688151836395264,
      "learning_rate": 0.00014887096774193547,
      "loss": 1.5374,
      "step": 1874
    },
    {
      "epoch": 5.033557046979865,
      "grad_norm": 0.09406140446662903,
      "learning_rate": 0.00014879032258064516,
      "loss": 1.4454,
      "step": 1875
    },
    {
      "epoch": 5.036241610738255,
      "grad_norm": 0.08787450194358826,
      "learning_rate": 0.00014870967741935484,
      "loss": 1.6162,
      "step": 1876
    },
    {
      "epoch": 5.038926174496645,
      "grad_norm": 0.08633635193109512,
      "learning_rate": 0.0001486290322580645,
      "loss": 1.4768,
      "step": 1877
    },
    {
      "epoch": 5.0416107382550335,
      "grad_norm": 0.0853961929678917,
      "learning_rate": 0.00014854838709677418,
      "loss": 1.5323,
      "step": 1878
    },
    {
      "epoch": 5.044295302013423,
      "grad_norm": 0.09204015880823135,
      "learning_rate": 0.00014846774193548386,
      "loss": 1.3218,
      "step": 1879
    },
    {
      "epoch": 5.046979865771812,
      "grad_norm": 0.09047313779592514,
      "learning_rate": 0.00014838709677419355,
      "loss": 1.3801,
      "step": 1880
    },
    {
      "epoch": 5.049664429530202,
      "grad_norm": 0.08873551338911057,
      "learning_rate": 0.00014830645161290323,
      "loss": 1.4589,
      "step": 1881
    },
    {
      "epoch": 5.05234899328859,
      "grad_norm": 0.09771423786878586,
      "learning_rate": 0.00014822580645161288,
      "loss": 1.4128,
      "step": 1882
    },
    {
      "epoch": 5.05503355704698,
      "grad_norm": 0.08372130990028381,
      "learning_rate": 0.00014814516129032257,
      "loss": 1.5988,
      "step": 1883
    },
    {
      "epoch": 5.057718120805369,
      "grad_norm": 0.0919012501835823,
      "learning_rate": 0.00014806451612903225,
      "loss": 1.4729,
      "step": 1884
    },
    {
      "epoch": 5.060402684563758,
      "grad_norm": 0.08135770261287689,
      "learning_rate": 0.00014798387096774193,
      "loss": 1.422,
      "step": 1885
    },
    {
      "epoch": 5.063087248322148,
      "grad_norm": 0.08712052553892136,
      "learning_rate": 0.0001479032258064516,
      "loss": 1.3748,
      "step": 1886
    },
    {
      "epoch": 5.065771812080537,
      "grad_norm": 0.09113702178001404,
      "learning_rate": 0.00014782258064516127,
      "loss": 1.4422,
      "step": 1887
    },
    {
      "epoch": 5.0684563758389265,
      "grad_norm": 0.09863962233066559,
      "learning_rate": 0.00014774193548387096,
      "loss": 1.3827,
      "step": 1888
    },
    {
      "epoch": 5.071140939597315,
      "grad_norm": 0.0807267278432846,
      "learning_rate": 0.00014766129032258064,
      "loss": 1.5987,
      "step": 1889
    },
    {
      "epoch": 5.073825503355705,
      "grad_norm": 0.08770907670259476,
      "learning_rate": 0.00014758064516129032,
      "loss": 1.4547,
      "step": 1890
    },
    {
      "epoch": 5.076510067114094,
      "grad_norm": 0.09959517419338226,
      "learning_rate": 0.00014749999999999998,
      "loss": 1.5305,
      "step": 1891
    },
    {
      "epoch": 5.079194630872483,
      "grad_norm": 0.08883354812860489,
      "learning_rate": 0.00014741935483870966,
      "loss": 1.4982,
      "step": 1892
    },
    {
      "epoch": 5.081879194630872,
      "grad_norm": 0.08540993928909302,
      "learning_rate": 0.00014733870967741935,
      "loss": 1.4648,
      "step": 1893
    },
    {
      "epoch": 5.084563758389262,
      "grad_norm": 0.09865075349807739,
      "learning_rate": 0.000147258064516129,
      "loss": 1.3914,
      "step": 1894
    },
    {
      "epoch": 5.087248322147651,
      "grad_norm": 0.08145461976528168,
      "learning_rate": 0.0001471774193548387,
      "loss": 1.4289,
      "step": 1895
    },
    {
      "epoch": 5.08993288590604,
      "grad_norm": 0.08299855887889862,
      "learning_rate": 0.00014709677419354837,
      "loss": 1.452,
      "step": 1896
    },
    {
      "epoch": 5.09261744966443,
      "grad_norm": 0.09967224299907684,
      "learning_rate": 0.00014701612903225805,
      "loss": 1.4131,
      "step": 1897
    },
    {
      "epoch": 5.095302013422819,
      "grad_norm": 0.08411247283220291,
      "learning_rate": 0.00014693548387096774,
      "loss": 1.4575,
      "step": 1898
    },
    {
      "epoch": 5.097986577181208,
      "grad_norm": 0.08392524719238281,
      "learning_rate": 0.0001468548387096774,
      "loss": 1.4427,
      "step": 1899
    },
    {
      "epoch": 5.100671140939597,
      "grad_norm": 0.09806041419506073,
      "learning_rate": 0.00014677419354838708,
      "loss": 1.451,
      "step": 1900
    },
    {
      "epoch": 5.103355704697987,
      "grad_norm": 0.09808596223592758,
      "learning_rate": 0.00014669354838709676,
      "loss": 1.3189,
      "step": 1901
    },
    {
      "epoch": 5.1060402684563755,
      "grad_norm": 0.09145372360944748,
      "learning_rate": 0.00014661290322580644,
      "loss": 1.4645,
      "step": 1902
    },
    {
      "epoch": 5.108724832214765,
      "grad_norm": 0.08336159586906433,
      "learning_rate": 0.00014653225806451613,
      "loss": 1.489,
      "step": 1903
    },
    {
      "epoch": 5.111409395973155,
      "grad_norm": 0.09839463979005814,
      "learning_rate": 0.00014645161290322578,
      "loss": 1.3435,
      "step": 1904
    },
    {
      "epoch": 5.114093959731544,
      "grad_norm": 0.09776496887207031,
      "learning_rate": 0.00014637096774193547,
      "loss": 1.418,
      "step": 1905
    },
    {
      "epoch": 5.116778523489933,
      "grad_norm": 0.08646871149539948,
      "learning_rate": 0.00014629032258064515,
      "loss": 1.451,
      "step": 1906
    },
    {
      "epoch": 5.119463087248322,
      "grad_norm": 0.0831022560596466,
      "learning_rate": 0.00014620967741935483,
      "loss": 1.5107,
      "step": 1907
    },
    {
      "epoch": 5.122147651006712,
      "grad_norm": 0.08505295217037201,
      "learning_rate": 0.00014612903225806452,
      "loss": 1.474,
      "step": 1908
    },
    {
      "epoch": 5.1248322147651,
      "grad_norm": 0.08274142444133759,
      "learning_rate": 0.00014604838709677417,
      "loss": 1.3782,
      "step": 1909
    },
    {
      "epoch": 5.12751677852349,
      "grad_norm": 0.0961737111210823,
      "learning_rate": 0.00014596774193548386,
      "loss": 1.4624,
      "step": 1910
    },
    {
      "epoch": 5.130201342281879,
      "grad_norm": 0.09765126556158066,
      "learning_rate": 0.00014588709677419354,
      "loss": 1.487,
      "step": 1911
    },
    {
      "epoch": 5.1328859060402685,
      "grad_norm": 0.0921131819486618,
      "learning_rate": 0.00014580645161290322,
      "loss": 1.5114,
      "step": 1912
    },
    {
      "epoch": 5.135570469798658,
      "grad_norm": 0.0889531672000885,
      "learning_rate": 0.0001457258064516129,
      "loss": 1.5793,
      "step": 1913
    },
    {
      "epoch": 5.138255033557047,
      "grad_norm": 0.10152935981750488,
      "learning_rate": 0.00014564516129032256,
      "loss": 1.5069,
      "step": 1914
    },
    {
      "epoch": 5.140939597315437,
      "grad_norm": 0.0825960636138916,
      "learning_rate": 0.00014556451612903224,
      "loss": 1.4037,
      "step": 1915
    },
    {
      "epoch": 5.143624161073825,
      "grad_norm": 0.08827337622642517,
      "learning_rate": 0.00014548387096774193,
      "loss": 1.5965,
      "step": 1916
    },
    {
      "epoch": 5.146308724832215,
      "grad_norm": 0.0837823748588562,
      "learning_rate": 0.0001454032258064516,
      "loss": 1.4486,
      "step": 1917
    },
    {
      "epoch": 5.148993288590604,
      "grad_norm": 0.08863512426614761,
      "learning_rate": 0.0001453225806451613,
      "loss": 1.4298,
      "step": 1918
    },
    {
      "epoch": 5.1516778523489934,
      "grad_norm": 0.0853147804737091,
      "learning_rate": 0.00014524193548387095,
      "loss": 1.5586,
      "step": 1919
    },
    {
      "epoch": 5.154362416107382,
      "grad_norm": 0.09546002745628357,
      "learning_rate": 0.00014516129032258063,
      "loss": 1.563,
      "step": 1920
    },
    {
      "epoch": 5.157046979865772,
      "grad_norm": 0.08272725343704224,
      "learning_rate": 0.00014508064516129032,
      "loss": 1.4789,
      "step": 1921
    },
    {
      "epoch": 5.1597315436241615,
      "grad_norm": 0.08282099664211273,
      "learning_rate": 0.000145,
      "loss": 1.4744,
      "step": 1922
    },
    {
      "epoch": 5.16241610738255,
      "grad_norm": 0.08932454884052277,
      "learning_rate": 0.00014491935483870968,
      "loss": 1.4638,
      "step": 1923
    },
    {
      "epoch": 5.16510067114094,
      "grad_norm": 0.0998549610376358,
      "learning_rate": 0.00014483870967741934,
      "loss": 1.436,
      "step": 1924
    },
    {
      "epoch": 5.167785234899329,
      "grad_norm": 0.09067384153604507,
      "learning_rate": 0.00014475806451612902,
      "loss": 1.4742,
      "step": 1925
    },
    {
      "epoch": 5.170469798657718,
      "grad_norm": 0.08967146277427673,
      "learning_rate": 0.00014467741935483868,
      "loss": 1.5189,
      "step": 1926
    },
    {
      "epoch": 5.173154362416107,
      "grad_norm": 0.09059670567512512,
      "learning_rate": 0.00014459677419354836,
      "loss": 1.4558,
      "step": 1927
    },
    {
      "epoch": 5.175838926174497,
      "grad_norm": 0.09807257354259491,
      "learning_rate": 0.00014451612903225805,
      "loss": 1.3971,
      "step": 1928
    },
    {
      "epoch": 5.178523489932886,
      "grad_norm": 0.08178572356700897,
      "learning_rate": 0.00014443548387096773,
      "loss": 1.3774,
      "step": 1929
    },
    {
      "epoch": 5.181208053691275,
      "grad_norm": 0.09614554792642593,
      "learning_rate": 0.0001443548387096774,
      "loss": 1.4427,
      "step": 1930
    },
    {
      "epoch": 5.183892617449664,
      "grad_norm": 0.08356626331806183,
      "learning_rate": 0.00014427419354838707,
      "loss": 1.4772,
      "step": 1931
    },
    {
      "epoch": 5.186577181208054,
      "grad_norm": 0.08497540652751923,
      "learning_rate": 0.00014419354838709675,
      "loss": 1.4808,
      "step": 1932
    },
    {
      "epoch": 5.189261744966443,
      "grad_norm": 0.09636997431516647,
      "learning_rate": 0.00014411290322580644,
      "loss": 1.4432,
      "step": 1933
    },
    {
      "epoch": 5.191946308724832,
      "grad_norm": 0.10444631427526474,
      "learning_rate": 0.00014403225806451612,
      "loss": 1.3748,
      "step": 1934
    },
    {
      "epoch": 5.194630872483222,
      "grad_norm": 0.08822042495012283,
      "learning_rate": 0.0001439516129032258,
      "loss": 1.5005,
      "step": 1935
    },
    {
      "epoch": 5.1973154362416105,
      "grad_norm": 0.08729434758424759,
      "learning_rate": 0.00014387096774193546,
      "loss": 1.4458,
      "step": 1936
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.08762193471193314,
      "learning_rate": 0.00014379032258064514,
      "loss": 1.5531,
      "step": 1937
    },
    {
      "epoch": 5.202684563758389,
      "grad_norm": 0.08526399731636047,
      "learning_rate": 0.00014370967741935483,
      "loss": 1.5487,
      "step": 1938
    },
    {
      "epoch": 5.205369127516779,
      "grad_norm": 0.08374256640672684,
      "learning_rate": 0.0001436290322580645,
      "loss": 1.5011,
      "step": 1939
    },
    {
      "epoch": 5.208053691275167,
      "grad_norm": 0.08710851520299911,
      "learning_rate": 0.0001435483870967742,
      "loss": 1.5989,
      "step": 1940
    },
    {
      "epoch": 5.210738255033557,
      "grad_norm": 0.0996866226196289,
      "learning_rate": 0.00014346774193548385,
      "loss": 1.3733,
      "step": 1941
    },
    {
      "epoch": 5.213422818791947,
      "grad_norm": 0.09248019754886627,
      "learning_rate": 0.00014338709677419353,
      "loss": 1.5655,
      "step": 1942
    },
    {
      "epoch": 5.2161073825503355,
      "grad_norm": 0.08784704655408859,
      "learning_rate": 0.00014330645161290322,
      "loss": 1.4886,
      "step": 1943
    },
    {
      "epoch": 5.218791946308725,
      "grad_norm": 0.08130644261837006,
      "learning_rate": 0.0001432258064516129,
      "loss": 1.4581,
      "step": 1944
    },
    {
      "epoch": 5.221476510067114,
      "grad_norm": 0.09037309885025024,
      "learning_rate": 0.00014314516129032258,
      "loss": 1.4819,
      "step": 1945
    },
    {
      "epoch": 5.2241610738255035,
      "grad_norm": 0.09048108756542206,
      "learning_rate": 0.00014306451612903224,
      "loss": 1.475,
      "step": 1946
    },
    {
      "epoch": 5.226845637583892,
      "grad_norm": 0.08912171423435211,
      "learning_rate": 0.00014298387096774192,
      "loss": 1.3374,
      "step": 1947
    },
    {
      "epoch": 5.229530201342282,
      "grad_norm": 0.08649016171693802,
      "learning_rate": 0.0001429032258064516,
      "loss": 1.5354,
      "step": 1948
    },
    {
      "epoch": 5.232214765100671,
      "grad_norm": 0.09834256768226624,
      "learning_rate": 0.0001428225806451613,
      "loss": 1.3769,
      "step": 1949
    },
    {
      "epoch": 5.23489932885906,
      "grad_norm": 0.10212881863117218,
      "learning_rate": 0.00014274193548387097,
      "loss": 1.5132,
      "step": 1950
    },
    {
      "epoch": 5.23758389261745,
      "grad_norm": 0.08487465977668762,
      "learning_rate": 0.00014266129032258063,
      "loss": 1.5333,
      "step": 1951
    },
    {
      "epoch": 5.240268456375839,
      "grad_norm": 0.09579720348119736,
      "learning_rate": 0.0001425806451612903,
      "loss": 1.4396,
      "step": 1952
    },
    {
      "epoch": 5.2429530201342285,
      "grad_norm": 0.08746648579835892,
      "learning_rate": 0.0001425,
      "loss": 1.5308,
      "step": 1953
    },
    {
      "epoch": 5.245637583892617,
      "grad_norm": 0.08864926546812057,
      "learning_rate": 0.00014241935483870968,
      "loss": 1.5605,
      "step": 1954
    },
    {
      "epoch": 5.248322147651007,
      "grad_norm": 0.08852966874837875,
      "learning_rate": 0.00014233870967741936,
      "loss": 1.3927,
      "step": 1955
    },
    {
      "epoch": 5.251006711409396,
      "grad_norm": 0.08571086823940277,
      "learning_rate": 0.00014225806451612902,
      "loss": 1.4151,
      "step": 1956
    },
    {
      "epoch": 5.253691275167785,
      "grad_norm": 0.09724094718694687,
      "learning_rate": 0.0001421774193548387,
      "loss": 1.5293,
      "step": 1957
    },
    {
      "epoch": 5.256375838926174,
      "grad_norm": 0.09221363067626953,
      "learning_rate": 0.00014209677419354836,
      "loss": 1.4036,
      "step": 1958
    },
    {
      "epoch": 5.259060402684564,
      "grad_norm": 0.08705644309520721,
      "learning_rate": 0.00014201612903225804,
      "loss": 1.5079,
      "step": 1959
    },
    {
      "epoch": 5.261744966442953,
      "grad_norm": 0.0847087949514389,
      "learning_rate": 0.00014193548387096772,
      "loss": 1.6135,
      "step": 1960
    },
    {
      "epoch": 5.264429530201342,
      "grad_norm": 0.08707854151725769,
      "learning_rate": 0.0001418548387096774,
      "loss": 1.3752,
      "step": 1961
    },
    {
      "epoch": 5.267114093959732,
      "grad_norm": 0.10542035102844238,
      "learning_rate": 0.0001417741935483871,
      "loss": 1.4275,
      "step": 1962
    },
    {
      "epoch": 5.269798657718121,
      "grad_norm": 0.0864669531583786,
      "learning_rate": 0.00014169354838709675,
      "loss": 1.499,
      "step": 1963
    },
    {
      "epoch": 5.27248322147651,
      "grad_norm": 0.0888553112745285,
      "learning_rate": 0.00014161290322580643,
      "loss": 1.5172,
      "step": 1964
    },
    {
      "epoch": 5.275167785234899,
      "grad_norm": 0.11733289062976837,
      "learning_rate": 0.0001415322580645161,
      "loss": 1.3913,
      "step": 1965
    },
    {
      "epoch": 5.277852348993289,
      "grad_norm": 0.09107387065887451,
      "learning_rate": 0.0001414516129032258,
      "loss": 1.38,
      "step": 1966
    },
    {
      "epoch": 5.2805369127516775,
      "grad_norm": 0.0865338072180748,
      "learning_rate": 0.00014137096774193548,
      "loss": 1.5028,
      "step": 1967
    },
    {
      "epoch": 5.283221476510067,
      "grad_norm": 0.09356852620840073,
      "learning_rate": 0.00014129032258064514,
      "loss": 1.419,
      "step": 1968
    },
    {
      "epoch": 5.285906040268457,
      "grad_norm": 0.08839605748653412,
      "learning_rate": 0.00014120967741935482,
      "loss": 1.512,
      "step": 1969
    },
    {
      "epoch": 5.2885906040268456,
      "grad_norm": 0.10048183053731918,
      "learning_rate": 0.0001411290322580645,
      "loss": 1.5271,
      "step": 1970
    },
    {
      "epoch": 5.291275167785235,
      "grad_norm": 0.10145744681358337,
      "learning_rate": 0.00014104838709677419,
      "loss": 1.4598,
      "step": 1971
    },
    {
      "epoch": 5.293959731543624,
      "grad_norm": 0.10075157135725021,
      "learning_rate": 0.00014096774193548387,
      "loss": 1.4093,
      "step": 1972
    },
    {
      "epoch": 5.296644295302014,
      "grad_norm": 0.09680880606174469,
      "learning_rate": 0.00014088709677419353,
      "loss": 1.4253,
      "step": 1973
    },
    {
      "epoch": 5.299328859060402,
      "grad_norm": 0.10806576162576675,
      "learning_rate": 0.0001408064516129032,
      "loss": 1.447,
      "step": 1974
    },
    {
      "epoch": 5.302013422818792,
      "grad_norm": 0.08510224521160126,
      "learning_rate": 0.0001407258064516129,
      "loss": 1.5422,
      "step": 1975
    },
    {
      "epoch": 5.304697986577181,
      "grad_norm": 0.08667828142642975,
      "learning_rate": 0.00014064516129032258,
      "loss": 1.4273,
      "step": 1976
    },
    {
      "epoch": 5.3073825503355705,
      "grad_norm": 0.09289512038230896,
      "learning_rate": 0.00014056451612903226,
      "loss": 1.4777,
      "step": 1977
    },
    {
      "epoch": 5.310067114093959,
      "grad_norm": 0.07308397442102432,
      "learning_rate": 0.00014048387096774191,
      "loss": 1.5427,
      "step": 1978
    },
    {
      "epoch": 5.312751677852349,
      "grad_norm": 0.08672966063022614,
      "learning_rate": 0.0001404032258064516,
      "loss": 1.4675,
      "step": 1979
    },
    {
      "epoch": 5.315436241610739,
      "grad_norm": 0.09640584886074066,
      "learning_rate": 0.00014032258064516128,
      "loss": 1.4947,
      "step": 1980
    },
    {
      "epoch": 5.318120805369127,
      "grad_norm": 0.08938607573509216,
      "learning_rate": 0.00014024193548387096,
      "loss": 1.4443,
      "step": 1981
    },
    {
      "epoch": 5.320805369127517,
      "grad_norm": 0.09089498966932297,
      "learning_rate": 0.00014016129032258065,
      "loss": 1.4266,
      "step": 1982
    },
    {
      "epoch": 5.323489932885906,
      "grad_norm": 0.10241682082414627,
      "learning_rate": 0.0001400806451612903,
      "loss": 1.5058,
      "step": 1983
    },
    {
      "epoch": 5.326174496644295,
      "grad_norm": 0.09432286024093628,
      "learning_rate": 0.00014,
      "loss": 1.4285,
      "step": 1984
    },
    {
      "epoch": 5.328859060402684,
      "grad_norm": 0.0819736197590828,
      "learning_rate": 0.00013991935483870964,
      "loss": 1.461,
      "step": 1985
    },
    {
      "epoch": 5.331543624161074,
      "grad_norm": 0.09440211206674576,
      "learning_rate": 0.00013983870967741935,
      "loss": 1.5729,
      "step": 1986
    },
    {
      "epoch": 5.334228187919463,
      "grad_norm": 0.0784040093421936,
      "learning_rate": 0.00013975806451612904,
      "loss": 1.4829,
      "step": 1987
    },
    {
      "epoch": 5.336912751677852,
      "grad_norm": 0.09461160004138947,
      "learning_rate": 0.0001396774193548387,
      "loss": 1.4123,
      "step": 1988
    },
    {
      "epoch": 5.339597315436242,
      "grad_norm": 0.09807971119880676,
      "learning_rate": 0.00013959677419354838,
      "loss": 1.4029,
      "step": 1989
    },
    {
      "epoch": 5.342281879194631,
      "grad_norm": 0.08761537075042725,
      "learning_rate": 0.00013951612903225803,
      "loss": 1.3813,
      "step": 1990
    },
    {
      "epoch": 5.34496644295302,
      "grad_norm": 0.09069456905126572,
      "learning_rate": 0.00013943548387096772,
      "loss": 1.51,
      "step": 1991
    },
    {
      "epoch": 5.347651006711409,
      "grad_norm": 0.08864963054656982,
      "learning_rate": 0.0001393548387096774,
      "loss": 1.4896,
      "step": 1992
    },
    {
      "epoch": 5.350335570469799,
      "grad_norm": 0.08685126900672913,
      "learning_rate": 0.00013927419354838708,
      "loss": 1.452,
      "step": 1993
    },
    {
      "epoch": 5.353020134228188,
      "grad_norm": 0.08338157832622528,
      "learning_rate": 0.00013919354838709677,
      "loss": 1.4808,
      "step": 1994
    },
    {
      "epoch": 5.355704697986577,
      "grad_norm": 0.08443856984376907,
      "learning_rate": 0.00013911290322580642,
      "loss": 1.444,
      "step": 1995
    },
    {
      "epoch": 5.358389261744966,
      "grad_norm": 0.0793013647198677,
      "learning_rate": 0.0001390322580645161,
      "loss": 1.55,
      "step": 1996
    },
    {
      "epoch": 5.361073825503356,
      "grad_norm": 0.09627652168273926,
      "learning_rate": 0.0001389516129032258,
      "loss": 1.5067,
      "step": 1997
    },
    {
      "epoch": 5.363758389261745,
      "grad_norm": 0.0948731079697609,
      "learning_rate": 0.00013887096774193547,
      "loss": 1.4025,
      "step": 1998
    },
    {
      "epoch": 5.366442953020134,
      "grad_norm": 0.09870320558547974,
      "learning_rate": 0.00013879032258064516,
      "loss": 1.5343,
      "step": 1999
    },
    {
      "epoch": 5.369127516778524,
      "grad_norm": 0.08708474785089493,
      "learning_rate": 0.0001387096774193548,
      "loss": 1.4715,
      "step": 2000
    },
    {
      "epoch": 5.3718120805369125,
      "grad_norm": 0.08723032474517822,
      "learning_rate": 0.0001386290322580645,
      "loss": 1.5269,
      "step": 2001
    },
    {
      "epoch": 5.374496644295302,
      "grad_norm": 0.07751334458589554,
      "learning_rate": 0.00013854838709677418,
      "loss": 1.4774,
      "step": 2002
    },
    {
      "epoch": 5.377181208053691,
      "grad_norm": 0.09148503094911575,
      "learning_rate": 0.00013846774193548386,
      "loss": 1.4437,
      "step": 2003
    },
    {
      "epoch": 5.379865771812081,
      "grad_norm": 0.08894174546003342,
      "learning_rate": 0.00013838709677419355,
      "loss": 1.4932,
      "step": 2004
    },
    {
      "epoch": 5.382550335570469,
      "grad_norm": 0.10259220004081726,
      "learning_rate": 0.0001383064516129032,
      "loss": 1.4876,
      "step": 2005
    },
    {
      "epoch": 5.385234899328859,
      "grad_norm": 0.09801249951124191,
      "learning_rate": 0.00013822580645161289,
      "loss": 1.3532,
      "step": 2006
    },
    {
      "epoch": 5.387919463087249,
      "grad_norm": 0.0921051949262619,
      "learning_rate": 0.00013814516129032257,
      "loss": 1.4892,
      "step": 2007
    },
    {
      "epoch": 5.390604026845637,
      "grad_norm": 0.09732022881507874,
      "learning_rate": 0.00013806451612903225,
      "loss": 1.5152,
      "step": 2008
    },
    {
      "epoch": 5.393288590604027,
      "grad_norm": 0.10082031786441803,
      "learning_rate": 0.00013798387096774194,
      "loss": 1.4161,
      "step": 2009
    },
    {
      "epoch": 5.395973154362416,
      "grad_norm": 0.10246004164218903,
      "learning_rate": 0.0001379032258064516,
      "loss": 1.5345,
      "step": 2010
    },
    {
      "epoch": 5.3986577181208055,
      "grad_norm": 0.09305478632450104,
      "learning_rate": 0.00013782258064516127,
      "loss": 1.4652,
      "step": 2011
    },
    {
      "epoch": 5.401342281879194,
      "grad_norm": 0.0942915603518486,
      "learning_rate": 0.00013774193548387096,
      "loss": 1.5057,
      "step": 2012
    },
    {
      "epoch": 5.404026845637584,
      "grad_norm": 0.08544297516345978,
      "learning_rate": 0.00013766129032258064,
      "loss": 1.4401,
      "step": 2013
    },
    {
      "epoch": 5.406711409395973,
      "grad_norm": 0.09285204857587814,
      "learning_rate": 0.00013758064516129032,
      "loss": 1.3703,
      "step": 2014
    },
    {
      "epoch": 5.409395973154362,
      "grad_norm": 0.09315291792154312,
      "learning_rate": 0.00013749999999999998,
      "loss": 1.4229,
      "step": 2015
    },
    {
      "epoch": 5.412080536912752,
      "grad_norm": 0.09631428867578506,
      "learning_rate": 0.00013741935483870966,
      "loss": 1.3658,
      "step": 2016
    },
    {
      "epoch": 5.414765100671141,
      "grad_norm": 0.09101574867963791,
      "learning_rate": 0.00013733870967741935,
      "loss": 1.5271,
      "step": 2017
    },
    {
      "epoch": 5.4174496644295305,
      "grad_norm": 0.09160532057285309,
      "learning_rate": 0.00013725806451612903,
      "loss": 1.3702,
      "step": 2018
    },
    {
      "epoch": 5.420134228187919,
      "grad_norm": 0.10419109463691711,
      "learning_rate": 0.00013717741935483871,
      "loss": 1.4979,
      "step": 2019
    },
    {
      "epoch": 5.422818791946309,
      "grad_norm": 0.09294992685317993,
      "learning_rate": 0.00013709677419354837,
      "loss": 1.4035,
      "step": 2020
    },
    {
      "epoch": 5.425503355704698,
      "grad_norm": 0.0977424681186676,
      "learning_rate": 0.00013701612903225805,
      "loss": 1.3812,
      "step": 2021
    },
    {
      "epoch": 5.428187919463087,
      "grad_norm": 0.097336046397686,
      "learning_rate": 0.00013693548387096774,
      "loss": 1.5111,
      "step": 2022
    },
    {
      "epoch": 5.430872483221476,
      "grad_norm": 0.09779107570648193,
      "learning_rate": 0.0001368548387096774,
      "loss": 1.3364,
      "step": 2023
    },
    {
      "epoch": 5.433557046979866,
      "grad_norm": 0.09692751616239548,
      "learning_rate": 0.00013677419354838708,
      "loss": 1.4759,
      "step": 2024
    },
    {
      "epoch": 5.436241610738255,
      "grad_norm": 0.08763296157121658,
      "learning_rate": 0.00013669354838709676,
      "loss": 1.5199,
      "step": 2025
    },
    {
      "epoch": 5.438926174496644,
      "grad_norm": 0.08547089248895645,
      "learning_rate": 0.00013661290322580644,
      "loss": 1.5312,
      "step": 2026
    },
    {
      "epoch": 5.441610738255034,
      "grad_norm": 0.09751991927623749,
      "learning_rate": 0.0001365322580645161,
      "loss": 1.4051,
      "step": 2027
    },
    {
      "epoch": 5.444295302013423,
      "grad_norm": 0.09169627726078033,
      "learning_rate": 0.00013645161290322578,
      "loss": 1.4932,
      "step": 2028
    },
    {
      "epoch": 5.446979865771812,
      "grad_norm": 0.09888079762458801,
      "learning_rate": 0.00013637096774193547,
      "loss": 1.5072,
      "step": 2029
    },
    {
      "epoch": 5.449664429530201,
      "grad_norm": 0.08865977078676224,
      "learning_rate": 0.00013629032258064515,
      "loss": 1.4634,
      "step": 2030
    },
    {
      "epoch": 5.452348993288591,
      "grad_norm": 0.08131628483533859,
      "learning_rate": 0.00013620967741935483,
      "loss": 1.5295,
      "step": 2031
    },
    {
      "epoch": 5.4550335570469795,
      "grad_norm": 0.08154474943876266,
      "learning_rate": 0.0001361290322580645,
      "loss": 1.4248,
      "step": 2032
    },
    {
      "epoch": 5.457718120805369,
      "grad_norm": 0.08953176438808441,
      "learning_rate": 0.00013604838709677417,
      "loss": 1.3976,
      "step": 2033
    },
    {
      "epoch": 5.460402684563759,
      "grad_norm": 0.09052793681621552,
      "learning_rate": 0.00013596774193548386,
      "loss": 1.5028,
      "step": 2034
    },
    {
      "epoch": 5.4630872483221475,
      "grad_norm": 0.08650092035531998,
      "learning_rate": 0.00013588709677419354,
      "loss": 1.5993,
      "step": 2035
    },
    {
      "epoch": 5.465771812080537,
      "grad_norm": 0.09366446733474731,
      "learning_rate": 0.00013580645161290322,
      "loss": 1.3223,
      "step": 2036
    },
    {
      "epoch": 5.468456375838926,
      "grad_norm": 0.0853295549750328,
      "learning_rate": 0.00013572580645161288,
      "loss": 1.4801,
      "step": 2037
    },
    {
      "epoch": 5.471140939597316,
      "grad_norm": 0.08995718508958817,
      "learning_rate": 0.00013564516129032256,
      "loss": 1.4477,
      "step": 2038
    },
    {
      "epoch": 5.473825503355704,
      "grad_norm": 0.08789464831352234,
      "learning_rate": 0.00013556451612903225,
      "loss": 1.574,
      "step": 2039
    },
    {
      "epoch": 5.476510067114094,
      "grad_norm": 0.08663040399551392,
      "learning_rate": 0.00013548387096774193,
      "loss": 1.4083,
      "step": 2040
    },
    {
      "epoch": 5.479194630872483,
      "grad_norm": 0.08521577715873718,
      "learning_rate": 0.0001354032258064516,
      "loss": 1.4346,
      "step": 2041
    },
    {
      "epoch": 5.4818791946308725,
      "grad_norm": 0.09393798559904099,
      "learning_rate": 0.00013532258064516127,
      "loss": 1.4663,
      "step": 2042
    },
    {
      "epoch": 5.484563758389262,
      "grad_norm": 0.09497247636318207,
      "learning_rate": 0.00013524193548387095,
      "loss": 1.3729,
      "step": 2043
    },
    {
      "epoch": 5.487248322147651,
      "grad_norm": 0.10222795605659485,
      "learning_rate": 0.00013516129032258064,
      "loss": 1.4948,
      "step": 2044
    },
    {
      "epoch": 5.489932885906041,
      "grad_norm": 0.09267094731330872,
      "learning_rate": 0.00013508064516129032,
      "loss": 1.379,
      "step": 2045
    },
    {
      "epoch": 5.492617449664429,
      "grad_norm": 0.09457934647798538,
      "learning_rate": 0.000135,
      "loss": 1.3817,
      "step": 2046
    },
    {
      "epoch": 5.495302013422819,
      "grad_norm": 0.0929926410317421,
      "learning_rate": 0.00013491935483870966,
      "loss": 1.4805,
      "step": 2047
    },
    {
      "epoch": 5.497986577181208,
      "grad_norm": 0.12717527151107788,
      "learning_rate": 0.00013483870967741934,
      "loss": 1.4084,
      "step": 2048
    },
    {
      "epoch": 5.500671140939597,
      "grad_norm": 0.09460895508527756,
      "learning_rate": 0.00013475806451612902,
      "loss": 1.4437,
      "step": 2049
    },
    {
      "epoch": 5.503355704697986,
      "grad_norm": 0.08601433038711548,
      "learning_rate": 0.0001346774193548387,
      "loss": 1.4928,
      "step": 2050
    },
    {
      "epoch": 5.506040268456376,
      "grad_norm": 0.10088850557804108,
      "learning_rate": 0.0001345967741935484,
      "loss": 1.3931,
      "step": 2051
    },
    {
      "epoch": 5.5087248322147655,
      "grad_norm": 0.08650680631399155,
      "learning_rate": 0.00013451612903225805,
      "loss": 1.5385,
      "step": 2052
    },
    {
      "epoch": 5.511409395973154,
      "grad_norm": 0.0916532352566719,
      "learning_rate": 0.00013443548387096773,
      "loss": 1.4875,
      "step": 2053
    },
    {
      "epoch": 5.514093959731544,
      "grad_norm": 0.09934603422880173,
      "learning_rate": 0.00013435483870967741,
      "loss": 1.5632,
      "step": 2054
    },
    {
      "epoch": 5.516778523489933,
      "grad_norm": 0.09985426813364029,
      "learning_rate": 0.00013427419354838707,
      "loss": 1.4323,
      "step": 2055
    },
    {
      "epoch": 5.519463087248322,
      "grad_norm": 0.08624759316444397,
      "learning_rate": 0.00013419354838709675,
      "loss": 1.5822,
      "step": 2056
    },
    {
      "epoch": 5.522147651006711,
      "grad_norm": 0.09387665241956711,
      "learning_rate": 0.00013411290322580644,
      "loss": 1.5643,
      "step": 2057
    },
    {
      "epoch": 5.524832214765101,
      "grad_norm": 0.08116483688354492,
      "learning_rate": 0.00013403225806451612,
      "loss": 1.465,
      "step": 2058
    },
    {
      "epoch": 5.5275167785234895,
      "grad_norm": 0.0934903472661972,
      "learning_rate": 0.0001339516129032258,
      "loss": 1.4816,
      "step": 2059
    },
    {
      "epoch": 5.530201342281879,
      "grad_norm": 0.09078187495470047,
      "learning_rate": 0.00013387096774193546,
      "loss": 1.4872,
      "step": 2060
    },
    {
      "epoch": 5.532885906040269,
      "grad_norm": 0.09444987773895264,
      "learning_rate": 0.00013379032258064514,
      "loss": 1.4147,
      "step": 2061
    },
    {
      "epoch": 5.535570469798658,
      "grad_norm": 0.08754543215036392,
      "learning_rate": 0.00013370967741935483,
      "loss": 1.3633,
      "step": 2062
    },
    {
      "epoch": 5.538255033557047,
      "grad_norm": 0.0962701365351677,
      "learning_rate": 0.0001336290322580645,
      "loss": 1.4671,
      "step": 2063
    },
    {
      "epoch": 5.540939597315436,
      "grad_norm": 0.08539091795682907,
      "learning_rate": 0.0001335483870967742,
      "loss": 1.4644,
      "step": 2064
    },
    {
      "epoch": 5.543624161073826,
      "grad_norm": 0.09084321558475494,
      "learning_rate": 0.00013346774193548385,
      "loss": 1.4398,
      "step": 2065
    },
    {
      "epoch": 5.5463087248322145,
      "grad_norm": 0.09971888363361359,
      "learning_rate": 0.00013338709677419353,
      "loss": 1.4755,
      "step": 2066
    },
    {
      "epoch": 5.548993288590604,
      "grad_norm": 0.07900729030370712,
      "learning_rate": 0.00013330645161290322,
      "loss": 1.5664,
      "step": 2067
    },
    {
      "epoch": 5.551677852348993,
      "grad_norm": 0.08950544893741608,
      "learning_rate": 0.0001332258064516129,
      "loss": 1.3097,
      "step": 2068
    },
    {
      "epoch": 5.554362416107383,
      "grad_norm": 0.09943939745426178,
      "learning_rate": 0.00013314516129032258,
      "loss": 1.5623,
      "step": 2069
    },
    {
      "epoch": 5.557046979865772,
      "grad_norm": 0.09458831697702408,
      "learning_rate": 0.00013306451612903224,
      "loss": 1.3813,
      "step": 2070
    },
    {
      "epoch": 5.559731543624161,
      "grad_norm": 0.09383536875247955,
      "learning_rate": 0.00013298387096774192,
      "loss": 1.4432,
      "step": 2071
    },
    {
      "epoch": 5.562416107382551,
      "grad_norm": 0.09577453881502151,
      "learning_rate": 0.0001329032258064516,
      "loss": 1.498,
      "step": 2072
    },
    {
      "epoch": 5.565100671140939,
      "grad_norm": 0.08986033499240875,
      "learning_rate": 0.0001328225806451613,
      "loss": 1.4058,
      "step": 2073
    },
    {
      "epoch": 5.567785234899329,
      "grad_norm": 0.09490583091974258,
      "learning_rate": 0.00013274193548387095,
      "loss": 1.4644,
      "step": 2074
    },
    {
      "epoch": 5.570469798657718,
      "grad_norm": 0.08584862947463989,
      "learning_rate": 0.00013266129032258063,
      "loss": 1.5118,
      "step": 2075
    },
    {
      "epoch": 5.5731543624161075,
      "grad_norm": 0.07648607343435287,
      "learning_rate": 0.0001325806451612903,
      "loss": 1.5991,
      "step": 2076
    },
    {
      "epoch": 5.575838926174496,
      "grad_norm": 0.09627220779657364,
      "learning_rate": 0.0001325,
      "loss": 1.5829,
      "step": 2077
    },
    {
      "epoch": 5.578523489932886,
      "grad_norm": 0.08719839155673981,
      "learning_rate": 0.00013241935483870968,
      "loss": 1.5368,
      "step": 2078
    },
    {
      "epoch": 5.581208053691276,
      "grad_norm": 0.08549150824546814,
      "learning_rate": 0.00013233870967741933,
      "loss": 1.4297,
      "step": 2079
    },
    {
      "epoch": 5.583892617449664,
      "grad_norm": 0.09435499459505081,
      "learning_rate": 0.00013225806451612902,
      "loss": 1.4503,
      "step": 2080
    },
    {
      "epoch": 5.586577181208054,
      "grad_norm": 0.0903841033577919,
      "learning_rate": 0.0001321774193548387,
      "loss": 1.4437,
      "step": 2081
    },
    {
      "epoch": 5.589261744966443,
      "grad_norm": 0.09532278031110764,
      "learning_rate": 0.00013209677419354838,
      "loss": 1.6118,
      "step": 2082
    },
    {
      "epoch": 5.5919463087248324,
      "grad_norm": 0.0818227156996727,
      "learning_rate": 0.00013201612903225807,
      "loss": 1.4393,
      "step": 2083
    },
    {
      "epoch": 5.594630872483221,
      "grad_norm": 0.09718282520771027,
      "learning_rate": 0.00013193548387096772,
      "loss": 1.5099,
      "step": 2084
    },
    {
      "epoch": 5.597315436241611,
      "grad_norm": 0.09560216963291168,
      "learning_rate": 0.0001318548387096774,
      "loss": 1.485,
      "step": 2085
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.08426328748464584,
      "learning_rate": 0.0001317741935483871,
      "loss": 1.4738,
      "step": 2086
    },
    {
      "epoch": 5.602684563758389,
      "grad_norm": 0.0880574882030487,
      "learning_rate": 0.00013169354838709675,
      "loss": 1.6184,
      "step": 2087
    },
    {
      "epoch": 5.605369127516779,
      "grad_norm": 0.09575124830007553,
      "learning_rate": 0.00013161290322580643,
      "loss": 1.2729,
      "step": 2088
    },
    {
      "epoch": 5.608053691275168,
      "grad_norm": 0.09450628608465195,
      "learning_rate": 0.00013153225806451611,
      "loss": 1.4303,
      "step": 2089
    },
    {
      "epoch": 5.610738255033557,
      "grad_norm": 0.08421164005994797,
      "learning_rate": 0.0001314516129032258,
      "loss": 1.5474,
      "step": 2090
    },
    {
      "epoch": 5.613422818791946,
      "grad_norm": 0.09243973344564438,
      "learning_rate": 0.00013137096774193548,
      "loss": 1.4006,
      "step": 2091
    },
    {
      "epoch": 5.616107382550336,
      "grad_norm": 0.08920426666736603,
      "learning_rate": 0.00013129032258064514,
      "loss": 1.4179,
      "step": 2092
    },
    {
      "epoch": 5.618791946308725,
      "grad_norm": 0.09085322171449661,
      "learning_rate": 0.00013120967741935482,
      "loss": 1.4395,
      "step": 2093
    },
    {
      "epoch": 5.621476510067114,
      "grad_norm": 0.09210625290870667,
      "learning_rate": 0.0001311290322580645,
      "loss": 1.4349,
      "step": 2094
    },
    {
      "epoch": 5.624161073825503,
      "grad_norm": 0.09806814044713974,
      "learning_rate": 0.0001310483870967742,
      "loss": 1.4578,
      "step": 2095
    },
    {
      "epoch": 5.626845637583893,
      "grad_norm": 0.08135005831718445,
      "learning_rate": 0.00013096774193548387,
      "loss": 1.5365,
      "step": 2096
    },
    {
      "epoch": 5.629530201342282,
      "grad_norm": 0.08592363446950912,
      "learning_rate": 0.00013088709677419353,
      "loss": 1.5261,
      "step": 2097
    },
    {
      "epoch": 5.632214765100671,
      "grad_norm": 0.10517682880163193,
      "learning_rate": 0.0001308064516129032,
      "loss": 1.4865,
      "step": 2098
    },
    {
      "epoch": 5.634899328859061,
      "grad_norm": 0.091743603348732,
      "learning_rate": 0.0001307258064516129,
      "loss": 1.5536,
      "step": 2099
    },
    {
      "epoch": 5.6375838926174495,
      "grad_norm": 0.09212517738342285,
      "learning_rate": 0.00013064516129032258,
      "loss": 1.5169,
      "step": 2100
    },
    {
      "epoch": 5.640268456375839,
      "grad_norm": 0.09882355481386185,
      "learning_rate": 0.00013056451612903226,
      "loss": 1.4013,
      "step": 2101
    },
    {
      "epoch": 5.642953020134228,
      "grad_norm": 0.09111490845680237,
      "learning_rate": 0.00013048387096774192,
      "loss": 1.4097,
      "step": 2102
    },
    {
      "epoch": 5.645637583892618,
      "grad_norm": 0.09502600133419037,
      "learning_rate": 0.0001304032258064516,
      "loss": 1.5103,
      "step": 2103
    },
    {
      "epoch": 5.648322147651006,
      "grad_norm": 0.090052530169487,
      "learning_rate": 0.00013032258064516128,
      "loss": 1.3315,
      "step": 2104
    },
    {
      "epoch": 5.651006711409396,
      "grad_norm": 0.08527049422264099,
      "learning_rate": 0.00013024193548387097,
      "loss": 1.3992,
      "step": 2105
    },
    {
      "epoch": 5.653691275167786,
      "grad_norm": 0.09774373471736908,
      "learning_rate": 0.00013016129032258065,
      "loss": 1.3598,
      "step": 2106
    },
    {
      "epoch": 5.6563758389261745,
      "grad_norm": 0.09496133029460907,
      "learning_rate": 0.0001300806451612903,
      "loss": 1.4437,
      "step": 2107
    },
    {
      "epoch": 5.659060402684564,
      "grad_norm": 0.09520216286182404,
      "learning_rate": 0.00013,
      "loss": 1.5491,
      "step": 2108
    },
    {
      "epoch": 5.661744966442953,
      "grad_norm": 0.0971122533082962,
      "learning_rate": 0.00012991935483870967,
      "loss": 1.37,
      "step": 2109
    },
    {
      "epoch": 5.6644295302013425,
      "grad_norm": 0.09949575364589691,
      "learning_rate": 0.00012983870967741936,
      "loss": 1.5181,
      "step": 2110
    },
    {
      "epoch": 5.667114093959731,
      "grad_norm": 0.10194159299135208,
      "learning_rate": 0.00012975806451612904,
      "loss": 1.5392,
      "step": 2111
    },
    {
      "epoch": 5.669798657718121,
      "grad_norm": 0.09016907215118408,
      "learning_rate": 0.0001296774193548387,
      "loss": 1.534,
      "step": 2112
    },
    {
      "epoch": 5.67248322147651,
      "grad_norm": 0.09329433739185333,
      "learning_rate": 0.00012959677419354838,
      "loss": 1.4889,
      "step": 2113
    },
    {
      "epoch": 5.675167785234899,
      "grad_norm": 0.08043286204338074,
      "learning_rate": 0.00012951612903225806,
      "loss": 1.5472,
      "step": 2114
    },
    {
      "epoch": 5.677852348993289,
      "grad_norm": 0.09012037515640259,
      "learning_rate": 0.00012943548387096774,
      "loss": 1.5501,
      "step": 2115
    },
    {
      "epoch": 5.680536912751678,
      "grad_norm": 0.08899424225091934,
      "learning_rate": 0.0001293548387096774,
      "loss": 1.3774,
      "step": 2116
    },
    {
      "epoch": 5.6832214765100675,
      "grad_norm": 0.09100283682346344,
      "learning_rate": 0.00012927419354838708,
      "loss": 1.3896,
      "step": 2117
    },
    {
      "epoch": 5.685906040268456,
      "grad_norm": 0.08225171267986298,
      "learning_rate": 0.00012919354838709677,
      "loss": 1.495,
      "step": 2118
    },
    {
      "epoch": 5.688590604026846,
      "grad_norm": 0.08503968268632889,
      "learning_rate": 0.00012911290322580642,
      "loss": 1.4605,
      "step": 2119
    },
    {
      "epoch": 5.691275167785235,
      "grad_norm": 0.09313353151082993,
      "learning_rate": 0.0001290322580645161,
      "loss": 1.4597,
      "step": 2120
    },
    {
      "epoch": 5.693959731543624,
      "grad_norm": 0.09727485477924347,
      "learning_rate": 0.0001289516129032258,
      "loss": 1.5014,
      "step": 2121
    },
    {
      "epoch": 5.696644295302013,
      "grad_norm": 0.09620785713195801,
      "learning_rate": 0.00012887096774193547,
      "loss": 1.5632,
      "step": 2122
    },
    {
      "epoch": 5.699328859060403,
      "grad_norm": 0.10277087241411209,
      "learning_rate": 0.00012879032258064516,
      "loss": 1.487,
      "step": 2123
    },
    {
      "epoch": 5.702013422818792,
      "grad_norm": 0.09122512489557266,
      "learning_rate": 0.0001287096774193548,
      "loss": 1.3372,
      "step": 2124
    },
    {
      "epoch": 5.704697986577181,
      "grad_norm": 0.09609226137399673,
      "learning_rate": 0.0001286290322580645,
      "loss": 1.4199,
      "step": 2125
    },
    {
      "epoch": 5.707382550335571,
      "grad_norm": 0.0998038500547409,
      "learning_rate": 0.00012854838709677418,
      "loss": 1.4547,
      "step": 2126
    },
    {
      "epoch": 5.71006711409396,
      "grad_norm": 0.09230616688728333,
      "learning_rate": 0.00012846774193548386,
      "loss": 1.4273,
      "step": 2127
    },
    {
      "epoch": 5.712751677852349,
      "grad_norm": 0.09044083207845688,
      "learning_rate": 0.00012838709677419355,
      "loss": 1.4721,
      "step": 2128
    },
    {
      "epoch": 5.715436241610738,
      "grad_norm": 0.10179485380649567,
      "learning_rate": 0.0001283064516129032,
      "loss": 1.5471,
      "step": 2129
    },
    {
      "epoch": 5.718120805369128,
      "grad_norm": 0.09383893758058548,
      "learning_rate": 0.00012822580645161289,
      "loss": 1.4578,
      "step": 2130
    },
    {
      "epoch": 5.7208053691275165,
      "grad_norm": 0.0928698405623436,
      "learning_rate": 0.00012814516129032257,
      "loss": 1.462,
      "step": 2131
    },
    {
      "epoch": 5.723489932885906,
      "grad_norm": 0.09596528112888336,
      "learning_rate": 0.00012806451612903225,
      "loss": 1.4907,
      "step": 2132
    },
    {
      "epoch": 5.726174496644296,
      "grad_norm": 0.09559383988380432,
      "learning_rate": 0.00012798387096774194,
      "loss": 1.4261,
      "step": 2133
    },
    {
      "epoch": 5.7288590604026846,
      "grad_norm": 0.08608534932136536,
      "learning_rate": 0.0001279032258064516,
      "loss": 1.6306,
      "step": 2134
    },
    {
      "epoch": 5.731543624161074,
      "grad_norm": 0.0865309089422226,
      "learning_rate": 0.00012782258064516128,
      "loss": 1.543,
      "step": 2135
    },
    {
      "epoch": 5.734228187919463,
      "grad_norm": 0.09105023741722107,
      "learning_rate": 0.00012774193548387096,
      "loss": 1.5036,
      "step": 2136
    },
    {
      "epoch": 5.736912751677853,
      "grad_norm": 0.09972839802503586,
      "learning_rate": 0.00012766129032258064,
      "loss": 1.4527,
      "step": 2137
    },
    {
      "epoch": 5.739597315436241,
      "grad_norm": 0.08031575381755829,
      "learning_rate": 0.00012758064516129033,
      "loss": 1.49,
      "step": 2138
    },
    {
      "epoch": 5.742281879194631,
      "grad_norm": 0.08918453752994537,
      "learning_rate": 0.00012749999999999998,
      "loss": 1.4809,
      "step": 2139
    },
    {
      "epoch": 5.74496644295302,
      "grad_norm": 0.08574272692203522,
      "learning_rate": 0.00012741935483870967,
      "loss": 1.4598,
      "step": 2140
    },
    {
      "epoch": 5.7476510067114095,
      "grad_norm": 0.09668165445327759,
      "learning_rate": 0.00012733870967741935,
      "loss": 1.4183,
      "step": 2141
    },
    {
      "epoch": 5.750335570469799,
      "grad_norm": 0.08294578641653061,
      "learning_rate": 0.00012725806451612903,
      "loss": 1.4191,
      "step": 2142
    },
    {
      "epoch": 5.753020134228188,
      "grad_norm": 0.09804584830999374,
      "learning_rate": 0.00012717741935483872,
      "loss": 1.4231,
      "step": 2143
    },
    {
      "epoch": 5.755704697986577,
      "grad_norm": 0.09036374092102051,
      "learning_rate": 0.00012709677419354837,
      "loss": 1.5072,
      "step": 2144
    },
    {
      "epoch": 5.758389261744966,
      "grad_norm": 0.08393306285142899,
      "learning_rate": 0.00012701612903225805,
      "loss": 1.5134,
      "step": 2145
    },
    {
      "epoch": 5.761073825503356,
      "grad_norm": 0.10620675981044769,
      "learning_rate": 0.0001269354838709677,
      "loss": 1.4516,
      "step": 2146
    },
    {
      "epoch": 5.763758389261745,
      "grad_norm": 0.09956373274326324,
      "learning_rate": 0.00012685483870967742,
      "loss": 1.5325,
      "step": 2147
    },
    {
      "epoch": 5.766442953020134,
      "grad_norm": 0.10357517004013062,
      "learning_rate": 0.0001267741935483871,
      "loss": 1.3373,
      "step": 2148
    },
    {
      "epoch": 5.769127516778523,
      "grad_norm": 0.08947053551673889,
      "learning_rate": 0.00012669354838709676,
      "loss": 1.4931,
      "step": 2149
    },
    {
      "epoch": 5.771812080536913,
      "grad_norm": 0.086810402572155,
      "learning_rate": 0.00012661290322580644,
      "loss": 1.3636,
      "step": 2150
    },
    {
      "epoch": 5.774496644295302,
      "grad_norm": 0.10293082147836685,
      "learning_rate": 0.0001265322580645161,
      "loss": 1.4059,
      "step": 2151
    },
    {
      "epoch": 5.777181208053691,
      "grad_norm": 0.09012714773416519,
      "learning_rate": 0.00012645161290322578,
      "loss": 1.4122,
      "step": 2152
    },
    {
      "epoch": 5.77986577181208,
      "grad_norm": 0.09090206027030945,
      "learning_rate": 0.00012637096774193547,
      "loss": 1.481,
      "step": 2153
    },
    {
      "epoch": 5.78255033557047,
      "grad_norm": 0.09226557612419128,
      "learning_rate": 0.00012629032258064515,
      "loss": 1.4162,
      "step": 2154
    },
    {
      "epoch": 5.785234899328859,
      "grad_norm": 0.09153870493173599,
      "learning_rate": 0.00012620967741935483,
      "loss": 1.4689,
      "step": 2155
    },
    {
      "epoch": 5.787919463087248,
      "grad_norm": 0.08914920687675476,
      "learning_rate": 0.0001261290322580645,
      "loss": 1.4315,
      "step": 2156
    },
    {
      "epoch": 5.790604026845638,
      "grad_norm": 0.08835616707801819,
      "learning_rate": 0.00012604838709677417,
      "loss": 1.462,
      "step": 2157
    },
    {
      "epoch": 5.793288590604027,
      "grad_norm": 0.0930747538805008,
      "learning_rate": 0.00012596774193548386,
      "loss": 1.6351,
      "step": 2158
    },
    {
      "epoch": 5.795973154362416,
      "grad_norm": 0.10511241108179092,
      "learning_rate": 0.00012588709677419354,
      "loss": 1.3673,
      "step": 2159
    },
    {
      "epoch": 5.798657718120805,
      "grad_norm": 0.09691420942544937,
      "learning_rate": 0.00012580645161290322,
      "loss": 1.5032,
      "step": 2160
    },
    {
      "epoch": 5.801342281879195,
      "grad_norm": 0.0884113684296608,
      "learning_rate": 0.00012572580645161288,
      "loss": 1.4924,
      "step": 2161
    },
    {
      "epoch": 5.804026845637583,
      "grad_norm": 0.09727756679058075,
      "learning_rate": 0.00012564516129032256,
      "loss": 1.5064,
      "step": 2162
    },
    {
      "epoch": 5.806711409395973,
      "grad_norm": 0.08752008527517319,
      "learning_rate": 0.00012556451612903225,
      "loss": 1.4838,
      "step": 2163
    },
    {
      "epoch": 5.809395973154363,
      "grad_norm": 0.08759687095880508,
      "learning_rate": 0.00012548387096774193,
      "loss": 1.4967,
      "step": 2164
    },
    {
      "epoch": 5.8120805369127515,
      "grad_norm": 0.085545115172863,
      "learning_rate": 0.0001254032258064516,
      "loss": 1.5712,
      "step": 2165
    },
    {
      "epoch": 5.814765100671141,
      "grad_norm": 0.09736347198486328,
      "learning_rate": 0.00012532258064516127,
      "loss": 1.3758,
      "step": 2166
    },
    {
      "epoch": 5.81744966442953,
      "grad_norm": 0.09755026549100876,
      "learning_rate": 0.00012524193548387095,
      "loss": 1.5445,
      "step": 2167
    },
    {
      "epoch": 5.82013422818792,
      "grad_norm": 0.09549173712730408,
      "learning_rate": 0.00012516129032258064,
      "loss": 1.4658,
      "step": 2168
    },
    {
      "epoch": 5.822818791946308,
      "grad_norm": 0.09735995531082153,
      "learning_rate": 0.00012508064516129032,
      "loss": 1.3932,
      "step": 2169
    },
    {
      "epoch": 5.825503355704698,
      "grad_norm": 0.08504433929920197,
      "learning_rate": 0.000125,
      "loss": 1.3305,
      "step": 2170
    },
    {
      "epoch": 5.828187919463087,
      "grad_norm": 0.0944555252790451,
      "learning_rate": 0.00012491935483870966,
      "loss": 1.4021,
      "step": 2171
    },
    {
      "epoch": 5.830872483221476,
      "grad_norm": 0.08556873351335526,
      "learning_rate": 0.00012483870967741934,
      "loss": 1.5121,
      "step": 2172
    },
    {
      "epoch": 5.833557046979866,
      "grad_norm": 0.08861517161130905,
      "learning_rate": 0.00012475806451612903,
      "loss": 1.4133,
      "step": 2173
    },
    {
      "epoch": 5.836241610738255,
      "grad_norm": 0.08889982849359512,
      "learning_rate": 0.0001246774193548387,
      "loss": 1.599,
      "step": 2174
    },
    {
      "epoch": 5.8389261744966445,
      "grad_norm": 0.09304486960172653,
      "learning_rate": 0.0001245967741935484,
      "loss": 1.4061,
      "step": 2175
    },
    {
      "epoch": 5.841610738255033,
      "grad_norm": 0.09455039352178574,
      "learning_rate": 0.00012451612903225805,
      "loss": 1.4571,
      "step": 2176
    },
    {
      "epoch": 5.844295302013423,
      "grad_norm": 0.08884783834218979,
      "learning_rate": 0.00012443548387096773,
      "loss": 1.5107,
      "step": 2177
    },
    {
      "epoch": 5.846979865771812,
      "grad_norm": 0.0929427519440651,
      "learning_rate": 0.0001243548387096774,
      "loss": 1.4198,
      "step": 2178
    },
    {
      "epoch": 5.849664429530201,
      "grad_norm": 0.08710912615060806,
      "learning_rate": 0.0001242741935483871,
      "loss": 1.4047,
      "step": 2179
    },
    {
      "epoch": 5.85234899328859,
      "grad_norm": 0.09529203921556473,
      "learning_rate": 0.00012419354838709678,
      "loss": 1.4499,
      "step": 2180
    },
    {
      "epoch": 5.85503355704698,
      "grad_norm": 0.08410590887069702,
      "learning_rate": 0.00012411290322580644,
      "loss": 1.5463,
      "step": 2181
    },
    {
      "epoch": 5.8577181208053695,
      "grad_norm": 0.09343264251947403,
      "learning_rate": 0.00012403225806451612,
      "loss": 1.5244,
      "step": 2182
    },
    {
      "epoch": 5.860402684563758,
      "grad_norm": 0.10962960869073868,
      "learning_rate": 0.00012395161290322578,
      "loss": 1.4093,
      "step": 2183
    },
    {
      "epoch": 5.863087248322148,
      "grad_norm": 0.09848658740520477,
      "learning_rate": 0.00012387096774193546,
      "loss": 1.3158,
      "step": 2184
    },
    {
      "epoch": 5.865771812080537,
      "grad_norm": 0.08541146665811539,
      "learning_rate": 0.00012379032258064514,
      "loss": 1.4212,
      "step": 2185
    },
    {
      "epoch": 5.868456375838926,
      "grad_norm": 0.0979633554816246,
      "learning_rate": 0.00012370967741935483,
      "loss": 1.3407,
      "step": 2186
    },
    {
      "epoch": 5.871140939597315,
      "grad_norm": 0.08327792584896088,
      "learning_rate": 0.0001236290322580645,
      "loss": 1.5244,
      "step": 2187
    },
    {
      "epoch": 5.873825503355705,
      "grad_norm": 0.0860094502568245,
      "learning_rate": 0.00012354838709677417,
      "loss": 1.4702,
      "step": 2188
    },
    {
      "epoch": 5.8765100671140935,
      "grad_norm": 0.08790361881256104,
      "learning_rate": 0.00012346774193548385,
      "loss": 1.4367,
      "step": 2189
    },
    {
      "epoch": 5.879194630872483,
      "grad_norm": 0.08573692291975021,
      "learning_rate": 0.00012338709677419353,
      "loss": 1.5538,
      "step": 2190
    },
    {
      "epoch": 5.881879194630873,
      "grad_norm": 0.09504631906747818,
      "learning_rate": 0.00012330645161290322,
      "loss": 1.3845,
      "step": 2191
    },
    {
      "epoch": 5.884563758389262,
      "grad_norm": 0.0851665735244751,
      "learning_rate": 0.0001232258064516129,
      "loss": 1.4044,
      "step": 2192
    },
    {
      "epoch": 5.887248322147651,
      "grad_norm": 0.09995847940444946,
      "learning_rate": 0.00012314516129032256,
      "loss": 1.5438,
      "step": 2193
    },
    {
      "epoch": 5.88993288590604,
      "grad_norm": 0.10368083417415619,
      "learning_rate": 0.00012306451612903224,
      "loss": 1.3828,
      "step": 2194
    },
    {
      "epoch": 5.89261744966443,
      "grad_norm": 0.09699202328920364,
      "learning_rate": 0.00012298387096774192,
      "loss": 1.4446,
      "step": 2195
    },
    {
      "epoch": 5.8953020134228185,
      "grad_norm": 0.09412431716918945,
      "learning_rate": 0.0001229032258064516,
      "loss": 1.5449,
      "step": 2196
    },
    {
      "epoch": 5.897986577181208,
      "grad_norm": 0.08901873230934143,
      "learning_rate": 0.0001228225806451613,
      "loss": 1.4871,
      "step": 2197
    },
    {
      "epoch": 5.900671140939597,
      "grad_norm": 0.08449900150299072,
      "learning_rate": 0.00012274193548387095,
      "loss": 1.4551,
      "step": 2198
    },
    {
      "epoch": 5.9033557046979865,
      "grad_norm": 0.11449085921049118,
      "learning_rate": 0.00012266129032258063,
      "loss": 1.3086,
      "step": 2199
    },
    {
      "epoch": 5.906040268456376,
      "grad_norm": 0.09464345127344131,
      "learning_rate": 0.0001225806451612903,
      "loss": 1.3766,
      "step": 2200
    },
    {
      "epoch": 5.908724832214765,
      "grad_norm": 0.0839221253991127,
      "learning_rate": 0.0001225,
      "loss": 1.5318,
      "step": 2201
    },
    {
      "epoch": 5.911409395973155,
      "grad_norm": 0.09959373623132706,
      "learning_rate": 0.00012241935483870968,
      "loss": 1.4312,
      "step": 2202
    },
    {
      "epoch": 5.914093959731543,
      "grad_norm": 0.10720603913068771,
      "learning_rate": 0.00012233870967741934,
      "loss": 1.3882,
      "step": 2203
    },
    {
      "epoch": 5.916778523489933,
      "grad_norm": 0.09666817635297775,
      "learning_rate": 0.00012225806451612902,
      "loss": 1.422,
      "step": 2204
    },
    {
      "epoch": 5.919463087248322,
      "grad_norm": 0.09493549168109894,
      "learning_rate": 0.0001221774193548387,
      "loss": 1.4319,
      "step": 2205
    },
    {
      "epoch": 5.9221476510067115,
      "grad_norm": 0.09746954590082169,
      "learning_rate": 0.00012209677419354839,
      "loss": 1.4188,
      "step": 2206
    },
    {
      "epoch": 5.9248322147651,
      "grad_norm": 0.09126239269971848,
      "learning_rate": 0.00012201612903225806,
      "loss": 1.4642,
      "step": 2207
    },
    {
      "epoch": 5.92751677852349,
      "grad_norm": 0.08605694770812988,
      "learning_rate": 0.00012193548387096773,
      "loss": 1.4411,
      "step": 2208
    },
    {
      "epoch": 5.93020134228188,
      "grad_norm": 0.09659497439861298,
      "learning_rate": 0.00012185483870967741,
      "loss": 1.4311,
      "step": 2209
    },
    {
      "epoch": 5.932885906040268,
      "grad_norm": 0.09176915884017944,
      "learning_rate": 0.00012177419354838708,
      "loss": 1.3371,
      "step": 2210
    },
    {
      "epoch": 5.935570469798658,
      "grad_norm": 0.10030053555965424,
      "learning_rate": 0.00012169354838709676,
      "loss": 1.2802,
      "step": 2211
    },
    {
      "epoch": 5.938255033557047,
      "grad_norm": 0.08737383782863617,
      "learning_rate": 0.00012161290322580644,
      "loss": 1.4438,
      "step": 2212
    },
    {
      "epoch": 5.940939597315436,
      "grad_norm": 0.10162018239498138,
      "learning_rate": 0.00012153225806451611,
      "loss": 1.3898,
      "step": 2213
    },
    {
      "epoch": 5.943624161073825,
      "grad_norm": 0.08935032784938812,
      "learning_rate": 0.0001214516129032258,
      "loss": 1.4687,
      "step": 2214
    },
    {
      "epoch": 5.946308724832215,
      "grad_norm": 0.10328256338834763,
      "learning_rate": 0.00012137096774193547,
      "loss": 1.4851,
      "step": 2215
    },
    {
      "epoch": 5.948993288590604,
      "grad_norm": 0.10136589407920837,
      "learning_rate": 0.00012129032258064515,
      "loss": 1.4796,
      "step": 2216
    },
    {
      "epoch": 5.951677852348993,
      "grad_norm": 0.0949927270412445,
      "learning_rate": 0.00012120967741935483,
      "loss": 1.392,
      "step": 2217
    },
    {
      "epoch": 5.954362416107383,
      "grad_norm": 0.08054277300834656,
      "learning_rate": 0.0001211290322580645,
      "loss": 1.4606,
      "step": 2218
    },
    {
      "epoch": 5.957046979865772,
      "grad_norm": 0.09549827128648758,
      "learning_rate": 0.00012104838709677419,
      "loss": 1.4034,
      "step": 2219
    },
    {
      "epoch": 5.959731543624161,
      "grad_norm": 0.09087624400854111,
      "learning_rate": 0.00012096774193548386,
      "loss": 1.4395,
      "step": 2220
    },
    {
      "epoch": 5.96241610738255,
      "grad_norm": 0.09948616474866867,
      "learning_rate": 0.00012088709677419354,
      "loss": 1.4354,
      "step": 2221
    },
    {
      "epoch": 5.96510067114094,
      "grad_norm": 0.09605897217988968,
      "learning_rate": 0.00012080645161290322,
      "loss": 1.4792,
      "step": 2222
    },
    {
      "epoch": 5.9677852348993286,
      "grad_norm": 0.09172433614730835,
      "learning_rate": 0.0001207258064516129,
      "loss": 1.4226,
      "step": 2223
    },
    {
      "epoch": 5.970469798657718,
      "grad_norm": 0.09778720140457153,
      "learning_rate": 0.00012064516129032258,
      "loss": 1.3585,
      "step": 2224
    },
    {
      "epoch": 5.973154362416107,
      "grad_norm": 0.09983301907777786,
      "learning_rate": 0.00012056451612903225,
      "loss": 1.3846,
      "step": 2225
    },
    {
      "epoch": 5.975838926174497,
      "grad_norm": 0.08896221965551376,
      "learning_rate": 0.00012048387096774193,
      "loss": 1.4808,
      "step": 2226
    },
    {
      "epoch": 5.978523489932886,
      "grad_norm": 0.0935269296169281,
      "learning_rate": 0.00012040322580645161,
      "loss": 1.3771,
      "step": 2227
    },
    {
      "epoch": 5.981208053691275,
      "grad_norm": 0.09178176522254944,
      "learning_rate": 0.00012032258064516127,
      "loss": 1.4434,
      "step": 2228
    },
    {
      "epoch": 5.983892617449665,
      "grad_norm": 0.09479950368404388,
      "learning_rate": 0.00012024193548387097,
      "loss": 1.5445,
      "step": 2229
    },
    {
      "epoch": 5.9865771812080535,
      "grad_norm": 0.09889396280050278,
      "learning_rate": 0.00012016129032258062,
      "loss": 1.4178,
      "step": 2230
    },
    {
      "epoch": 5.989261744966443,
      "grad_norm": 0.08552584052085876,
      "learning_rate": 0.0001200806451612903,
      "loss": 1.4737,
      "step": 2231
    },
    {
      "epoch": 5.991946308724832,
      "grad_norm": 0.10061053931713104,
      "learning_rate": 0.00011999999999999999,
      "loss": 1.5539,
      "step": 2232
    },
    {
      "epoch": 5.994630872483222,
      "grad_norm": 0.08850859850645065,
      "learning_rate": 0.00011991935483870966,
      "loss": 1.5544,
      "step": 2233
    },
    {
      "epoch": 5.99731543624161,
      "grad_norm": 0.09383528679609299,
      "learning_rate": 0.00011983870967741934,
      "loss": 1.5455,
      "step": 2234
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.10936184227466583,
      "learning_rate": 0.00011975806451612901,
      "loss": 1.3901,
      "step": 2235
    },
    {
      "epoch": 6.00268456375839,
      "grad_norm": 0.08780965954065323,
      "learning_rate": 0.0001196774193548387,
      "loss": 1.396,
      "step": 2236
    },
    {
      "epoch": 6.005369127516778,
      "grad_norm": 0.08604709059000015,
      "learning_rate": 0.00011959677419354838,
      "loss": 1.5853,
      "step": 2237
    },
    {
      "epoch": 6.008053691275168,
      "grad_norm": 0.10273955017328262,
      "learning_rate": 0.00011951612903225805,
      "loss": 1.4251,
      "step": 2238
    },
    {
      "epoch": 6.010738255033557,
      "grad_norm": 0.1045430526137352,
      "learning_rate": 0.00011943548387096773,
      "loss": 1.4326,
      "step": 2239
    },
    {
      "epoch": 6.0134228187919465,
      "grad_norm": 0.09041975438594818,
      "learning_rate": 0.0001193548387096774,
      "loss": 1.3466,
      "step": 2240
    },
    {
      "epoch": 6.016107382550335,
      "grad_norm": 0.08281620591878891,
      "learning_rate": 0.00011927419354838709,
      "loss": 1.5134,
      "step": 2241
    },
    {
      "epoch": 6.018791946308725,
      "grad_norm": 0.0913451537489891,
      "learning_rate": 0.00011919354838709677,
      "loss": 1.455,
      "step": 2242
    },
    {
      "epoch": 6.021476510067114,
      "grad_norm": 0.09746610373258591,
      "learning_rate": 0.00011911290322580644,
      "loss": 1.3913,
      "step": 2243
    },
    {
      "epoch": 6.024161073825503,
      "grad_norm": 0.08592335879802704,
      "learning_rate": 0.00011903225806451612,
      "loss": 1.4815,
      "step": 2244
    },
    {
      "epoch": 6.026845637583893,
      "grad_norm": 0.10112529247999191,
      "learning_rate": 0.00011895161290322579,
      "loss": 1.4196,
      "step": 2245
    },
    {
      "epoch": 6.029530201342282,
      "grad_norm": 0.08790828287601471,
      "learning_rate": 0.00011887096774193547,
      "loss": 1.4464,
      "step": 2246
    },
    {
      "epoch": 6.0322147651006714,
      "grad_norm": 0.10708366334438324,
      "learning_rate": 0.00011879032258064516,
      "loss": 1.3931,
      "step": 2247
    },
    {
      "epoch": 6.03489932885906,
      "grad_norm": 0.1074765995144844,
      "learning_rate": 0.00011870967741935483,
      "loss": 1.3401,
      "step": 2248
    },
    {
      "epoch": 6.03758389261745,
      "grad_norm": 0.1064840704202652,
      "learning_rate": 0.00011862903225806451,
      "loss": 1.3894,
      "step": 2249
    },
    {
      "epoch": 6.040268456375839,
      "grad_norm": 0.09028726816177368,
      "learning_rate": 0.00011854838709677418,
      "loss": 1.5326,
      "step": 2250
    },
    {
      "epoch": 6.042953020134228,
      "grad_norm": 0.09215462952852249,
      "learning_rate": 0.00011846774193548386,
      "loss": 1.4187,
      "step": 2251
    },
    {
      "epoch": 6.045637583892617,
      "grad_norm": 0.09949668496847153,
      "learning_rate": 0.00011838709677419355,
      "loss": 1.5479,
      "step": 2252
    },
    {
      "epoch": 6.048322147651007,
      "grad_norm": 0.0915016308426857,
      "learning_rate": 0.00011830645161290322,
      "loss": 1.4149,
      "step": 2253
    },
    {
      "epoch": 6.051006711409396,
      "grad_norm": 0.09264665096998215,
      "learning_rate": 0.0001182258064516129,
      "loss": 1.4954,
      "step": 2254
    },
    {
      "epoch": 6.053691275167785,
      "grad_norm": 0.07930096238851547,
      "learning_rate": 0.00011814516129032257,
      "loss": 1.6371,
      "step": 2255
    },
    {
      "epoch": 6.056375838926175,
      "grad_norm": 0.09363283216953278,
      "learning_rate": 0.00011806451612903225,
      "loss": 1.4657,
      "step": 2256
    },
    {
      "epoch": 6.059060402684564,
      "grad_norm": 0.09060930460691452,
      "learning_rate": 0.00011798387096774192,
      "loss": 1.5247,
      "step": 2257
    },
    {
      "epoch": 6.061744966442953,
      "grad_norm": 0.08009963482618332,
      "learning_rate": 0.00011790322580645161,
      "loss": 1.4197,
      "step": 2258
    },
    {
      "epoch": 6.064429530201342,
      "grad_norm": 0.0888025090098381,
      "learning_rate": 0.00011782258064516129,
      "loss": 1.4693,
      "step": 2259
    },
    {
      "epoch": 6.067114093959732,
      "grad_norm": 0.08356209099292755,
      "learning_rate": 0.00011774193548387095,
      "loss": 1.4549,
      "step": 2260
    },
    {
      "epoch": 6.06979865771812,
      "grad_norm": 0.10438044369220734,
      "learning_rate": 0.00011766129032258064,
      "loss": 1.3329,
      "step": 2261
    },
    {
      "epoch": 6.07248322147651,
      "grad_norm": 0.11178400367498398,
      "learning_rate": 0.0001175806451612903,
      "loss": 1.4255,
      "step": 2262
    },
    {
      "epoch": 6.0751677852349,
      "grad_norm": 0.08276717364788055,
      "learning_rate": 0.00011749999999999998,
      "loss": 1.4277,
      "step": 2263
    },
    {
      "epoch": 6.0778523489932885,
      "grad_norm": 0.08505908399820328,
      "learning_rate": 0.00011741935483870967,
      "loss": 1.5109,
      "step": 2264
    },
    {
      "epoch": 6.080536912751678,
      "grad_norm": 0.10325690358877182,
      "learning_rate": 0.00011733870967741934,
      "loss": 1.4365,
      "step": 2265
    },
    {
      "epoch": 6.083221476510067,
      "grad_norm": 0.08381608873605728,
      "learning_rate": 0.00011725806451612902,
      "loss": 1.5191,
      "step": 2266
    },
    {
      "epoch": 6.085906040268457,
      "grad_norm": 0.09420318156480789,
      "learning_rate": 0.00011717741935483869,
      "loss": 1.491,
      "step": 2267
    },
    {
      "epoch": 6.088590604026845,
      "grad_norm": 0.08534255623817444,
      "learning_rate": 0.00011709677419354837,
      "loss": 1.5234,
      "step": 2268
    },
    {
      "epoch": 6.091275167785235,
      "grad_norm": 0.09071828424930573,
      "learning_rate": 0.00011701612903225806,
      "loss": 1.4934,
      "step": 2269
    },
    {
      "epoch": 6.093959731543624,
      "grad_norm": 0.08446766436100006,
      "learning_rate": 0.00011693548387096773,
      "loss": 1.4453,
      "step": 2270
    },
    {
      "epoch": 6.0966442953020135,
      "grad_norm": 0.09683176875114441,
      "learning_rate": 0.00011685483870967741,
      "loss": 1.5043,
      "step": 2271
    },
    {
      "epoch": 6.099328859060403,
      "grad_norm": 0.08154713362455368,
      "learning_rate": 0.00011677419354838708,
      "loss": 1.485,
      "step": 2272
    },
    {
      "epoch": 6.102013422818792,
      "grad_norm": 0.08059791475534439,
      "learning_rate": 0.00011669354838709676,
      "loss": 1.4798,
      "step": 2273
    },
    {
      "epoch": 6.1046979865771815,
      "grad_norm": 0.09616515785455704,
      "learning_rate": 0.00011661290322580645,
      "loss": 1.4258,
      "step": 2274
    },
    {
      "epoch": 6.10738255033557,
      "grad_norm": 0.09100332856178284,
      "learning_rate": 0.00011653225806451612,
      "loss": 1.4128,
      "step": 2275
    },
    {
      "epoch": 6.11006711409396,
      "grad_norm": 0.09049677848815918,
      "learning_rate": 0.0001164516129032258,
      "loss": 1.4926,
      "step": 2276
    },
    {
      "epoch": 6.112751677852349,
      "grad_norm": 0.08541765064001083,
      "learning_rate": 0.00011637096774193547,
      "loss": 1.5761,
      "step": 2277
    },
    {
      "epoch": 6.115436241610738,
      "grad_norm": 0.08694741129875183,
      "learning_rate": 0.00011629032258064515,
      "loss": 1.3586,
      "step": 2278
    },
    {
      "epoch": 6.118120805369127,
      "grad_norm": 0.09092336148023605,
      "learning_rate": 0.00011620967741935483,
      "loss": 1.5183,
      "step": 2279
    },
    {
      "epoch": 6.120805369127517,
      "grad_norm": 0.08575978875160217,
      "learning_rate": 0.0001161290322580645,
      "loss": 1.5193,
      "step": 2280
    },
    {
      "epoch": 6.1234899328859065,
      "grad_norm": 0.09660353511571884,
      "learning_rate": 0.00011604838709677419,
      "loss": 1.4626,
      "step": 2281
    },
    {
      "epoch": 6.126174496644295,
      "grad_norm": 0.0779700055718422,
      "learning_rate": 0.00011596774193548386,
      "loss": 1.4354,
      "step": 2282
    },
    {
      "epoch": 6.128859060402685,
      "grad_norm": 0.09419804811477661,
      "learning_rate": 0.00011588709677419354,
      "loss": 1.3629,
      "step": 2283
    },
    {
      "epoch": 6.131543624161074,
      "grad_norm": 0.08570920675992966,
      "learning_rate": 0.00011580645161290322,
      "loss": 1.4576,
      "step": 2284
    },
    {
      "epoch": 6.134228187919463,
      "grad_norm": 0.09212992340326309,
      "learning_rate": 0.0001157258064516129,
      "loss": 1.4565,
      "step": 2285
    },
    {
      "epoch": 6.136912751677852,
      "grad_norm": 0.09172125160694122,
      "learning_rate": 0.00011564516129032258,
      "loss": 1.3674,
      "step": 2286
    },
    {
      "epoch": 6.139597315436242,
      "grad_norm": 0.09332431852817535,
      "learning_rate": 0.00011556451612903225,
      "loss": 1.5078,
      "step": 2287
    },
    {
      "epoch": 6.1422818791946305,
      "grad_norm": 0.09029731899499893,
      "learning_rate": 0.00011548387096774193,
      "loss": 1.5252,
      "step": 2288
    },
    {
      "epoch": 6.14496644295302,
      "grad_norm": 0.08380445837974548,
      "learning_rate": 0.00011540322580645161,
      "loss": 1.5666,
      "step": 2289
    },
    {
      "epoch": 6.14765100671141,
      "grad_norm": 0.09285883605480194,
      "learning_rate": 0.00011532258064516128,
      "loss": 1.4875,
      "step": 2290
    },
    {
      "epoch": 6.150335570469799,
      "grad_norm": 0.08240009844303131,
      "learning_rate": 0.00011524193548387097,
      "loss": 1.5231,
      "step": 2291
    },
    {
      "epoch": 6.153020134228188,
      "grad_norm": 0.09016541391611099,
      "learning_rate": 0.00011516129032258062,
      "loss": 1.5315,
      "step": 2292
    },
    {
      "epoch": 6.155704697986577,
      "grad_norm": 0.09236167371273041,
      "learning_rate": 0.00011508064516129032,
      "loss": 1.4855,
      "step": 2293
    },
    {
      "epoch": 6.158389261744967,
      "grad_norm": 0.08808315545320511,
      "learning_rate": 0.000115,
      "loss": 1.4135,
      "step": 2294
    },
    {
      "epoch": 6.1610738255033555,
      "grad_norm": 0.09170921891927719,
      "learning_rate": 0.00011491935483870966,
      "loss": 1.5815,
      "step": 2295
    },
    {
      "epoch": 6.163758389261745,
      "grad_norm": 0.0875004455447197,
      "learning_rate": 0.00011483870967741934,
      "loss": 1.4123,
      "step": 2296
    },
    {
      "epoch": 6.166442953020134,
      "grad_norm": 0.0842844694852829,
      "learning_rate": 0.00011475806451612901,
      "loss": 1.5244,
      "step": 2297
    },
    {
      "epoch": 6.169127516778524,
      "grad_norm": 0.09094950556755066,
      "learning_rate": 0.0001146774193548387,
      "loss": 1.4062,
      "step": 2298
    },
    {
      "epoch": 6.171812080536912,
      "grad_norm": 0.0970596894621849,
      "learning_rate": 0.00011459677419354838,
      "loss": 1.3352,
      "step": 2299
    },
    {
      "epoch": 6.174496644295302,
      "grad_norm": 0.09502683579921722,
      "learning_rate": 0.00011451612903225805,
      "loss": 1.4408,
      "step": 2300
    },
    {
      "epoch": 6.177181208053692,
      "grad_norm": 0.08580215275287628,
      "learning_rate": 0.00011443548387096773,
      "loss": 1.4736,
      "step": 2301
    },
    {
      "epoch": 6.17986577181208,
      "grad_norm": 0.08958392590284348,
      "learning_rate": 0.0001143548387096774,
      "loss": 1.367,
      "step": 2302
    },
    {
      "epoch": 6.18255033557047,
      "grad_norm": 0.0919526070356369,
      "learning_rate": 0.00011427419354838709,
      "loss": 1.4418,
      "step": 2303
    },
    {
      "epoch": 6.185234899328859,
      "grad_norm": 0.08485976606607437,
      "learning_rate": 0.00011419354838709676,
      "loss": 1.5077,
      "step": 2304
    },
    {
      "epoch": 6.1879194630872485,
      "grad_norm": 0.09136693179607391,
      "learning_rate": 0.00011411290322580644,
      "loss": 1.4372,
      "step": 2305
    },
    {
      "epoch": 6.190604026845637,
      "grad_norm": 0.0895620808005333,
      "learning_rate": 0.00011403225806451612,
      "loss": 1.4093,
      "step": 2306
    },
    {
      "epoch": 6.193288590604027,
      "grad_norm": 0.09155617654323578,
      "learning_rate": 0.00011395161290322579,
      "loss": 1.5034,
      "step": 2307
    },
    {
      "epoch": 6.195973154362416,
      "grad_norm": 0.09572719037532806,
      "learning_rate": 0.00011387096774193548,
      "loss": 1.5043,
      "step": 2308
    },
    {
      "epoch": 6.198657718120805,
      "grad_norm": 0.09891846030950546,
      "learning_rate": 0.00011379032258064514,
      "loss": 1.3882,
      "step": 2309
    },
    {
      "epoch": 6.201342281879195,
      "grad_norm": 0.10594727098941803,
      "learning_rate": 0.00011370967741935483,
      "loss": 1.3864,
      "step": 2310
    },
    {
      "epoch": 6.204026845637584,
      "grad_norm": 0.09292814135551453,
      "learning_rate": 0.00011362903225806451,
      "loss": 1.3571,
      "step": 2311
    },
    {
      "epoch": 6.206711409395973,
      "grad_norm": 0.10988149046897888,
      "learning_rate": 0.00011354838709677418,
      "loss": 1.4216,
      "step": 2312
    },
    {
      "epoch": 6.209395973154362,
      "grad_norm": 0.1020892858505249,
      "learning_rate": 0.00011346774193548386,
      "loss": 1.4608,
      "step": 2313
    },
    {
      "epoch": 6.212080536912752,
      "grad_norm": 0.09367543458938599,
      "learning_rate": 0.00011338709677419353,
      "loss": 1.4198,
      "step": 2314
    },
    {
      "epoch": 6.214765100671141,
      "grad_norm": 0.09412689507007599,
      "learning_rate": 0.00011330645161290322,
      "loss": 1.4488,
      "step": 2315
    },
    {
      "epoch": 6.21744966442953,
      "grad_norm": 0.10171541571617126,
      "learning_rate": 0.0001132258064516129,
      "loss": 1.4121,
      "step": 2316
    },
    {
      "epoch": 6.220134228187919,
      "grad_norm": 0.08896466344594955,
      "learning_rate": 0.00011314516129032257,
      "loss": 1.563,
      "step": 2317
    },
    {
      "epoch": 6.222818791946309,
      "grad_norm": 0.08497761189937592,
      "learning_rate": 0.00011306451612903225,
      "loss": 1.5232,
      "step": 2318
    },
    {
      "epoch": 6.225503355704698,
      "grad_norm": 0.08963020145893097,
      "learning_rate": 0.00011298387096774192,
      "loss": 1.4475,
      "step": 2319
    },
    {
      "epoch": 6.228187919463087,
      "grad_norm": 0.09092816710472107,
      "learning_rate": 0.00011290322580645161,
      "loss": 1.5523,
      "step": 2320
    },
    {
      "epoch": 6.230872483221477,
      "grad_norm": 0.09582845121622086,
      "learning_rate": 0.00011282258064516129,
      "loss": 1.5103,
      "step": 2321
    },
    {
      "epoch": 6.233557046979866,
      "grad_norm": 0.09395751357078552,
      "learning_rate": 0.00011274193548387096,
      "loss": 1.5096,
      "step": 2322
    },
    {
      "epoch": 6.236241610738255,
      "grad_norm": 0.09172752499580383,
      "learning_rate": 0.00011266129032258064,
      "loss": 1.5644,
      "step": 2323
    },
    {
      "epoch": 6.238926174496644,
      "grad_norm": 0.09172845631837845,
      "learning_rate": 0.0001125806451612903,
      "loss": 1.4614,
      "step": 2324
    },
    {
      "epoch": 6.241610738255034,
      "grad_norm": 0.08726565539836884,
      "learning_rate": 0.0001125,
      "loss": 1.5649,
      "step": 2325
    },
    {
      "epoch": 6.244295302013422,
      "grad_norm": 0.09753306955099106,
      "learning_rate": 0.00011241935483870968,
      "loss": 1.4258,
      "step": 2326
    },
    {
      "epoch": 6.246979865771812,
      "grad_norm": 0.0873626321554184,
      "learning_rate": 0.00011233870967741934,
      "loss": 1.5237,
      "step": 2327
    },
    {
      "epoch": 6.249664429530202,
      "grad_norm": 0.09202967584133148,
      "learning_rate": 0.00011225806451612902,
      "loss": 1.4297,
      "step": 2328
    },
    {
      "epoch": 6.2523489932885905,
      "grad_norm": 0.09228914976119995,
      "learning_rate": 0.00011217741935483869,
      "loss": 1.5127,
      "step": 2329
    },
    {
      "epoch": 6.25503355704698,
      "grad_norm": 0.10828002542257309,
      "learning_rate": 0.00011209677419354837,
      "loss": 1.3437,
      "step": 2330
    },
    {
      "epoch": 6.257718120805369,
      "grad_norm": 0.09695393592119217,
      "learning_rate": 0.00011201612903225806,
      "loss": 1.4467,
      "step": 2331
    },
    {
      "epoch": 6.260402684563759,
      "grad_norm": 0.09624236822128296,
      "learning_rate": 0.00011193548387096773,
      "loss": 1.5596,
      "step": 2332
    },
    {
      "epoch": 6.263087248322147,
      "grad_norm": 0.09824347496032715,
      "learning_rate": 0.00011185483870967741,
      "loss": 1.4817,
      "step": 2333
    },
    {
      "epoch": 6.265771812080537,
      "grad_norm": 0.08877924829721451,
      "learning_rate": 0.00011177419354838708,
      "loss": 1.2672,
      "step": 2334
    },
    {
      "epoch": 6.268456375838926,
      "grad_norm": 0.09190568327903748,
      "learning_rate": 0.00011169354838709676,
      "loss": 1.5742,
      "step": 2335
    },
    {
      "epoch": 6.2711409395973154,
      "grad_norm": 0.10808984935283661,
      "learning_rate": 0.00011161290322580645,
      "loss": 1.3721,
      "step": 2336
    },
    {
      "epoch": 6.273825503355705,
      "grad_norm": 0.0886220708489418,
      "learning_rate": 0.00011153225806451612,
      "loss": 1.4818,
      "step": 2337
    },
    {
      "epoch": 6.276510067114094,
      "grad_norm": 0.0867404192686081,
      "learning_rate": 0.0001114516129032258,
      "loss": 1.4054,
      "step": 2338
    },
    {
      "epoch": 6.2791946308724835,
      "grad_norm": 0.09126688539981842,
      "learning_rate": 0.00011137096774193547,
      "loss": 1.4202,
      "step": 2339
    },
    {
      "epoch": 6.281879194630872,
      "grad_norm": 0.09085620939731598,
      "learning_rate": 0.00011129032258064515,
      "loss": 1.4527,
      "step": 2340
    },
    {
      "epoch": 6.284563758389262,
      "grad_norm": 0.09042380750179291,
      "learning_rate": 0.00011120967741935484,
      "loss": 1.3951,
      "step": 2341
    },
    {
      "epoch": 6.287248322147651,
      "grad_norm": 0.09858293831348419,
      "learning_rate": 0.0001111290322580645,
      "loss": 1.337,
      "step": 2342
    },
    {
      "epoch": 6.28993288590604,
      "grad_norm": 0.10297205299139023,
      "learning_rate": 0.00011104838709677419,
      "loss": 1.4501,
      "step": 2343
    },
    {
      "epoch": 6.292617449664429,
      "grad_norm": 0.08780890703201294,
      "learning_rate": 0.00011096774193548386,
      "loss": 1.4294,
      "step": 2344
    },
    {
      "epoch": 6.295302013422819,
      "grad_norm": 0.09921110421419144,
      "learning_rate": 0.00011088709677419354,
      "loss": 1.395,
      "step": 2345
    },
    {
      "epoch": 6.2979865771812085,
      "grad_norm": 0.0928187444806099,
      "learning_rate": 0.00011080645161290322,
      "loss": 1.444,
      "step": 2346
    },
    {
      "epoch": 6.300671140939597,
      "grad_norm": 0.08517806231975555,
      "learning_rate": 0.0001107258064516129,
      "loss": 1.4269,
      "step": 2347
    },
    {
      "epoch": 6.303355704697987,
      "grad_norm": 0.0970279723405838,
      "learning_rate": 0.00011064516129032258,
      "loss": 1.4317,
      "step": 2348
    },
    {
      "epoch": 6.306040268456376,
      "grad_norm": 0.09202000498771667,
      "learning_rate": 0.00011056451612903225,
      "loss": 1.4851,
      "step": 2349
    },
    {
      "epoch": 6.308724832214765,
      "grad_norm": 0.08175984770059586,
      "learning_rate": 0.00011048387096774193,
      "loss": 1.5379,
      "step": 2350
    },
    {
      "epoch": 6.311409395973154,
      "grad_norm": 0.09641751646995544,
      "learning_rate": 0.0001104032258064516,
      "loss": 1.3453,
      "step": 2351
    },
    {
      "epoch": 6.314093959731544,
      "grad_norm": 0.10903086513280869,
      "learning_rate": 0.00011032258064516128,
      "loss": 1.331,
      "step": 2352
    },
    {
      "epoch": 6.3167785234899325,
      "grad_norm": 0.10548481345176697,
      "learning_rate": 0.00011024193548387097,
      "loss": 1.4112,
      "step": 2353
    },
    {
      "epoch": 6.319463087248322,
      "grad_norm": 0.09301329404115677,
      "learning_rate": 0.00011016129032258064,
      "loss": 1.5284,
      "step": 2354
    },
    {
      "epoch": 6.322147651006711,
      "grad_norm": 0.0998813733458519,
      "learning_rate": 0.00011008064516129032,
      "loss": 1.4163,
      "step": 2355
    },
    {
      "epoch": 6.324832214765101,
      "grad_norm": 0.09692491590976715,
      "learning_rate": 0.00010999999999999998,
      "loss": 1.3847,
      "step": 2356
    },
    {
      "epoch": 6.32751677852349,
      "grad_norm": 0.09934353083372116,
      "learning_rate": 0.00010991935483870966,
      "loss": 1.4985,
      "step": 2357
    },
    {
      "epoch": 6.330201342281879,
      "grad_norm": 0.10234396159648895,
      "learning_rate": 0.00010983870967741936,
      "loss": 1.4813,
      "step": 2358
    },
    {
      "epoch": 6.332885906040269,
      "grad_norm": 0.09346035122871399,
      "learning_rate": 0.00010975806451612901,
      "loss": 1.4077,
      "step": 2359
    },
    {
      "epoch": 6.3355704697986575,
      "grad_norm": 0.08919606357812881,
      "learning_rate": 0.0001096774193548387,
      "loss": 1.5352,
      "step": 2360
    },
    {
      "epoch": 6.338255033557047,
      "grad_norm": 0.09095203876495361,
      "learning_rate": 0.00010959677419354837,
      "loss": 1.4841,
      "step": 2361
    },
    {
      "epoch": 6.340939597315436,
      "grad_norm": 0.09785066545009613,
      "learning_rate": 0.00010951612903225805,
      "loss": 1.4881,
      "step": 2362
    },
    {
      "epoch": 6.3436241610738255,
      "grad_norm": 0.10453546792268753,
      "learning_rate": 0.00010943548387096773,
      "loss": 1.4955,
      "step": 2363
    },
    {
      "epoch": 6.346308724832214,
      "grad_norm": 0.09937439113855362,
      "learning_rate": 0.0001093548387096774,
      "loss": 1.4426,
      "step": 2364
    },
    {
      "epoch": 6.348993288590604,
      "grad_norm": 0.09534648060798645,
      "learning_rate": 0.00010927419354838709,
      "loss": 1.4771,
      "step": 2365
    },
    {
      "epoch": 6.351677852348994,
      "grad_norm": 0.09149020165205002,
      "learning_rate": 0.00010919354838709676,
      "loss": 1.426,
      "step": 2366
    },
    {
      "epoch": 6.354362416107382,
      "grad_norm": 0.08680018782615662,
      "learning_rate": 0.00010911290322580644,
      "loss": 1.6121,
      "step": 2367
    },
    {
      "epoch": 6.357046979865772,
      "grad_norm": 0.10829117894172668,
      "learning_rate": 0.00010903225806451612,
      "loss": 1.3184,
      "step": 2368
    },
    {
      "epoch": 6.359731543624161,
      "grad_norm": 0.09819401800632477,
      "learning_rate": 0.00010895161290322579,
      "loss": 1.4051,
      "step": 2369
    },
    {
      "epoch": 6.3624161073825505,
      "grad_norm": 0.09500064700841904,
      "learning_rate": 0.00010887096774193548,
      "loss": 1.4752,
      "step": 2370
    },
    {
      "epoch": 6.365100671140939,
      "grad_norm": 0.0850568562746048,
      "learning_rate": 0.00010879032258064515,
      "loss": 1.5972,
      "step": 2371
    },
    {
      "epoch": 6.367785234899329,
      "grad_norm": 0.0975109338760376,
      "learning_rate": 0.00010870967741935483,
      "loss": 1.3738,
      "step": 2372
    },
    {
      "epoch": 6.370469798657718,
      "grad_norm": 0.09918095171451569,
      "learning_rate": 0.00010862903225806451,
      "loss": 1.5271,
      "step": 2373
    },
    {
      "epoch": 6.373154362416107,
      "grad_norm": 0.09028355777263641,
      "learning_rate": 0.00010854838709677418,
      "loss": 1.3684,
      "step": 2374
    },
    {
      "epoch": 6.375838926174497,
      "grad_norm": 0.09197153896093369,
      "learning_rate": 0.00010846774193548387,
      "loss": 1.5964,
      "step": 2375
    },
    {
      "epoch": 6.378523489932886,
      "grad_norm": 0.09264518320560455,
      "learning_rate": 0.00010838709677419353,
      "loss": 1.3939,
      "step": 2376
    },
    {
      "epoch": 6.381208053691275,
      "grad_norm": 0.0887693241238594,
      "learning_rate": 0.00010830645161290322,
      "loss": 1.426,
      "step": 2377
    },
    {
      "epoch": 6.383892617449664,
      "grad_norm": 0.08331546932458878,
      "learning_rate": 0.0001082258064516129,
      "loss": 1.512,
      "step": 2378
    },
    {
      "epoch": 6.386577181208054,
      "grad_norm": 0.10395964980125427,
      "learning_rate": 0.00010814516129032257,
      "loss": 1.5588,
      "step": 2379
    },
    {
      "epoch": 6.389261744966443,
      "grad_norm": 0.09300317615270615,
      "learning_rate": 0.00010806451612903225,
      "loss": 1.3613,
      "step": 2380
    },
    {
      "epoch": 6.391946308724832,
      "grad_norm": 0.08745460957288742,
      "learning_rate": 0.00010798387096774192,
      "loss": 1.504,
      "step": 2381
    },
    {
      "epoch": 6.394630872483221,
      "grad_norm": 0.09043123573064804,
      "learning_rate": 0.00010790322580645161,
      "loss": 1.4687,
      "step": 2382
    },
    {
      "epoch": 6.397315436241611,
      "grad_norm": 0.10325142741203308,
      "learning_rate": 0.00010782258064516129,
      "loss": 1.4427,
      "step": 2383
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.09458201378583908,
      "learning_rate": 0.00010774193548387096,
      "loss": 1.3691,
      "step": 2384
    },
    {
      "epoch": 6.402684563758389,
      "grad_norm": 0.1016714870929718,
      "learning_rate": 0.00010766129032258064,
      "loss": 1.3856,
      "step": 2385
    },
    {
      "epoch": 6.405369127516779,
      "grad_norm": 0.09495661407709122,
      "learning_rate": 0.00010758064516129031,
      "loss": 1.3849,
      "step": 2386
    },
    {
      "epoch": 6.4080536912751676,
      "grad_norm": 0.08215579390525818,
      "learning_rate": 0.0001075,
      "loss": 1.567,
      "step": 2387
    },
    {
      "epoch": 6.410738255033557,
      "grad_norm": 0.08846195787191391,
      "learning_rate": 0.00010741935483870968,
      "loss": 1.4689,
      "step": 2388
    },
    {
      "epoch": 6.413422818791946,
      "grad_norm": 0.0928778424859047,
      "learning_rate": 0.00010733870967741934,
      "loss": 1.4864,
      "step": 2389
    },
    {
      "epoch": 6.416107382550336,
      "grad_norm": 0.0889519602060318,
      "learning_rate": 0.00010725806451612903,
      "loss": 1.4097,
      "step": 2390
    },
    {
      "epoch": 6.418791946308724,
      "grad_norm": 0.09393565356731415,
      "learning_rate": 0.00010717741935483869,
      "loss": 1.5697,
      "step": 2391
    },
    {
      "epoch": 6.421476510067114,
      "grad_norm": 0.09011014550924301,
      "learning_rate": 0.00010709677419354837,
      "loss": 1.4679,
      "step": 2392
    },
    {
      "epoch": 6.424161073825504,
      "grad_norm": 0.08998718857765198,
      "learning_rate": 0.00010701612903225806,
      "loss": 1.4151,
      "step": 2393
    },
    {
      "epoch": 6.4268456375838925,
      "grad_norm": 0.09092453867197037,
      "learning_rate": 0.00010693548387096773,
      "loss": 1.5488,
      "step": 2394
    },
    {
      "epoch": 6.429530201342282,
      "grad_norm": 0.08947920799255371,
      "learning_rate": 0.00010685483870967741,
      "loss": 1.4027,
      "step": 2395
    },
    {
      "epoch": 6.432214765100671,
      "grad_norm": 0.09093912690877914,
      "learning_rate": 0.00010677419354838708,
      "loss": 1.4971,
      "step": 2396
    },
    {
      "epoch": 6.434899328859061,
      "grad_norm": 0.08424371480941772,
      "learning_rate": 0.00010669354838709676,
      "loss": 1.5852,
      "step": 2397
    },
    {
      "epoch": 6.437583892617449,
      "grad_norm": 0.08042825013399124,
      "learning_rate": 0.00010661290322580643,
      "loss": 1.5539,
      "step": 2398
    },
    {
      "epoch": 6.440268456375839,
      "grad_norm": 0.09612590819597244,
      "learning_rate": 0.00010653225806451612,
      "loss": 1.3906,
      "step": 2399
    },
    {
      "epoch": 6.442953020134228,
      "grad_norm": 0.0898652896285057,
      "learning_rate": 0.0001064516129032258,
      "loss": 1.4134,
      "step": 2400
    },
    {
      "epoch": 6.445637583892617,
      "grad_norm": 0.0869654044508934,
      "learning_rate": 0.00010637096774193547,
      "loss": 1.5693,
      "step": 2401
    },
    {
      "epoch": 6.448322147651007,
      "grad_norm": 0.09516064822673798,
      "learning_rate": 0.00010629032258064515,
      "loss": 1.4843,
      "step": 2402
    },
    {
      "epoch": 6.451006711409396,
      "grad_norm": 0.08475631475448608,
      "learning_rate": 0.00010620967741935482,
      "loss": 1.4343,
      "step": 2403
    },
    {
      "epoch": 6.4536912751677855,
      "grad_norm": 0.08279988169670105,
      "learning_rate": 0.0001061290322580645,
      "loss": 1.5105,
      "step": 2404
    },
    {
      "epoch": 6.456375838926174,
      "grad_norm": 0.08869785070419312,
      "learning_rate": 0.00010604838709677419,
      "loss": 1.4096,
      "step": 2405
    },
    {
      "epoch": 6.459060402684564,
      "grad_norm": 0.0841936245560646,
      "learning_rate": 0.00010596774193548386,
      "loss": 1.422,
      "step": 2406
    },
    {
      "epoch": 6.461744966442953,
      "grad_norm": 0.08083663880825043,
      "learning_rate": 0.00010588709677419354,
      "loss": 1.5948,
      "step": 2407
    },
    {
      "epoch": 6.464429530201342,
      "grad_norm": 0.0961143970489502,
      "learning_rate": 0.00010580645161290321,
      "loss": 1.4186,
      "step": 2408
    },
    {
      "epoch": 6.467114093959731,
      "grad_norm": 0.09048481285572052,
      "learning_rate": 0.0001057258064516129,
      "loss": 1.4355,
      "step": 2409
    },
    {
      "epoch": 6.469798657718121,
      "grad_norm": 0.0905708447098732,
      "learning_rate": 0.00010564516129032258,
      "loss": 1.4383,
      "step": 2410
    },
    {
      "epoch": 6.4724832214765105,
      "grad_norm": 0.09754873812198639,
      "learning_rate": 0.00010556451612903225,
      "loss": 1.5259,
      "step": 2411
    },
    {
      "epoch": 6.475167785234899,
      "grad_norm": 0.09968137741088867,
      "learning_rate": 0.00010548387096774193,
      "loss": 1.4714,
      "step": 2412
    },
    {
      "epoch": 6.477852348993289,
      "grad_norm": 0.08536466956138611,
      "learning_rate": 0.0001054032258064516,
      "loss": 1.5409,
      "step": 2413
    },
    {
      "epoch": 6.480536912751678,
      "grad_norm": 0.09188959747552872,
      "learning_rate": 0.00010532258064516128,
      "loss": 1.4602,
      "step": 2414
    },
    {
      "epoch": 6.483221476510067,
      "grad_norm": 0.08245500177145004,
      "learning_rate": 0.00010524193548387097,
      "loss": 1.4363,
      "step": 2415
    },
    {
      "epoch": 6.485906040268456,
      "grad_norm": 0.09046337753534317,
      "learning_rate": 0.00010516129032258064,
      "loss": 1.3665,
      "step": 2416
    },
    {
      "epoch": 6.488590604026846,
      "grad_norm": 0.08659966289997101,
      "learning_rate": 0.00010508064516129032,
      "loss": 1.6795,
      "step": 2417
    },
    {
      "epoch": 6.4912751677852345,
      "grad_norm": 0.09483111649751663,
      "learning_rate": 0.00010499999999999999,
      "loss": 1.4681,
      "step": 2418
    },
    {
      "epoch": 6.493959731543624,
      "grad_norm": 0.1032705307006836,
      "learning_rate": 0.00010491935483870967,
      "loss": 1.3667,
      "step": 2419
    },
    {
      "epoch": 6.496644295302014,
      "grad_norm": 0.10048729181289673,
      "learning_rate": 0.00010483870967741936,
      "loss": 1.4439,
      "step": 2420
    },
    {
      "epoch": 6.499328859060403,
      "grad_norm": 0.09216488897800446,
      "learning_rate": 0.00010475806451612901,
      "loss": 1.378,
      "step": 2421
    },
    {
      "epoch": 6.502013422818792,
      "grad_norm": 0.10008186101913452,
      "learning_rate": 0.00010467741935483871,
      "loss": 1.3865,
      "step": 2422
    },
    {
      "epoch": 6.504697986577181,
      "grad_norm": 0.09454921633005142,
      "learning_rate": 0.00010459677419354837,
      "loss": 1.5319,
      "step": 2423
    },
    {
      "epoch": 6.507382550335571,
      "grad_norm": 0.0869658812880516,
      "learning_rate": 0.00010451612903225805,
      "loss": 1.4685,
      "step": 2424
    },
    {
      "epoch": 6.510067114093959,
      "grad_norm": 0.08818444609642029,
      "learning_rate": 0.00010443548387096773,
      "loss": 1.5406,
      "step": 2425
    },
    {
      "epoch": 6.512751677852349,
      "grad_norm": 0.08474048972129822,
      "learning_rate": 0.0001043548387096774,
      "loss": 1.5335,
      "step": 2426
    },
    {
      "epoch": 6.515436241610738,
      "grad_norm": 0.08383812755346298,
      "learning_rate": 0.00010427419354838709,
      "loss": 1.4584,
      "step": 2427
    },
    {
      "epoch": 6.5181208053691275,
      "grad_norm": 0.09638652205467224,
      "learning_rate": 0.00010419354838709676,
      "loss": 1.3069,
      "step": 2428
    },
    {
      "epoch": 6.520805369127517,
      "grad_norm": 0.0946073979139328,
      "learning_rate": 0.00010411290322580644,
      "loss": 1.384,
      "step": 2429
    },
    {
      "epoch": 6.523489932885906,
      "grad_norm": 0.09596967697143555,
      "learning_rate": 0.00010403225806451612,
      "loss": 1.4596,
      "step": 2430
    },
    {
      "epoch": 6.526174496644296,
      "grad_norm": 0.08231721818447113,
      "learning_rate": 0.00010395161290322579,
      "loss": 1.4129,
      "step": 2431
    },
    {
      "epoch": 6.528859060402684,
      "grad_norm": 0.09727238863706589,
      "learning_rate": 0.00010387096774193548,
      "loss": 1.4442,
      "step": 2432
    },
    {
      "epoch": 6.531543624161074,
      "grad_norm": 0.10190726071596146,
      "learning_rate": 0.00010379032258064515,
      "loss": 1.4201,
      "step": 2433
    },
    {
      "epoch": 6.534228187919463,
      "grad_norm": 0.08561918884515762,
      "learning_rate": 0.00010370967741935483,
      "loss": 1.5053,
      "step": 2434
    },
    {
      "epoch": 6.5369127516778525,
      "grad_norm": 0.08002913743257523,
      "learning_rate": 0.00010362903225806451,
      "loss": 1.5988,
      "step": 2435
    },
    {
      "epoch": 6.539597315436241,
      "grad_norm": 0.0928778126835823,
      "learning_rate": 0.00010354838709677418,
      "loss": 1.4366,
      "step": 2436
    },
    {
      "epoch": 6.542281879194631,
      "grad_norm": 0.08955472707748413,
      "learning_rate": 0.00010346774193548387,
      "loss": 1.453,
      "step": 2437
    },
    {
      "epoch": 6.5449664429530205,
      "grad_norm": 0.09741714596748352,
      "learning_rate": 0.00010338709677419354,
      "loss": 1.5189,
      "step": 2438
    },
    {
      "epoch": 6.547651006711409,
      "grad_norm": 0.10097839683294296,
      "learning_rate": 0.00010330645161290322,
      "loss": 1.5006,
      "step": 2439
    },
    {
      "epoch": 6.550335570469799,
      "grad_norm": 0.09145156294107437,
      "learning_rate": 0.0001032258064516129,
      "loss": 1.5241,
      "step": 2440
    },
    {
      "epoch": 6.553020134228188,
      "grad_norm": 0.08844567090272903,
      "learning_rate": 0.00010314516129032257,
      "loss": 1.5702,
      "step": 2441
    },
    {
      "epoch": 6.555704697986577,
      "grad_norm": 0.08653333783149719,
      "learning_rate": 0.00010306451612903226,
      "loss": 1.4411,
      "step": 2442
    },
    {
      "epoch": 6.558389261744966,
      "grad_norm": 0.08976610749959946,
      "learning_rate": 0.00010298387096774192,
      "loss": 1.3818,
      "step": 2443
    },
    {
      "epoch": 6.561073825503356,
      "grad_norm": 0.0903453677892685,
      "learning_rate": 0.00010290322580645161,
      "loss": 1.3684,
      "step": 2444
    },
    {
      "epoch": 6.563758389261745,
      "grad_norm": 0.09410523623228073,
      "learning_rate": 0.00010282258064516128,
      "loss": 1.378,
      "step": 2445
    },
    {
      "epoch": 6.566442953020134,
      "grad_norm": 0.09190062433481216,
      "learning_rate": 0.00010274193548387096,
      "loss": 1.4406,
      "step": 2446
    },
    {
      "epoch": 6.569127516778524,
      "grad_norm": 0.08491325378417969,
      "learning_rate": 0.00010266129032258064,
      "loss": 1.5132,
      "step": 2447
    },
    {
      "epoch": 6.571812080536913,
      "grad_norm": 0.10243190079927444,
      "learning_rate": 0.00010258064516129031,
      "loss": 1.4177,
      "step": 2448
    },
    {
      "epoch": 6.574496644295302,
      "grad_norm": 0.08657587319612503,
      "learning_rate": 0.0001025,
      "loss": 1.5278,
      "step": 2449
    },
    {
      "epoch": 6.577181208053691,
      "grad_norm": 0.08936058729887009,
      "learning_rate": 0.00010241935483870965,
      "loss": 1.4556,
      "step": 2450
    },
    {
      "epoch": 6.579865771812081,
      "grad_norm": 0.08066213876008987,
      "learning_rate": 0.00010233870967741935,
      "loss": 1.3746,
      "step": 2451
    },
    {
      "epoch": 6.5825503355704695,
      "grad_norm": 0.0870051458477974,
      "learning_rate": 0.00010225806451612903,
      "loss": 1.4902,
      "step": 2452
    },
    {
      "epoch": 6.585234899328859,
      "grad_norm": 0.09442245960235596,
      "learning_rate": 0.00010217741935483869,
      "loss": 1.4336,
      "step": 2453
    },
    {
      "epoch": 6.587919463087248,
      "grad_norm": 0.10572847723960876,
      "learning_rate": 0.00010209677419354839,
      "loss": 1.5746,
      "step": 2454
    },
    {
      "epoch": 6.590604026845638,
      "grad_norm": 0.08483689278364182,
      "learning_rate": 0.00010201612903225804,
      "loss": 1.4385,
      "step": 2455
    },
    {
      "epoch": 6.593288590604027,
      "grad_norm": 0.0928456112742424,
      "learning_rate": 0.00010193548387096773,
      "loss": 1.4802,
      "step": 2456
    },
    {
      "epoch": 6.595973154362416,
      "grad_norm": 0.09369926154613495,
      "learning_rate": 0.00010185483870967741,
      "loss": 1.4399,
      "step": 2457
    },
    {
      "epoch": 6.598657718120806,
      "grad_norm": 0.09186606854200363,
      "learning_rate": 0.00010177419354838708,
      "loss": 1.4895,
      "step": 2458
    },
    {
      "epoch": 6.6013422818791945,
      "grad_norm": 0.0784878209233284,
      "learning_rate": 0.00010169354838709676,
      "loss": 1.5556,
      "step": 2459
    },
    {
      "epoch": 6.604026845637584,
      "grad_norm": 0.09519454836845398,
      "learning_rate": 0.00010161290322580643,
      "loss": 1.5275,
      "step": 2460
    },
    {
      "epoch": 6.606711409395973,
      "grad_norm": 0.09238642454147339,
      "learning_rate": 0.00010153225806451612,
      "loss": 1.493,
      "step": 2461
    },
    {
      "epoch": 6.609395973154363,
      "grad_norm": 0.08182089030742645,
      "learning_rate": 0.0001014516129032258,
      "loss": 1.5391,
      "step": 2462
    },
    {
      "epoch": 6.612080536912751,
      "grad_norm": 0.09170491993427277,
      "learning_rate": 0.00010137096774193547,
      "loss": 1.3327,
      "step": 2463
    },
    {
      "epoch": 6.614765100671141,
      "grad_norm": 0.09424954652786255,
      "learning_rate": 0.00010129032258064515,
      "loss": 1.3954,
      "step": 2464
    },
    {
      "epoch": 6.617449664429531,
      "grad_norm": 0.09563647210597992,
      "learning_rate": 0.00010120967741935482,
      "loss": 1.3813,
      "step": 2465
    },
    {
      "epoch": 6.620134228187919,
      "grad_norm": 0.0973028764128685,
      "learning_rate": 0.0001011290322580645,
      "loss": 1.4865,
      "step": 2466
    },
    {
      "epoch": 6.622818791946309,
      "grad_norm": 0.10025761276483536,
      "learning_rate": 0.00010104838709677419,
      "loss": 1.4429,
      "step": 2467
    },
    {
      "epoch": 6.625503355704698,
      "grad_norm": 0.0900910347700119,
      "learning_rate": 0.00010096774193548386,
      "loss": 1.3501,
      "step": 2468
    },
    {
      "epoch": 6.6281879194630875,
      "grad_norm": 0.08521164208650589,
      "learning_rate": 0.00010088709677419354,
      "loss": 1.4809,
      "step": 2469
    },
    {
      "epoch": 6.630872483221476,
      "grad_norm": 0.09338055551052094,
      "learning_rate": 0.00010080645161290321,
      "loss": 1.239,
      "step": 2470
    },
    {
      "epoch": 6.633557046979866,
      "grad_norm": 0.10051469504833221,
      "learning_rate": 0.0001007258064516129,
      "loss": 1.4707,
      "step": 2471
    },
    {
      "epoch": 6.636241610738255,
      "grad_norm": 0.11051726341247559,
      "learning_rate": 0.00010064516129032258,
      "loss": 1.4114,
      "step": 2472
    },
    {
      "epoch": 6.638926174496644,
      "grad_norm": 0.09554821252822876,
      "learning_rate": 0.00010056451612903225,
      "loss": 1.3622,
      "step": 2473
    },
    {
      "epoch": 6.641610738255034,
      "grad_norm": 0.08930515497922897,
      "learning_rate": 0.00010048387096774193,
      "loss": 1.3746,
      "step": 2474
    },
    {
      "epoch": 6.644295302013423,
      "grad_norm": 0.0959499329328537,
      "learning_rate": 0.0001004032258064516,
      "loss": 1.4063,
      "step": 2475
    },
    {
      "epoch": 6.646979865771812,
      "grad_norm": 0.09487736970186234,
      "learning_rate": 0.00010032258064516129,
      "loss": 1.3318,
      "step": 2476
    },
    {
      "epoch": 6.649664429530201,
      "grad_norm": 0.09117250144481659,
      "learning_rate": 0.00010024193548387097,
      "loss": 1.5423,
      "step": 2477
    },
    {
      "epoch": 6.652348993288591,
      "grad_norm": 0.0897795781493187,
      "learning_rate": 0.00010016129032258064,
      "loss": 1.4037,
      "step": 2478
    },
    {
      "epoch": 6.65503355704698,
      "grad_norm": 0.0896105244755745,
      "learning_rate": 0.00010008064516129032,
      "loss": 1.3942,
      "step": 2479
    },
    {
      "epoch": 6.657718120805369,
      "grad_norm": 0.08632194250822067,
      "learning_rate": 9.999999999999999e-05,
      "loss": 1.4441,
      "step": 2480
    },
    {
      "epoch": 6.660402684563758,
      "grad_norm": 0.09293805807828903,
      "learning_rate": 9.991935483870967e-05,
      "loss": 1.5214,
      "step": 2481
    },
    {
      "epoch": 6.663087248322148,
      "grad_norm": 0.0903671532869339,
      "learning_rate": 9.983870967741936e-05,
      "loss": 1.4448,
      "step": 2482
    },
    {
      "epoch": 6.665771812080537,
      "grad_norm": 0.0828728899359703,
      "learning_rate": 9.975806451612903e-05,
      "loss": 1.5175,
      "step": 2483
    },
    {
      "epoch": 6.668456375838926,
      "grad_norm": 0.08541280776262283,
      "learning_rate": 9.967741935483871e-05,
      "loss": 1.5925,
      "step": 2484
    },
    {
      "epoch": 6.671140939597316,
      "grad_norm": 0.09291210025548935,
      "learning_rate": 9.959677419354837e-05,
      "loss": 1.37,
      "step": 2485
    },
    {
      "epoch": 6.673825503355705,
      "grad_norm": 0.08433398604393005,
      "learning_rate": 9.951612903225806e-05,
      "loss": 1.4988,
      "step": 2486
    },
    {
      "epoch": 6.676510067114094,
      "grad_norm": 0.10160886496305466,
      "learning_rate": 9.943548387096775e-05,
      "loss": 1.4364,
      "step": 2487
    },
    {
      "epoch": 6.679194630872483,
      "grad_norm": 0.10286807268857956,
      "learning_rate": 9.93548387096774e-05,
      "loss": 1.5121,
      "step": 2488
    },
    {
      "epoch": 6.681879194630873,
      "grad_norm": 0.09355854988098145,
      "learning_rate": 9.927419354838709e-05,
      "loss": 1.3986,
      "step": 2489
    },
    {
      "epoch": 6.684563758389261,
      "grad_norm": 0.08658188581466675,
      "learning_rate": 9.919354838709676e-05,
      "loss": 1.5028,
      "step": 2490
    },
    {
      "epoch": 6.687248322147651,
      "grad_norm": 0.08713621646165848,
      "learning_rate": 9.911290322580644e-05,
      "loss": 1.5106,
      "step": 2491
    },
    {
      "epoch": 6.689932885906041,
      "grad_norm": 0.0926244929432869,
      "learning_rate": 9.903225806451611e-05,
      "loss": 1.3282,
      "step": 2492
    },
    {
      "epoch": 6.6926174496644295,
      "grad_norm": 0.09751683473587036,
      "learning_rate": 9.895161290322579e-05,
      "loss": 1.388,
      "step": 2493
    },
    {
      "epoch": 6.695302013422819,
      "grad_norm": 0.10543455183506012,
      "learning_rate": 9.887096774193548e-05,
      "loss": 1.5319,
      "step": 2494
    },
    {
      "epoch": 6.697986577181208,
      "grad_norm": 0.09666363149881363,
      "learning_rate": 9.879032258064515e-05,
      "loss": 1.4428,
      "step": 2495
    },
    {
      "epoch": 6.700671140939598,
      "grad_norm": 0.09156663715839386,
      "learning_rate": 9.870967741935483e-05,
      "loss": 1.3762,
      "step": 2496
    },
    {
      "epoch": 6.703355704697986,
      "grad_norm": 0.09358736872673035,
      "learning_rate": 9.86290322580645e-05,
      "loss": 1.43,
      "step": 2497
    },
    {
      "epoch": 6.706040268456376,
      "grad_norm": 0.08460109680891037,
      "learning_rate": 9.854838709677418e-05,
      "loss": 1.4698,
      "step": 2498
    },
    {
      "epoch": 6.708724832214765,
      "grad_norm": 0.10009565949440002,
      "learning_rate": 9.846774193548387e-05,
      "loss": 1.402,
      "step": 2499
    },
    {
      "epoch": 6.7114093959731544,
      "grad_norm": 0.0815369263291359,
      "learning_rate": 9.838709677419354e-05,
      "loss": 1.4979,
      "step": 2500
    },
    {
      "epoch": 6.714093959731544,
      "grad_norm": 0.10269387066364288,
      "learning_rate": 9.830645161290322e-05,
      "loss": 1.3536,
      "step": 2501
    },
    {
      "epoch": 6.716778523489933,
      "grad_norm": 0.09119123220443726,
      "learning_rate": 9.822580645161289e-05,
      "loss": 1.3684,
      "step": 2502
    },
    {
      "epoch": 6.7194630872483225,
      "grad_norm": 0.09220793098211288,
      "learning_rate": 9.814516129032257e-05,
      "loss": 1.4889,
      "step": 2503
    },
    {
      "epoch": 6.722147651006711,
      "grad_norm": 0.09406924247741699,
      "learning_rate": 9.806451612903226e-05,
      "loss": 1.5298,
      "step": 2504
    },
    {
      "epoch": 6.724832214765101,
      "grad_norm": 0.09556356072425842,
      "learning_rate": 9.798387096774193e-05,
      "loss": 1.3705,
      "step": 2505
    },
    {
      "epoch": 6.72751677852349,
      "grad_norm": 0.09266401827335358,
      "learning_rate": 9.790322580645161e-05,
      "loss": 1.3897,
      "step": 2506
    },
    {
      "epoch": 6.730201342281879,
      "grad_norm": 0.0898907482624054,
      "learning_rate": 9.782258064516128e-05,
      "loss": 1.5961,
      "step": 2507
    },
    {
      "epoch": 6.732885906040268,
      "grad_norm": 0.09349134564399719,
      "learning_rate": 9.774193548387096e-05,
      "loss": 1.4536,
      "step": 2508
    },
    {
      "epoch": 6.735570469798658,
      "grad_norm": 0.0887693390250206,
      "learning_rate": 9.766129032258065e-05,
      "loss": 1.4848,
      "step": 2509
    },
    {
      "epoch": 6.7382550335570475,
      "grad_norm": 0.0894610732793808,
      "learning_rate": 9.758064516129031e-05,
      "loss": 1.3692,
      "step": 2510
    },
    {
      "epoch": 6.740939597315436,
      "grad_norm": 0.09298869967460632,
      "learning_rate": 9.75e-05,
      "loss": 1.5533,
      "step": 2511
    },
    {
      "epoch": 6.743624161073825,
      "grad_norm": 0.09954467415809631,
      "learning_rate": 9.741935483870967e-05,
      "loss": 1.5324,
      "step": 2512
    },
    {
      "epoch": 6.746308724832215,
      "grad_norm": 0.08466647565364838,
      "learning_rate": 9.733870967741935e-05,
      "loss": 1.42,
      "step": 2513
    },
    {
      "epoch": 6.748993288590604,
      "grad_norm": 0.08930358290672302,
      "learning_rate": 9.725806451612903e-05,
      "loss": 1.4502,
      "step": 2514
    },
    {
      "epoch": 6.751677852348993,
      "grad_norm": 0.09637404978275299,
      "learning_rate": 9.71774193548387e-05,
      "loss": 1.5357,
      "step": 2515
    },
    {
      "epoch": 6.754362416107383,
      "grad_norm": 0.08715421706438065,
      "learning_rate": 9.709677419354839e-05,
      "loss": 1.5846,
      "step": 2516
    },
    {
      "epoch": 6.7570469798657715,
      "grad_norm": 0.08933984488248825,
      "learning_rate": 9.701612903225804e-05,
      "loss": 1.4382,
      "step": 2517
    },
    {
      "epoch": 6.759731543624161,
      "grad_norm": 0.08896736055612564,
      "learning_rate": 9.693548387096773e-05,
      "loss": 1.5574,
      "step": 2518
    },
    {
      "epoch": 6.762416107382551,
      "grad_norm": 0.08096319437026978,
      "learning_rate": 9.685483870967742e-05,
      "loss": 1.4282,
      "step": 2519
    },
    {
      "epoch": 6.76510067114094,
      "grad_norm": 0.09883437305688858,
      "learning_rate": 9.677419354838708e-05,
      "loss": 1.4873,
      "step": 2520
    },
    {
      "epoch": 6.767785234899328,
      "grad_norm": 0.10147366672754288,
      "learning_rate": 9.669354838709676e-05,
      "loss": 1.3684,
      "step": 2521
    },
    {
      "epoch": 6.770469798657718,
      "grad_norm": 0.10445471853017807,
      "learning_rate": 9.661290322580643e-05,
      "loss": 1.3645,
      "step": 2522
    },
    {
      "epoch": 6.773154362416108,
      "grad_norm": 0.08916822820901871,
      "learning_rate": 9.653225806451612e-05,
      "loss": 1.3983,
      "step": 2523
    },
    {
      "epoch": 6.7758389261744965,
      "grad_norm": 0.09413491189479828,
      "learning_rate": 9.64516129032258e-05,
      "loss": 1.5302,
      "step": 2524
    },
    {
      "epoch": 6.778523489932886,
      "grad_norm": 0.09737715125083923,
      "learning_rate": 9.637096774193547e-05,
      "loss": 1.4453,
      "step": 2525
    },
    {
      "epoch": 6.781208053691275,
      "grad_norm": 0.10275339335203171,
      "learning_rate": 9.629032258064515e-05,
      "loss": 1.3375,
      "step": 2526
    },
    {
      "epoch": 6.7838926174496645,
      "grad_norm": 0.09300433844327927,
      "learning_rate": 9.620967741935482e-05,
      "loss": 1.4795,
      "step": 2527
    },
    {
      "epoch": 6.786577181208053,
      "grad_norm": 0.11041995137929916,
      "learning_rate": 9.61290322580645e-05,
      "loss": 1.3658,
      "step": 2528
    },
    {
      "epoch": 6.789261744966443,
      "grad_norm": 0.10170435905456543,
      "learning_rate": 9.604838709677419e-05,
      "loss": 1.4208,
      "step": 2529
    },
    {
      "epoch": 6.791946308724832,
      "grad_norm": 0.08558696508407593,
      "learning_rate": 9.596774193548386e-05,
      "loss": 1.5755,
      "step": 2530
    },
    {
      "epoch": 6.794630872483221,
      "grad_norm": 0.08635375648736954,
      "learning_rate": 9.588709677419354e-05,
      "loss": 1.4956,
      "step": 2531
    },
    {
      "epoch": 6.797315436241611,
      "grad_norm": 0.09197816997766495,
      "learning_rate": 9.580645161290321e-05,
      "loss": 1.4676,
      "step": 2532
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.09151696413755417,
      "learning_rate": 9.57258064516129e-05,
      "loss": 1.5822,
      "step": 2533
    },
    {
      "epoch": 6.8026845637583895,
      "grad_norm": 0.08890754729509354,
      "learning_rate": 9.564516129032258e-05,
      "loss": 1.4131,
      "step": 2534
    },
    {
      "epoch": 6.805369127516778,
      "grad_norm": 0.09954468160867691,
      "learning_rate": 9.556451612903225e-05,
      "loss": 1.3914,
      "step": 2535
    },
    {
      "epoch": 6.808053691275168,
      "grad_norm": 0.08935624361038208,
      "learning_rate": 9.548387096774193e-05,
      "loss": 1.5114,
      "step": 2536
    },
    {
      "epoch": 6.810738255033557,
      "grad_norm": 0.09253334999084473,
      "learning_rate": 9.54032258064516e-05,
      "loss": 1.3879,
      "step": 2537
    },
    {
      "epoch": 6.813422818791946,
      "grad_norm": 0.08459025621414185,
      "learning_rate": 9.532258064516129e-05,
      "loss": 1.473,
      "step": 2538
    },
    {
      "epoch": 6.816107382550335,
      "grad_norm": 0.08998715132474899,
      "learning_rate": 9.524193548387096e-05,
      "loss": 1.4924,
      "step": 2539
    },
    {
      "epoch": 6.818791946308725,
      "grad_norm": 0.08484320342540741,
      "learning_rate": 9.516129032258064e-05,
      "loss": 1.4874,
      "step": 2540
    },
    {
      "epoch": 6.821476510067114,
      "grad_norm": 0.08730505406856537,
      "learning_rate": 9.508064516129032e-05,
      "loss": 1.4303,
      "step": 2541
    },
    {
      "epoch": 6.824161073825503,
      "grad_norm": 0.09363730251789093,
      "learning_rate": 9.499999999999999e-05,
      "loss": 1.4398,
      "step": 2542
    },
    {
      "epoch": 6.826845637583893,
      "grad_norm": 0.0992254987359047,
      "learning_rate": 9.491935483870968e-05,
      "loss": 1.3434,
      "step": 2543
    },
    {
      "epoch": 6.829530201342282,
      "grad_norm": 0.10112675279378891,
      "learning_rate": 9.483870967741934e-05,
      "loss": 1.4168,
      "step": 2544
    },
    {
      "epoch": 6.832214765100671,
      "grad_norm": 0.10335297137498856,
      "learning_rate": 9.475806451612903e-05,
      "loss": 1.4186,
      "step": 2545
    },
    {
      "epoch": 6.83489932885906,
      "grad_norm": 0.11014170944690704,
      "learning_rate": 9.467741935483871e-05,
      "loss": 1.3517,
      "step": 2546
    },
    {
      "epoch": 6.83758389261745,
      "grad_norm": 0.09301621466875076,
      "learning_rate": 9.459677419354838e-05,
      "loss": 1.3963,
      "step": 2547
    },
    {
      "epoch": 6.8402684563758385,
      "grad_norm": 0.08074811100959778,
      "learning_rate": 9.451612903225806e-05,
      "loss": 1.4665,
      "step": 2548
    },
    {
      "epoch": 6.842953020134228,
      "grad_norm": 0.09252561628818512,
      "learning_rate": 9.443548387096772e-05,
      "loss": 1.5068,
      "step": 2549
    },
    {
      "epoch": 6.845637583892618,
      "grad_norm": 0.09173763543367386,
      "learning_rate": 9.43548387096774e-05,
      "loss": 1.5985,
      "step": 2550
    },
    {
      "epoch": 6.8483221476510066,
      "grad_norm": 0.08291210234165192,
      "learning_rate": 9.42741935483871e-05,
      "loss": 1.485,
      "step": 2551
    },
    {
      "epoch": 6.851006711409396,
      "grad_norm": 0.09091696888208389,
      "learning_rate": 9.419354838709676e-05,
      "loss": 1.4939,
      "step": 2552
    },
    {
      "epoch": 6.853691275167785,
      "grad_norm": 0.08658172935247421,
      "learning_rate": 9.411290322580644e-05,
      "loss": 1.4432,
      "step": 2553
    },
    {
      "epoch": 6.856375838926175,
      "grad_norm": 0.10204198956489563,
      "learning_rate": 9.403225806451611e-05,
      "loss": 1.4918,
      "step": 2554
    },
    {
      "epoch": 6.859060402684563,
      "grad_norm": 0.09753213822841644,
      "learning_rate": 9.39516129032258e-05,
      "loss": 1.5832,
      "step": 2555
    },
    {
      "epoch": 6.861744966442953,
      "grad_norm": 0.10602450370788574,
      "learning_rate": 9.387096774193548e-05,
      "loss": 1.3803,
      "step": 2556
    },
    {
      "epoch": 6.864429530201342,
      "grad_norm": 0.08528828620910645,
      "learning_rate": 9.379032258064515e-05,
      "loss": 1.3795,
      "step": 2557
    },
    {
      "epoch": 6.8671140939597315,
      "grad_norm": 0.09069425612688065,
      "learning_rate": 9.370967741935483e-05,
      "loss": 1.412,
      "step": 2558
    },
    {
      "epoch": 6.869798657718121,
      "grad_norm": 0.08495784550905228,
      "learning_rate": 9.36290322580645e-05,
      "loss": 1.474,
      "step": 2559
    },
    {
      "epoch": 6.87248322147651,
      "grad_norm": 0.0930655375123024,
      "learning_rate": 9.354838709677418e-05,
      "loss": 1.432,
      "step": 2560
    },
    {
      "epoch": 6.8751677852349,
      "grad_norm": 0.09742811322212219,
      "learning_rate": 9.346774193548387e-05,
      "loss": 1.3044,
      "step": 2561
    },
    {
      "epoch": 6.877852348993288,
      "grad_norm": 0.09243598580360413,
      "learning_rate": 9.338709677419354e-05,
      "loss": 1.5119,
      "step": 2562
    },
    {
      "epoch": 6.880536912751678,
      "grad_norm": 0.08775471150875092,
      "learning_rate": 9.330645161290322e-05,
      "loss": 1.6346,
      "step": 2563
    },
    {
      "epoch": 6.883221476510067,
      "grad_norm": 0.1035836786031723,
      "learning_rate": 9.322580645161289e-05,
      "loss": 1.4865,
      "step": 2564
    },
    {
      "epoch": 6.885906040268456,
      "grad_norm": 0.0895213782787323,
      "learning_rate": 9.314516129032257e-05,
      "loss": 1.5542,
      "step": 2565
    },
    {
      "epoch": 6.888590604026845,
      "grad_norm": 0.10211481899023056,
      "learning_rate": 9.306451612903226e-05,
      "loss": 1.4281,
      "step": 2566
    },
    {
      "epoch": 6.891275167785235,
      "grad_norm": 0.0910252258181572,
      "learning_rate": 9.298387096774193e-05,
      "loss": 1.5522,
      "step": 2567
    },
    {
      "epoch": 6.8939597315436245,
      "grad_norm": 0.09907501190900803,
      "learning_rate": 9.290322580645161e-05,
      "loss": 1.4473,
      "step": 2568
    },
    {
      "epoch": 6.896644295302013,
      "grad_norm": 0.09699025005102158,
      "learning_rate": 9.282258064516128e-05,
      "loss": 1.4844,
      "step": 2569
    },
    {
      "epoch": 6.899328859060403,
      "grad_norm": 0.0973035916686058,
      "learning_rate": 9.274193548387096e-05,
      "loss": 1.4052,
      "step": 2570
    },
    {
      "epoch": 6.902013422818792,
      "grad_norm": 0.09742197394371033,
      "learning_rate": 9.266129032258065e-05,
      "loss": 1.512,
      "step": 2571
    },
    {
      "epoch": 6.904697986577181,
      "grad_norm": 0.08753080666065216,
      "learning_rate": 9.258064516129032e-05,
      "loss": 1.4718,
      "step": 2572
    },
    {
      "epoch": 6.90738255033557,
      "grad_norm": 0.08813624083995819,
      "learning_rate": 9.25e-05,
      "loss": 1.4902,
      "step": 2573
    },
    {
      "epoch": 6.91006711409396,
      "grad_norm": 0.09799724817276001,
      "learning_rate": 9.241935483870967e-05,
      "loss": 1.4591,
      "step": 2574
    },
    {
      "epoch": 6.912751677852349,
      "grad_norm": 0.08421643078327179,
      "learning_rate": 9.233870967741935e-05,
      "loss": 1.6123,
      "step": 2575
    },
    {
      "epoch": 6.915436241610738,
      "grad_norm": 0.09761703759431839,
      "learning_rate": 9.225806451612904e-05,
      "loss": 1.4085,
      "step": 2576
    },
    {
      "epoch": 6.918120805369128,
      "grad_norm": 0.09259634464979172,
      "learning_rate": 9.21774193548387e-05,
      "loss": 1.4603,
      "step": 2577
    },
    {
      "epoch": 6.920805369127517,
      "grad_norm": 0.09247851371765137,
      "learning_rate": 9.209677419354839e-05,
      "loss": 1.5757,
      "step": 2578
    },
    {
      "epoch": 6.923489932885906,
      "grad_norm": 0.08772578835487366,
      "learning_rate": 9.201612903225806e-05,
      "loss": 1.4208,
      "step": 2579
    },
    {
      "epoch": 6.926174496644295,
      "grad_norm": 0.0912320464849472,
      "learning_rate": 9.193548387096774e-05,
      "loss": 1.5183,
      "step": 2580
    },
    {
      "epoch": 6.928859060402685,
      "grad_norm": 0.10219355672597885,
      "learning_rate": 9.18548387096774e-05,
      "loss": 1.2355,
      "step": 2581
    },
    {
      "epoch": 6.9315436241610735,
      "grad_norm": 0.09269146621227264,
      "learning_rate": 9.177419354838708e-05,
      "loss": 1.3677,
      "step": 2582
    },
    {
      "epoch": 6.934228187919463,
      "grad_norm": 0.10109271854162216,
      "learning_rate": 9.169354838709678e-05,
      "loss": 1.4836,
      "step": 2583
    },
    {
      "epoch": 6.936912751677852,
      "grad_norm": 0.09679077565670013,
      "learning_rate": 9.161290322580643e-05,
      "loss": 1.4706,
      "step": 2584
    },
    {
      "epoch": 6.939597315436242,
      "grad_norm": 0.09050222486257553,
      "learning_rate": 9.153225806451612e-05,
      "loss": 1.4431,
      "step": 2585
    },
    {
      "epoch": 6.942281879194631,
      "grad_norm": 0.0906674861907959,
      "learning_rate": 9.145161290322579e-05,
      "loss": 1.5444,
      "step": 2586
    },
    {
      "epoch": 6.94496644295302,
      "grad_norm": 0.0906202420592308,
      "learning_rate": 9.137096774193547e-05,
      "loss": 1.4046,
      "step": 2587
    },
    {
      "epoch": 6.94765100671141,
      "grad_norm": 0.08530732244253159,
      "learning_rate": 9.129032258064515e-05,
      "loss": 1.5781,
      "step": 2588
    },
    {
      "epoch": 6.950335570469798,
      "grad_norm": 0.08602871745824814,
      "learning_rate": 9.120967741935482e-05,
      "loss": 1.5018,
      "step": 2589
    },
    {
      "epoch": 6.953020134228188,
      "grad_norm": 0.08906418085098267,
      "learning_rate": 9.112903225806451e-05,
      "loss": 1.4349,
      "step": 2590
    },
    {
      "epoch": 6.955704697986577,
      "grad_norm": 0.09412378817796707,
      "learning_rate": 9.104838709677418e-05,
      "loss": 1.3608,
      "step": 2591
    },
    {
      "epoch": 6.9583892617449665,
      "grad_norm": 0.08491086214780807,
      "learning_rate": 9.096774193548386e-05,
      "loss": 1.4664,
      "step": 2592
    },
    {
      "epoch": 6.961073825503355,
      "grad_norm": 0.0885990634560585,
      "learning_rate": 9.088709677419354e-05,
      "loss": 1.5637,
      "step": 2593
    },
    {
      "epoch": 6.963758389261745,
      "grad_norm": 0.0887046754360199,
      "learning_rate": 9.080645161290321e-05,
      "loss": 1.497,
      "step": 2594
    },
    {
      "epoch": 6.966442953020135,
      "grad_norm": 0.09396754205226898,
      "learning_rate": 9.07258064516129e-05,
      "loss": 1.4231,
      "step": 2595
    },
    {
      "epoch": 6.969127516778523,
      "grad_norm": 0.09094162285327911,
      "learning_rate": 9.064516129032257e-05,
      "loss": 1.6107,
      "step": 2596
    },
    {
      "epoch": 6.971812080536913,
      "grad_norm": 0.09374909847974777,
      "learning_rate": 9.056451612903225e-05,
      "loss": 1.4911,
      "step": 2597
    },
    {
      "epoch": 6.974496644295302,
      "grad_norm": 0.08904681354761124,
      "learning_rate": 9.048387096774193e-05,
      "loss": 1.3953,
      "step": 2598
    },
    {
      "epoch": 6.9771812080536915,
      "grad_norm": 0.08915990591049194,
      "learning_rate": 9.04032258064516e-05,
      "loss": 1.4387,
      "step": 2599
    },
    {
      "epoch": 6.97986577181208,
      "grad_norm": 0.0947628915309906,
      "learning_rate": 9.032258064516129e-05,
      "loss": 1.4256,
      "step": 2600
    },
    {
      "epoch": 6.98255033557047,
      "grad_norm": 0.08357610553503036,
      "learning_rate": 9.024193548387096e-05,
      "loss": 1.5596,
      "step": 2601
    },
    {
      "epoch": 6.985234899328859,
      "grad_norm": 0.08984001725912094,
      "learning_rate": 9.016129032258064e-05,
      "loss": 1.425,
      "step": 2602
    },
    {
      "epoch": 6.987919463087248,
      "grad_norm": 0.1158517375588417,
      "learning_rate": 9.008064516129032e-05,
      "loss": 1.4222,
      "step": 2603
    },
    {
      "epoch": 6.990604026845638,
      "grad_norm": 0.08662548661231995,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.421,
      "step": 2604
    },
    {
      "epoch": 6.993288590604027,
      "grad_norm": 0.1138155609369278,
      "learning_rate": 8.991935483870968e-05,
      "loss": 1.3557,
      "step": 2605
    },
    {
      "epoch": 6.995973154362416,
      "grad_norm": 0.10354345291852951,
      "learning_rate": 8.983870967741935e-05,
      "loss": 1.3515,
      "step": 2606
    },
    {
      "epoch": 6.998657718120805,
      "grad_norm": 0.10462880879640579,
      "learning_rate": 8.975806451612903e-05,
      "loss": 1.3641,
      "step": 2607
    },
    {
      "epoch": 7.001342281879195,
      "grad_norm": 0.11511147767305374,
      "learning_rate": 8.967741935483871e-05,
      "loss": 1.4714,
      "step": 2608
    },
    {
      "epoch": 7.004026845637584,
      "grad_norm": 0.09079177677631378,
      "learning_rate": 8.959677419354838e-05,
      "loss": 1.5327,
      "step": 2609
    },
    {
      "epoch": 7.006711409395973,
      "grad_norm": 0.09156239032745361,
      "learning_rate": 8.951612903225806e-05,
      "loss": 1.5269,
      "step": 2610
    },
    {
      "epoch": 7.009395973154362,
      "grad_norm": 0.10480818152427673,
      "learning_rate": 8.943548387096772e-05,
      "loss": 1.3035,
      "step": 2611
    },
    {
      "epoch": 7.012080536912752,
      "grad_norm": 0.10157176107168198,
      "learning_rate": 8.935483870967742e-05,
      "loss": 1.4719,
      "step": 2612
    },
    {
      "epoch": 7.014765100671141,
      "grad_norm": 0.0978950783610344,
      "learning_rate": 8.92741935483871e-05,
      "loss": 1.6061,
      "step": 2613
    },
    {
      "epoch": 7.01744966442953,
      "grad_norm": 0.09266845881938934,
      "learning_rate": 8.919354838709676e-05,
      "loss": 1.4952,
      "step": 2614
    },
    {
      "epoch": 7.02013422818792,
      "grad_norm": 0.09513897448778152,
      "learning_rate": 8.911290322580645e-05,
      "loss": 1.36,
      "step": 2615
    },
    {
      "epoch": 7.0228187919463085,
      "grad_norm": 0.08869130909442902,
      "learning_rate": 8.903225806451611e-05,
      "loss": 1.467,
      "step": 2616
    },
    {
      "epoch": 7.025503355704698,
      "grad_norm": 0.10167296230792999,
      "learning_rate": 8.89516129032258e-05,
      "loss": 1.3336,
      "step": 2617
    },
    {
      "epoch": 7.028187919463087,
      "grad_norm": 0.10618525743484497,
      "learning_rate": 8.887096774193548e-05,
      "loss": 1.3556,
      "step": 2618
    },
    {
      "epoch": 7.030872483221477,
      "grad_norm": 0.08527404069900513,
      "learning_rate": 8.879032258064515e-05,
      "loss": 1.4711,
      "step": 2619
    },
    {
      "epoch": 7.033557046979865,
      "grad_norm": 0.08569677174091339,
      "learning_rate": 8.870967741935483e-05,
      "loss": 1.3825,
      "step": 2620
    },
    {
      "epoch": 7.036241610738255,
      "grad_norm": 0.09856416285037994,
      "learning_rate": 8.86290322580645e-05,
      "loss": 1.385,
      "step": 2621
    },
    {
      "epoch": 7.038926174496645,
      "grad_norm": 0.10299404710531235,
      "learning_rate": 8.854838709677418e-05,
      "loss": 1.46,
      "step": 2622
    },
    {
      "epoch": 7.0416107382550335,
      "grad_norm": 0.09019820392131805,
      "learning_rate": 8.846774193548387e-05,
      "loss": 1.5749,
      "step": 2623
    },
    {
      "epoch": 7.044295302013423,
      "grad_norm": 0.09192164242267609,
      "learning_rate": 8.838709677419354e-05,
      "loss": 1.4888,
      "step": 2624
    },
    {
      "epoch": 7.046979865771812,
      "grad_norm": 0.10015245527029037,
      "learning_rate": 8.830645161290322e-05,
      "loss": 1.4292,
      "step": 2625
    },
    {
      "epoch": 7.049664429530202,
      "grad_norm": 0.08883378654718399,
      "learning_rate": 8.822580645161289e-05,
      "loss": 1.4333,
      "step": 2626
    },
    {
      "epoch": 7.05234899328859,
      "grad_norm": 0.10533777624368668,
      "learning_rate": 8.814516129032257e-05,
      "loss": 1.3608,
      "step": 2627
    },
    {
      "epoch": 7.05503355704698,
      "grad_norm": 0.08981560170650482,
      "learning_rate": 8.806451612903224e-05,
      "loss": 1.6084,
      "step": 2628
    },
    {
      "epoch": 7.057718120805369,
      "grad_norm": 0.08483533561229706,
      "learning_rate": 8.798387096774193e-05,
      "loss": 1.4109,
      "step": 2629
    },
    {
      "epoch": 7.060402684563758,
      "grad_norm": 0.09948372095823288,
      "learning_rate": 8.790322580645161e-05,
      "loss": 1.4547,
      "step": 2630
    },
    {
      "epoch": 7.063087248322148,
      "grad_norm": 0.08438057452440262,
      "learning_rate": 8.782258064516128e-05,
      "loss": 1.3634,
      "step": 2631
    },
    {
      "epoch": 7.065771812080537,
      "grad_norm": 0.09567444026470184,
      "learning_rate": 8.774193548387096e-05,
      "loss": 1.414,
      "step": 2632
    },
    {
      "epoch": 7.0684563758389265,
      "grad_norm": 0.08636439591646194,
      "learning_rate": 8.766129032258063e-05,
      "loss": 1.4409,
      "step": 2633
    },
    {
      "epoch": 7.071140939597315,
      "grad_norm": 0.09632689505815506,
      "learning_rate": 8.758064516129032e-05,
      "loss": 1.4496,
      "step": 2634
    },
    {
      "epoch": 7.073825503355705,
      "grad_norm": 0.08615469187498093,
      "learning_rate": 8.75e-05,
      "loss": 1.4939,
      "step": 2635
    },
    {
      "epoch": 7.076510067114094,
      "grad_norm": 0.08305079489946365,
      "learning_rate": 8.741935483870967e-05,
      "loss": 1.5477,
      "step": 2636
    },
    {
      "epoch": 7.079194630872483,
      "grad_norm": 0.09299511462450027,
      "learning_rate": 8.733870967741935e-05,
      "loss": 1.352,
      "step": 2637
    },
    {
      "epoch": 7.081879194630872,
      "grad_norm": 0.08549260348081589,
      "learning_rate": 8.725806451612902e-05,
      "loss": 1.4632,
      "step": 2638
    },
    {
      "epoch": 7.084563758389262,
      "grad_norm": 0.08123061060905457,
      "learning_rate": 8.71774193548387e-05,
      "loss": 1.3498,
      "step": 2639
    },
    {
      "epoch": 7.087248322147651,
      "grad_norm": 0.10206399857997894,
      "learning_rate": 8.709677419354839e-05,
      "loss": 1.4025,
      "step": 2640
    },
    {
      "epoch": 7.08993288590604,
      "grad_norm": 0.09564530104398727,
      "learning_rate": 8.701612903225806e-05,
      "loss": 1.3698,
      "step": 2641
    },
    {
      "epoch": 7.09261744966443,
      "grad_norm": 0.0940585806965828,
      "learning_rate": 8.693548387096774e-05,
      "loss": 1.4582,
      "step": 2642
    },
    {
      "epoch": 7.095302013422819,
      "grad_norm": 0.09877084195613861,
      "learning_rate": 8.68548387096774e-05,
      "loss": 1.3988,
      "step": 2643
    },
    {
      "epoch": 7.097986577181208,
      "grad_norm": 0.10415581613779068,
      "learning_rate": 8.67741935483871e-05,
      "loss": 1.4387,
      "step": 2644
    },
    {
      "epoch": 7.100671140939597,
      "grad_norm": 0.09775251150131226,
      "learning_rate": 8.669354838709678e-05,
      "loss": 1.4458,
      "step": 2645
    },
    {
      "epoch": 7.103355704697987,
      "grad_norm": 0.09534837305545807,
      "learning_rate": 8.661290322580643e-05,
      "loss": 1.4263,
      "step": 2646
    },
    {
      "epoch": 7.1060402684563755,
      "grad_norm": 0.09406223893165588,
      "learning_rate": 8.653225806451613e-05,
      "loss": 1.3927,
      "step": 2647
    },
    {
      "epoch": 7.108724832214765,
      "grad_norm": 0.1008608415722847,
      "learning_rate": 8.645161290322579e-05,
      "loss": 1.4598,
      "step": 2648
    },
    {
      "epoch": 7.111409395973155,
      "grad_norm": 0.09468185901641846,
      "learning_rate": 8.637096774193547e-05,
      "loss": 1.424,
      "step": 2649
    },
    {
      "epoch": 7.114093959731544,
      "grad_norm": 0.10567124933004379,
      "learning_rate": 8.629032258064515e-05,
      "loss": 1.4816,
      "step": 2650
    },
    {
      "epoch": 7.116778523489933,
      "grad_norm": 0.09594568610191345,
      "learning_rate": 8.620967741935482e-05,
      "loss": 1.5484,
      "step": 2651
    },
    {
      "epoch": 7.119463087248322,
      "grad_norm": 0.09800863265991211,
      "learning_rate": 8.612903225806451e-05,
      "loss": 1.555,
      "step": 2652
    },
    {
      "epoch": 7.122147651006712,
      "grad_norm": 0.0913621187210083,
      "learning_rate": 8.604838709677418e-05,
      "loss": 1.4954,
      "step": 2653
    },
    {
      "epoch": 7.1248322147651,
      "grad_norm": 0.10124074667692184,
      "learning_rate": 8.596774193548386e-05,
      "loss": 1.3929,
      "step": 2654
    },
    {
      "epoch": 7.12751677852349,
      "grad_norm": 0.10137265920639038,
      "learning_rate": 8.588709677419354e-05,
      "loss": 1.4719,
      "step": 2655
    },
    {
      "epoch": 7.130201342281879,
      "grad_norm": 0.09924658387899399,
      "learning_rate": 8.580645161290321e-05,
      "loss": 1.4674,
      "step": 2656
    },
    {
      "epoch": 7.1328859060402685,
      "grad_norm": 0.08488953113555908,
      "learning_rate": 8.57258064516129e-05,
      "loss": 1.5809,
      "step": 2657
    },
    {
      "epoch": 7.135570469798658,
      "grad_norm": 0.09299412369728088,
      "learning_rate": 8.564516129032257e-05,
      "loss": 1.4976,
      "step": 2658
    },
    {
      "epoch": 7.138255033557047,
      "grad_norm": 0.08777804672718048,
      "learning_rate": 8.556451612903225e-05,
      "loss": 1.4455,
      "step": 2659
    },
    {
      "epoch": 7.140939597315437,
      "grad_norm": 0.10744272172451019,
      "learning_rate": 8.548387096774193e-05,
      "loss": 1.3423,
      "step": 2660
    },
    {
      "epoch": 7.143624161073825,
      "grad_norm": 0.09191851317882538,
      "learning_rate": 8.54032258064516e-05,
      "loss": 1.4441,
      "step": 2661
    },
    {
      "epoch": 7.146308724832215,
      "grad_norm": 0.0835876539349556,
      "learning_rate": 8.532258064516129e-05,
      "loss": 1.4846,
      "step": 2662
    },
    {
      "epoch": 7.148993288590604,
      "grad_norm": 0.0901363343000412,
      "learning_rate": 8.524193548387096e-05,
      "loss": 1.2744,
      "step": 2663
    },
    {
      "epoch": 7.1516778523489934,
      "grad_norm": 0.09017560631036758,
      "learning_rate": 8.516129032258064e-05,
      "loss": 1.3953,
      "step": 2664
    },
    {
      "epoch": 7.154362416107382,
      "grad_norm": 0.10645012557506561,
      "learning_rate": 8.508064516129032e-05,
      "loss": 1.4341,
      "step": 2665
    },
    {
      "epoch": 7.157046979865772,
      "grad_norm": 0.08885229378938675,
      "learning_rate": 8.499999999999999e-05,
      "loss": 1.4083,
      "step": 2666
    },
    {
      "epoch": 7.1597315436241615,
      "grad_norm": 0.08606793731451035,
      "learning_rate": 8.491935483870968e-05,
      "loss": 1.4622,
      "step": 2667
    },
    {
      "epoch": 7.16241610738255,
      "grad_norm": 0.081082783639431,
      "learning_rate": 8.483870967741935e-05,
      "loss": 1.5545,
      "step": 2668
    },
    {
      "epoch": 7.16510067114094,
      "grad_norm": 0.08780905604362488,
      "learning_rate": 8.475806451612903e-05,
      "loss": 1.4586,
      "step": 2669
    },
    {
      "epoch": 7.167785234899329,
      "grad_norm": 0.09615518152713776,
      "learning_rate": 8.467741935483871e-05,
      "loss": 1.5277,
      "step": 2670
    },
    {
      "epoch": 7.170469798657718,
      "grad_norm": 0.0758960172533989,
      "learning_rate": 8.459677419354838e-05,
      "loss": 1.4461,
      "step": 2671
    },
    {
      "epoch": 7.173154362416107,
      "grad_norm": 0.09560280293226242,
      "learning_rate": 8.451612903225807e-05,
      "loss": 1.4133,
      "step": 2672
    },
    {
      "epoch": 7.175838926174497,
      "grad_norm": 0.09489798545837402,
      "learning_rate": 8.443548387096774e-05,
      "loss": 1.5938,
      "step": 2673
    },
    {
      "epoch": 7.178523489932886,
      "grad_norm": 0.0950406938791275,
      "learning_rate": 8.435483870967742e-05,
      "loss": 1.4148,
      "step": 2674
    },
    {
      "epoch": 7.181208053691275,
      "grad_norm": 0.11080990731716156,
      "learning_rate": 8.427419354838707e-05,
      "loss": 1.2656,
      "step": 2675
    },
    {
      "epoch": 7.183892617449664,
      "grad_norm": 0.08878586441278458,
      "learning_rate": 8.419354838709677e-05,
      "loss": 1.4481,
      "step": 2676
    },
    {
      "epoch": 7.186577181208054,
      "grad_norm": 0.08517775684595108,
      "learning_rate": 8.411290322580645e-05,
      "loss": 1.4268,
      "step": 2677
    },
    {
      "epoch": 7.189261744966443,
      "grad_norm": 0.1012444943189621,
      "learning_rate": 8.403225806451611e-05,
      "loss": 1.5342,
      "step": 2678
    },
    {
      "epoch": 7.191946308724832,
      "grad_norm": 0.08732376247644424,
      "learning_rate": 8.39516129032258e-05,
      "loss": 1.4113,
      "step": 2679
    },
    {
      "epoch": 7.194630872483222,
      "grad_norm": 0.08086371421813965,
      "learning_rate": 8.387096774193546e-05,
      "loss": 1.459,
      "step": 2680
    },
    {
      "epoch": 7.1973154362416105,
      "grad_norm": 0.0843890830874443,
      "learning_rate": 8.379032258064515e-05,
      "loss": 1.4362,
      "step": 2681
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.09707354009151459,
      "learning_rate": 8.370967741935483e-05,
      "loss": 1.4031,
      "step": 2682
    },
    {
      "epoch": 7.202684563758389,
      "grad_norm": 0.08072426170110703,
      "learning_rate": 8.36290322580645e-05,
      "loss": 1.4894,
      "step": 2683
    },
    {
      "epoch": 7.205369127516779,
      "grad_norm": 0.0901704952120781,
      "learning_rate": 8.354838709677418e-05,
      "loss": 1.4962,
      "step": 2684
    },
    {
      "epoch": 7.208053691275167,
      "grad_norm": 0.09811800718307495,
      "learning_rate": 8.346774193548385e-05,
      "loss": 1.3502,
      "step": 2685
    },
    {
      "epoch": 7.210738255033557,
      "grad_norm": 0.08758299052715302,
      "learning_rate": 8.338709677419354e-05,
      "loss": 1.3651,
      "step": 2686
    },
    {
      "epoch": 7.213422818791947,
      "grad_norm": 0.08837304264307022,
      "learning_rate": 8.330645161290322e-05,
      "loss": 1.4222,
      "step": 2687
    },
    {
      "epoch": 7.2161073825503355,
      "grad_norm": 0.08916479349136353,
      "learning_rate": 8.322580645161289e-05,
      "loss": 1.599,
      "step": 2688
    },
    {
      "epoch": 7.218791946308725,
      "grad_norm": 0.09661757946014404,
      "learning_rate": 8.314516129032257e-05,
      "loss": 1.4061,
      "step": 2689
    },
    {
      "epoch": 7.221476510067114,
      "grad_norm": 0.08524918556213379,
      "learning_rate": 8.306451612903224e-05,
      "loss": 1.5709,
      "step": 2690
    },
    {
      "epoch": 7.2241610738255035,
      "grad_norm": 0.09036600589752197,
      "learning_rate": 8.298387096774193e-05,
      "loss": 1.3705,
      "step": 2691
    },
    {
      "epoch": 7.226845637583892,
      "grad_norm": 0.10617458075284958,
      "learning_rate": 8.290322580645161e-05,
      "loss": 1.4231,
      "step": 2692
    },
    {
      "epoch": 7.229530201342282,
      "grad_norm": 0.09595586359500885,
      "learning_rate": 8.282258064516128e-05,
      "loss": 1.4538,
      "step": 2693
    },
    {
      "epoch": 7.232214765100671,
      "grad_norm": 0.07643330097198486,
      "learning_rate": 8.274193548387096e-05,
      "loss": 1.4734,
      "step": 2694
    },
    {
      "epoch": 7.23489932885906,
      "grad_norm": 0.10237310081720352,
      "learning_rate": 8.266129032258063e-05,
      "loss": 1.3788,
      "step": 2695
    },
    {
      "epoch": 7.23758389261745,
      "grad_norm": 0.08789845556020737,
      "learning_rate": 8.258064516129032e-05,
      "loss": 1.5533,
      "step": 2696
    },
    {
      "epoch": 7.240268456375839,
      "grad_norm": 0.09085866808891296,
      "learning_rate": 8.25e-05,
      "loss": 1.3381,
      "step": 2697
    },
    {
      "epoch": 7.2429530201342285,
      "grad_norm": 0.09266510605812073,
      "learning_rate": 8.241935483870967e-05,
      "loss": 1.5616,
      "step": 2698
    },
    {
      "epoch": 7.245637583892617,
      "grad_norm": 0.08288387954235077,
      "learning_rate": 8.233870967741935e-05,
      "loss": 1.4466,
      "step": 2699
    },
    {
      "epoch": 7.248322147651007,
      "grad_norm": 0.09973280131816864,
      "learning_rate": 8.225806451612902e-05,
      "loss": 1.5298,
      "step": 2700
    },
    {
      "epoch": 7.251006711409396,
      "grad_norm": 0.09322208911180496,
      "learning_rate": 8.21774193548387e-05,
      "loss": 1.2664,
      "step": 2701
    },
    {
      "epoch": 7.253691275167785,
      "grad_norm": 0.09500955790281296,
      "learning_rate": 8.209677419354839e-05,
      "loss": 1.4737,
      "step": 2702
    },
    {
      "epoch": 7.256375838926174,
      "grad_norm": 0.10134610533714294,
      "learning_rate": 8.201612903225806e-05,
      "loss": 1.4055,
      "step": 2703
    },
    {
      "epoch": 7.259060402684564,
      "grad_norm": 0.10781288146972656,
      "learning_rate": 8.193548387096774e-05,
      "loss": 1.2543,
      "step": 2704
    },
    {
      "epoch": 7.261744966442953,
      "grad_norm": 0.08651631325483322,
      "learning_rate": 8.185483870967741e-05,
      "loss": 1.3869,
      "step": 2705
    },
    {
      "epoch": 7.264429530201342,
      "grad_norm": 0.0943150445818901,
      "learning_rate": 8.17741935483871e-05,
      "loss": 1.6049,
      "step": 2706
    },
    {
      "epoch": 7.267114093959732,
      "grad_norm": 0.08796898275613785,
      "learning_rate": 8.169354838709678e-05,
      "loss": 1.4835,
      "step": 2707
    },
    {
      "epoch": 7.269798657718121,
      "grad_norm": 0.08496224880218506,
      "learning_rate": 8.161290322580645e-05,
      "loss": 1.3966,
      "step": 2708
    },
    {
      "epoch": 7.27248322147651,
      "grad_norm": 0.09650127589702606,
      "learning_rate": 8.153225806451613e-05,
      "loss": 1.5613,
      "step": 2709
    },
    {
      "epoch": 7.275167785234899,
      "grad_norm": 0.09485071152448654,
      "learning_rate": 8.145161290322579e-05,
      "loss": 1.5463,
      "step": 2710
    },
    {
      "epoch": 7.277852348993289,
      "grad_norm": 0.10055653005838394,
      "learning_rate": 8.137096774193547e-05,
      "loss": 1.4398,
      "step": 2711
    },
    {
      "epoch": 7.2805369127516775,
      "grad_norm": 0.08707556128501892,
      "learning_rate": 8.129032258064517e-05,
      "loss": 1.4853,
      "step": 2712
    },
    {
      "epoch": 7.283221476510067,
      "grad_norm": 0.0852828100323677,
      "learning_rate": 8.120967741935482e-05,
      "loss": 1.4276,
      "step": 2713
    },
    {
      "epoch": 7.285906040268457,
      "grad_norm": 0.09777054935693741,
      "learning_rate": 8.112903225806451e-05,
      "loss": 1.2812,
      "step": 2714
    },
    {
      "epoch": 7.2885906040268456,
      "grad_norm": 0.09870366752147675,
      "learning_rate": 8.104838709677418e-05,
      "loss": 1.3936,
      "step": 2715
    },
    {
      "epoch": 7.291275167785235,
      "grad_norm": 0.08537502586841583,
      "learning_rate": 8.096774193548386e-05,
      "loss": 1.4885,
      "step": 2716
    },
    {
      "epoch": 7.293959731543624,
      "grad_norm": 0.09430141001939774,
      "learning_rate": 8.088709677419354e-05,
      "loss": 1.3965,
      "step": 2717
    },
    {
      "epoch": 7.296644295302014,
      "grad_norm": 0.08576403558254242,
      "learning_rate": 8.080645161290321e-05,
      "loss": 1.4642,
      "step": 2718
    },
    {
      "epoch": 7.299328859060402,
      "grad_norm": 0.08882368355989456,
      "learning_rate": 8.07258064516129e-05,
      "loss": 1.4673,
      "step": 2719
    },
    {
      "epoch": 7.302013422818792,
      "grad_norm": 0.08952444046735764,
      "learning_rate": 8.064516129032257e-05,
      "loss": 1.5369,
      "step": 2720
    },
    {
      "epoch": 7.304697986577181,
      "grad_norm": 0.09373757988214493,
      "learning_rate": 8.056451612903225e-05,
      "loss": 1.4457,
      "step": 2721
    },
    {
      "epoch": 7.3073825503355705,
      "grad_norm": 0.0864739716053009,
      "learning_rate": 8.048387096774192e-05,
      "loss": 1.4106,
      "step": 2722
    },
    {
      "epoch": 7.310067114093959,
      "grad_norm": 0.09492210298776627,
      "learning_rate": 8.04032258064516e-05,
      "loss": 1.4115,
      "step": 2723
    },
    {
      "epoch": 7.312751677852349,
      "grad_norm": 0.09020504355430603,
      "learning_rate": 8.032258064516129e-05,
      "loss": 1.4091,
      "step": 2724
    },
    {
      "epoch": 7.315436241610739,
      "grad_norm": 0.09800463914871216,
      "learning_rate": 8.024193548387096e-05,
      "loss": 1.6179,
      "step": 2725
    },
    {
      "epoch": 7.318120805369127,
      "grad_norm": 0.09068037569522858,
      "learning_rate": 8.016129032258064e-05,
      "loss": 1.4073,
      "step": 2726
    },
    {
      "epoch": 7.320805369127517,
      "grad_norm": 0.08420611917972565,
      "learning_rate": 8.008064516129031e-05,
      "loss": 1.3991,
      "step": 2727
    },
    {
      "epoch": 7.323489932885906,
      "grad_norm": 0.10043508559465408,
      "learning_rate": 7.999999999999999e-05,
      "loss": 1.4192,
      "step": 2728
    },
    {
      "epoch": 7.326174496644295,
      "grad_norm": 0.0975140854716301,
      "learning_rate": 7.991935483870968e-05,
      "loss": 1.1734,
      "step": 2729
    },
    {
      "epoch": 7.328859060402684,
      "grad_norm": 0.1118740513920784,
      "learning_rate": 7.983870967741935e-05,
      "loss": 1.4139,
      "step": 2730
    },
    {
      "epoch": 7.331543624161074,
      "grad_norm": 0.09106093645095825,
      "learning_rate": 7.975806451612903e-05,
      "loss": 1.4821,
      "step": 2731
    },
    {
      "epoch": 7.334228187919463,
      "grad_norm": 0.09369715303182602,
      "learning_rate": 7.96774193548387e-05,
      "loss": 1.4571,
      "step": 2732
    },
    {
      "epoch": 7.336912751677852,
      "grad_norm": 0.0882185697555542,
      "learning_rate": 7.959677419354838e-05,
      "loss": 1.4425,
      "step": 2733
    },
    {
      "epoch": 7.339597315436242,
      "grad_norm": 0.08914867788553238,
      "learning_rate": 7.951612903225807e-05,
      "loss": 1.485,
      "step": 2734
    },
    {
      "epoch": 7.342281879194631,
      "grad_norm": 0.09498331695795059,
      "learning_rate": 7.943548387096774e-05,
      "loss": 1.4403,
      "step": 2735
    },
    {
      "epoch": 7.34496644295302,
      "grad_norm": 0.08550910651683807,
      "learning_rate": 7.935483870967742e-05,
      "loss": 1.412,
      "step": 2736
    },
    {
      "epoch": 7.347651006711409,
      "grad_norm": 0.09226804971694946,
      "learning_rate": 7.927419354838709e-05,
      "loss": 1.5474,
      "step": 2737
    },
    {
      "epoch": 7.350335570469799,
      "grad_norm": 0.08478288352489471,
      "learning_rate": 7.919354838709677e-05,
      "loss": 1.5681,
      "step": 2738
    },
    {
      "epoch": 7.353020134228188,
      "grad_norm": 0.09322063624858856,
      "learning_rate": 7.911290322580646e-05,
      "loss": 1.4274,
      "step": 2739
    },
    {
      "epoch": 7.355704697986577,
      "grad_norm": 0.09909825772047043,
      "learning_rate": 7.903225806451613e-05,
      "loss": 1.3547,
      "step": 2740
    },
    {
      "epoch": 7.358389261744966,
      "grad_norm": 0.0972476601600647,
      "learning_rate": 7.895161290322581e-05,
      "loss": 1.379,
      "step": 2741
    },
    {
      "epoch": 7.361073825503356,
      "grad_norm": 0.09829829633235931,
      "learning_rate": 7.887096774193546e-05,
      "loss": 1.5005,
      "step": 2742
    },
    {
      "epoch": 7.363758389261745,
      "grad_norm": 0.0865735113620758,
      "learning_rate": 7.879032258064515e-05,
      "loss": 1.4425,
      "step": 2743
    },
    {
      "epoch": 7.366442953020134,
      "grad_norm": 0.09660298377275467,
      "learning_rate": 7.870967741935484e-05,
      "loss": 1.4521,
      "step": 2744
    },
    {
      "epoch": 7.369127516778524,
      "grad_norm": 0.0865727886557579,
      "learning_rate": 7.86290322580645e-05,
      "loss": 1.4811,
      "step": 2745
    },
    {
      "epoch": 7.3718120805369125,
      "grad_norm": 0.09022077172994614,
      "learning_rate": 7.854838709677418e-05,
      "loss": 1.5584,
      "step": 2746
    },
    {
      "epoch": 7.374496644295302,
      "grad_norm": 0.08746327459812164,
      "learning_rate": 7.846774193548385e-05,
      "loss": 1.4282,
      "step": 2747
    },
    {
      "epoch": 7.377181208053691,
      "grad_norm": 0.08273578435182571,
      "learning_rate": 7.838709677419354e-05,
      "loss": 1.55,
      "step": 2748
    },
    {
      "epoch": 7.379865771812081,
      "grad_norm": 0.09712404012680054,
      "learning_rate": 7.830645161290322e-05,
      "loss": 1.366,
      "step": 2749
    },
    {
      "epoch": 7.382550335570469,
      "grad_norm": 0.09249964356422424,
      "learning_rate": 7.822580645161289e-05,
      "loss": 1.4982,
      "step": 2750
    },
    {
      "epoch": 7.385234899328859,
      "grad_norm": 0.11237180978059769,
      "learning_rate": 7.814516129032257e-05,
      "loss": 1.3513,
      "step": 2751
    },
    {
      "epoch": 7.387919463087249,
      "grad_norm": 0.09687444567680359,
      "learning_rate": 7.806451612903224e-05,
      "loss": 1.5431,
      "step": 2752
    },
    {
      "epoch": 7.390604026845637,
      "grad_norm": 0.09694015234708786,
      "learning_rate": 7.798387096774193e-05,
      "loss": 1.5387,
      "step": 2753
    },
    {
      "epoch": 7.393288590604027,
      "grad_norm": 0.09693583101034164,
      "learning_rate": 7.790322580645161e-05,
      "loss": 1.4805,
      "step": 2754
    },
    {
      "epoch": 7.395973154362416,
      "grad_norm": 0.09363467246294022,
      "learning_rate": 7.782258064516128e-05,
      "loss": 1.3917,
      "step": 2755
    },
    {
      "epoch": 7.3986577181208055,
      "grad_norm": 0.08652882277965546,
      "learning_rate": 7.774193548387096e-05,
      "loss": 1.5404,
      "step": 2756
    },
    {
      "epoch": 7.401342281879194,
      "grad_norm": 0.08334984630346298,
      "learning_rate": 7.766129032258063e-05,
      "loss": 1.46,
      "step": 2757
    },
    {
      "epoch": 7.404026845637584,
      "grad_norm": 0.100010447204113,
      "learning_rate": 7.758064516129032e-05,
      "loss": 1.329,
      "step": 2758
    },
    {
      "epoch": 7.406711409395973,
      "grad_norm": 0.08507631719112396,
      "learning_rate": 7.75e-05,
      "loss": 1.4985,
      "step": 2759
    },
    {
      "epoch": 7.409395973154362,
      "grad_norm": 0.08849075436592102,
      "learning_rate": 7.741935483870967e-05,
      "loss": 1.5705,
      "step": 2760
    },
    {
      "epoch": 7.412080536912752,
      "grad_norm": 0.09381840378046036,
      "learning_rate": 7.733870967741935e-05,
      "loss": 1.4663,
      "step": 2761
    },
    {
      "epoch": 7.414765100671141,
      "grad_norm": 0.09312979876995087,
      "learning_rate": 7.725806451612902e-05,
      "loss": 1.374,
      "step": 2762
    },
    {
      "epoch": 7.4174496644295305,
      "grad_norm": 0.08622722327709198,
      "learning_rate": 7.71774193548387e-05,
      "loss": 1.4788,
      "step": 2763
    },
    {
      "epoch": 7.420134228187919,
      "grad_norm": 0.10916195064783096,
      "learning_rate": 7.709677419354839e-05,
      "loss": 1.3639,
      "step": 2764
    },
    {
      "epoch": 7.422818791946309,
      "grad_norm": 0.086266428232193,
      "learning_rate": 7.701612903225806e-05,
      "loss": 1.6007,
      "step": 2765
    },
    {
      "epoch": 7.425503355704698,
      "grad_norm": 0.09768154472112656,
      "learning_rate": 7.693548387096774e-05,
      "loss": 1.3971,
      "step": 2766
    },
    {
      "epoch": 7.428187919463087,
      "grad_norm": 0.09268112480640411,
      "learning_rate": 7.685483870967741e-05,
      "loss": 1.5013,
      "step": 2767
    },
    {
      "epoch": 7.430872483221476,
      "grad_norm": 0.08875679224729538,
      "learning_rate": 7.67741935483871e-05,
      "loss": 1.4786,
      "step": 2768
    },
    {
      "epoch": 7.433557046979866,
      "grad_norm": 0.0864412859082222,
      "learning_rate": 7.669354838709677e-05,
      "loss": 1.4072,
      "step": 2769
    },
    {
      "epoch": 7.436241610738255,
      "grad_norm": 0.08412299305200577,
      "learning_rate": 7.661290322580645e-05,
      "loss": 1.5329,
      "step": 2770
    },
    {
      "epoch": 7.438926174496644,
      "grad_norm": 0.07939204573631287,
      "learning_rate": 7.653225806451613e-05,
      "loss": 1.4643,
      "step": 2771
    },
    {
      "epoch": 7.441610738255034,
      "grad_norm": 0.08864092081785202,
      "learning_rate": 7.645161290322579e-05,
      "loss": 1.4941,
      "step": 2772
    },
    {
      "epoch": 7.444295302013423,
      "grad_norm": 0.10289495438337326,
      "learning_rate": 7.637096774193549e-05,
      "loss": 1.3967,
      "step": 2773
    },
    {
      "epoch": 7.446979865771812,
      "grad_norm": 0.09845445305109024,
      "learning_rate": 7.629032258064514e-05,
      "loss": 1.3955,
      "step": 2774
    },
    {
      "epoch": 7.449664429530201,
      "grad_norm": 0.09721169620752335,
      "learning_rate": 7.620967741935482e-05,
      "loss": 1.5075,
      "step": 2775
    },
    {
      "epoch": 7.452348993288591,
      "grad_norm": 0.08928326517343521,
      "learning_rate": 7.612903225806452e-05,
      "loss": 1.4129,
      "step": 2776
    },
    {
      "epoch": 7.4550335570469795,
      "grad_norm": 0.09098802506923676,
      "learning_rate": 7.604838709677418e-05,
      "loss": 1.5419,
      "step": 2777
    },
    {
      "epoch": 7.457718120805369,
      "grad_norm": 0.08816791325807571,
      "learning_rate": 7.596774193548386e-05,
      "loss": 1.4429,
      "step": 2778
    },
    {
      "epoch": 7.460402684563759,
      "grad_norm": 0.09667714685201645,
      "learning_rate": 7.588709677419353e-05,
      "loss": 1.3762,
      "step": 2779
    },
    {
      "epoch": 7.4630872483221475,
      "grad_norm": 0.08966276794672012,
      "learning_rate": 7.580645161290321e-05,
      "loss": 1.4918,
      "step": 2780
    },
    {
      "epoch": 7.465771812080537,
      "grad_norm": 0.09407909214496613,
      "learning_rate": 7.57258064516129e-05,
      "loss": 1.4037,
      "step": 2781
    },
    {
      "epoch": 7.468456375838926,
      "grad_norm": 0.10874626785516739,
      "learning_rate": 7.564516129032257e-05,
      "loss": 1.3802,
      "step": 2782
    },
    {
      "epoch": 7.471140939597316,
      "grad_norm": 0.08885852992534637,
      "learning_rate": 7.556451612903225e-05,
      "loss": 1.4487,
      "step": 2783
    },
    {
      "epoch": 7.473825503355704,
      "grad_norm": 0.09168890118598938,
      "learning_rate": 7.548387096774192e-05,
      "loss": 1.5589,
      "step": 2784
    },
    {
      "epoch": 7.476510067114094,
      "grad_norm": 0.09646020084619522,
      "learning_rate": 7.54032258064516e-05,
      "loss": 1.4215,
      "step": 2785
    },
    {
      "epoch": 7.479194630872483,
      "grad_norm": 0.10255058109760284,
      "learning_rate": 7.532258064516129e-05,
      "loss": 1.443,
      "step": 2786
    },
    {
      "epoch": 7.4818791946308725,
      "grad_norm": 0.08922175318002701,
      "learning_rate": 7.524193548387096e-05,
      "loss": 1.4568,
      "step": 2787
    },
    {
      "epoch": 7.484563758389262,
      "grad_norm": 0.09041862189769745,
      "learning_rate": 7.516129032258064e-05,
      "loss": 1.4745,
      "step": 2788
    },
    {
      "epoch": 7.487248322147651,
      "grad_norm": 0.0951717421412468,
      "learning_rate": 7.508064516129031e-05,
      "loss": 1.4169,
      "step": 2789
    },
    {
      "epoch": 7.489932885906041,
      "grad_norm": 0.10898274928331375,
      "learning_rate": 7.5e-05,
      "loss": 1.3825,
      "step": 2790
    },
    {
      "epoch": 7.492617449664429,
      "grad_norm": 0.09156092256307602,
      "learning_rate": 7.491935483870966e-05,
      "loss": 1.4994,
      "step": 2791
    },
    {
      "epoch": 7.495302013422819,
      "grad_norm": 0.09726511687040329,
      "learning_rate": 7.483870967741935e-05,
      "loss": 1.429,
      "step": 2792
    },
    {
      "epoch": 7.497986577181208,
      "grad_norm": 0.09138447046279907,
      "learning_rate": 7.475806451612903e-05,
      "loss": 1.5672,
      "step": 2793
    },
    {
      "epoch": 7.500671140939597,
      "grad_norm": 0.09805984795093536,
      "learning_rate": 7.46774193548387e-05,
      "loss": 1.555,
      "step": 2794
    },
    {
      "epoch": 7.503355704697986,
      "grad_norm": 0.09809496253728867,
      "learning_rate": 7.459677419354838e-05,
      "loss": 1.3826,
      "step": 2795
    },
    {
      "epoch": 7.506040268456376,
      "grad_norm": 0.09744301438331604,
      "learning_rate": 7.451612903225805e-05,
      "loss": 1.4112,
      "step": 2796
    },
    {
      "epoch": 7.5087248322147655,
      "grad_norm": 0.11904910951852798,
      "learning_rate": 7.443548387096774e-05,
      "loss": 1.3907,
      "step": 2797
    },
    {
      "epoch": 7.511409395973154,
      "grad_norm": 0.0960146114230156,
      "learning_rate": 7.435483870967742e-05,
      "loss": 1.4113,
      "step": 2798
    },
    {
      "epoch": 7.514093959731544,
      "grad_norm": 0.10178683698177338,
      "learning_rate": 7.427419354838709e-05,
      "loss": 1.5388,
      "step": 2799
    },
    {
      "epoch": 7.516778523489933,
      "grad_norm": 0.09039456397294998,
      "learning_rate": 7.419354838709677e-05,
      "loss": 1.3984,
      "step": 2800
    },
    {
      "epoch": 7.519463087248322,
      "grad_norm": 0.09425120800733566,
      "learning_rate": 7.411290322580644e-05,
      "loss": 1.3977,
      "step": 2801
    },
    {
      "epoch": 7.522147651006711,
      "grad_norm": 0.0894818976521492,
      "learning_rate": 7.403225806451613e-05,
      "loss": 1.4545,
      "step": 2802
    },
    {
      "epoch": 7.524832214765101,
      "grad_norm": 0.0931096076965332,
      "learning_rate": 7.39516129032258e-05,
      "loss": 1.3897,
      "step": 2803
    },
    {
      "epoch": 7.5275167785234895,
      "grad_norm": 0.0896114781498909,
      "learning_rate": 7.387096774193548e-05,
      "loss": 1.4962,
      "step": 2804
    },
    {
      "epoch": 7.530201342281879,
      "grad_norm": 0.09571539610624313,
      "learning_rate": 7.379032258064516e-05,
      "loss": 1.5865,
      "step": 2805
    },
    {
      "epoch": 7.532885906040269,
      "grad_norm": 0.09345446527004242,
      "learning_rate": 7.370967741935483e-05,
      "loss": 1.3995,
      "step": 2806
    },
    {
      "epoch": 7.535570469798658,
      "grad_norm": 0.0903095006942749,
      "learning_rate": 7.36290322580645e-05,
      "loss": 1.5595,
      "step": 2807
    },
    {
      "epoch": 7.538255033557047,
      "grad_norm": 0.08781798183917999,
      "learning_rate": 7.354838709677418e-05,
      "loss": 1.4798,
      "step": 2808
    },
    {
      "epoch": 7.540939597315436,
      "grad_norm": 0.09671641886234283,
      "learning_rate": 7.346774193548387e-05,
      "loss": 1.5707,
      "step": 2809
    },
    {
      "epoch": 7.543624161073826,
      "grad_norm": 0.0901956856250763,
      "learning_rate": 7.338709677419354e-05,
      "loss": 1.5402,
      "step": 2810
    },
    {
      "epoch": 7.5463087248322145,
      "grad_norm": 0.09013789892196655,
      "learning_rate": 7.330645161290322e-05,
      "loss": 1.4394,
      "step": 2811
    },
    {
      "epoch": 7.548993288590604,
      "grad_norm": 0.10162137448787689,
      "learning_rate": 7.322580645161289e-05,
      "loss": 1.4245,
      "step": 2812
    },
    {
      "epoch": 7.551677852348993,
      "grad_norm": 0.0910094678401947,
      "learning_rate": 7.314516129032257e-05,
      "loss": 1.5216,
      "step": 2813
    },
    {
      "epoch": 7.554362416107383,
      "grad_norm": 0.10145185142755508,
      "learning_rate": 7.306451612903226e-05,
      "loss": 1.4679,
      "step": 2814
    },
    {
      "epoch": 7.557046979865772,
      "grad_norm": 0.09132348746061325,
      "learning_rate": 7.298387096774193e-05,
      "loss": 1.4492,
      "step": 2815
    },
    {
      "epoch": 7.559731543624161,
      "grad_norm": 0.10333062708377838,
      "learning_rate": 7.290322580645161e-05,
      "loss": 1.5197,
      "step": 2816
    },
    {
      "epoch": 7.562416107382551,
      "grad_norm": 0.09028119593858719,
      "learning_rate": 7.282258064516128e-05,
      "loss": 1.4936,
      "step": 2817
    },
    {
      "epoch": 7.565100671140939,
      "grad_norm": 0.09478770941495895,
      "learning_rate": 7.274193548387096e-05,
      "loss": 1.5362,
      "step": 2818
    },
    {
      "epoch": 7.567785234899329,
      "grad_norm": 0.09283439069986343,
      "learning_rate": 7.266129032258065e-05,
      "loss": 1.4503,
      "step": 2819
    },
    {
      "epoch": 7.570469798657718,
      "grad_norm": 0.09480907022953033,
      "learning_rate": 7.258064516129032e-05,
      "loss": 1.4668,
      "step": 2820
    },
    {
      "epoch": 7.5731543624161075,
      "grad_norm": 0.09866975992918015,
      "learning_rate": 7.25e-05,
      "loss": 1.4493,
      "step": 2821
    },
    {
      "epoch": 7.575838926174496,
      "grad_norm": 0.10502278059720993,
      "learning_rate": 7.241935483870967e-05,
      "loss": 1.428,
      "step": 2822
    },
    {
      "epoch": 7.578523489932886,
      "grad_norm": 0.08570463210344315,
      "learning_rate": 7.233870967741934e-05,
      "loss": 1.6022,
      "step": 2823
    },
    {
      "epoch": 7.581208053691276,
      "grad_norm": 0.09512006491422653,
      "learning_rate": 7.225806451612902e-05,
      "loss": 1.4141,
      "step": 2824
    },
    {
      "epoch": 7.583892617449664,
      "grad_norm": 0.08963900804519653,
      "learning_rate": 7.21774193548387e-05,
      "loss": 1.3721,
      "step": 2825
    },
    {
      "epoch": 7.586577181208054,
      "grad_norm": 0.07916092127561569,
      "learning_rate": 7.209677419354838e-05,
      "loss": 1.4994,
      "step": 2826
    },
    {
      "epoch": 7.589261744966443,
      "grad_norm": 0.08849149197340012,
      "learning_rate": 7.201612903225806e-05,
      "loss": 1.5244,
      "step": 2827
    },
    {
      "epoch": 7.5919463087248324,
      "grad_norm": 0.08559317141771317,
      "learning_rate": 7.193548387096773e-05,
      "loss": 1.5213,
      "step": 2828
    },
    {
      "epoch": 7.594630872483221,
      "grad_norm": 0.0953543558716774,
      "learning_rate": 7.185483870967741e-05,
      "loss": 1.5342,
      "step": 2829
    },
    {
      "epoch": 7.597315436241611,
      "grad_norm": 0.09818023443222046,
      "learning_rate": 7.17741935483871e-05,
      "loss": 1.4491,
      "step": 2830
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.08535007387399673,
      "learning_rate": 7.169354838709677e-05,
      "loss": 1.5099,
      "step": 2831
    },
    {
      "epoch": 7.602684563758389,
      "grad_norm": 0.09751272946596146,
      "learning_rate": 7.161290322580645e-05,
      "loss": 1.4213,
      "step": 2832
    },
    {
      "epoch": 7.605369127516779,
      "grad_norm": 0.10423405468463898,
      "learning_rate": 7.153225806451612e-05,
      "loss": 1.3381,
      "step": 2833
    },
    {
      "epoch": 7.608053691275168,
      "grad_norm": 0.09611556679010391,
      "learning_rate": 7.14516129032258e-05,
      "loss": 1.5313,
      "step": 2834
    },
    {
      "epoch": 7.610738255033557,
      "grad_norm": 0.09059648960828781,
      "learning_rate": 7.137096774193549e-05,
      "loss": 1.4454,
      "step": 2835
    },
    {
      "epoch": 7.613422818791946,
      "grad_norm": 0.09217989444732666,
      "learning_rate": 7.129032258064516e-05,
      "loss": 1.4132,
      "step": 2836
    },
    {
      "epoch": 7.616107382550336,
      "grad_norm": 0.08803681284189224,
      "learning_rate": 7.120967741935484e-05,
      "loss": 1.4863,
      "step": 2837
    },
    {
      "epoch": 7.618791946308725,
      "grad_norm": 0.08963814377784729,
      "learning_rate": 7.112903225806451e-05,
      "loss": 1.4573,
      "step": 2838
    },
    {
      "epoch": 7.621476510067114,
      "grad_norm": 0.10125800222158432,
      "learning_rate": 7.104838709677418e-05,
      "loss": 1.318,
      "step": 2839
    },
    {
      "epoch": 7.624161073825503,
      "grad_norm": 0.10808426886796951,
      "learning_rate": 7.096774193548386e-05,
      "loss": 1.3552,
      "step": 2840
    },
    {
      "epoch": 7.626845637583893,
      "grad_norm": 0.10466394573450089,
      "learning_rate": 7.088709677419355e-05,
      "loss": 1.4274,
      "step": 2841
    },
    {
      "epoch": 7.629530201342282,
      "grad_norm": 0.08314356952905655,
      "learning_rate": 7.080645161290321e-05,
      "loss": 1.4938,
      "step": 2842
    },
    {
      "epoch": 7.632214765100671,
      "grad_norm": 0.09583730250597,
      "learning_rate": 7.07258064516129e-05,
      "loss": 1.4952,
      "step": 2843
    },
    {
      "epoch": 7.634899328859061,
      "grad_norm": 0.09229084849357605,
      "learning_rate": 7.064516129032257e-05,
      "loss": 1.4664,
      "step": 2844
    },
    {
      "epoch": 7.6375838926174495,
      "grad_norm": 0.10891588777303696,
      "learning_rate": 7.056451612903225e-05,
      "loss": 1.5032,
      "step": 2845
    },
    {
      "epoch": 7.640268456375839,
      "grad_norm": 0.0899089127779007,
      "learning_rate": 7.048387096774193e-05,
      "loss": 1.5849,
      "step": 2846
    },
    {
      "epoch": 7.642953020134228,
      "grad_norm": 0.09952894598245621,
      "learning_rate": 7.04032258064516e-05,
      "loss": 1.4958,
      "step": 2847
    },
    {
      "epoch": 7.645637583892618,
      "grad_norm": 0.09887974709272385,
      "learning_rate": 7.032258064516129e-05,
      "loss": 1.4955,
      "step": 2848
    },
    {
      "epoch": 7.648322147651006,
      "grad_norm": 0.08400338143110275,
      "learning_rate": 7.024193548387096e-05,
      "loss": 1.4539,
      "step": 2849
    },
    {
      "epoch": 7.651006711409396,
      "grad_norm": 0.08744999021291733,
      "learning_rate": 7.016129032258064e-05,
      "loss": 1.4297,
      "step": 2850
    },
    {
      "epoch": 7.653691275167786,
      "grad_norm": 0.0921512022614479,
      "learning_rate": 7.008064516129032e-05,
      "loss": 1.4264,
      "step": 2851
    },
    {
      "epoch": 7.6563758389261745,
      "grad_norm": 0.0861104428768158,
      "learning_rate": 7e-05,
      "loss": 1.6036,
      "step": 2852
    },
    {
      "epoch": 7.659060402684564,
      "grad_norm": 0.09978774189949036,
      "learning_rate": 6.991935483870968e-05,
      "loss": 1.5854,
      "step": 2853
    },
    {
      "epoch": 7.661744966442953,
      "grad_norm": 0.10036282986402512,
      "learning_rate": 6.983870967741935e-05,
      "loss": 1.4945,
      "step": 2854
    },
    {
      "epoch": 7.6644295302013425,
      "grad_norm": 0.08874024450778961,
      "learning_rate": 6.975806451612902e-05,
      "loss": 1.4643,
      "step": 2855
    },
    {
      "epoch": 7.667114093959731,
      "grad_norm": 0.09536979347467422,
      "learning_rate": 6.96774193548387e-05,
      "loss": 1.4797,
      "step": 2856
    },
    {
      "epoch": 7.669798657718121,
      "grad_norm": 0.09648957848548889,
      "learning_rate": 6.959677419354838e-05,
      "loss": 1.3931,
      "step": 2857
    },
    {
      "epoch": 7.67248322147651,
      "grad_norm": 0.0928712710738182,
      "learning_rate": 6.951612903225805e-05,
      "loss": 1.5509,
      "step": 2858
    },
    {
      "epoch": 7.675167785234899,
      "grad_norm": 0.0913601964712143,
      "learning_rate": 6.943548387096774e-05,
      "loss": 1.3974,
      "step": 2859
    },
    {
      "epoch": 7.677852348993289,
      "grad_norm": 0.08851700276136398,
      "learning_rate": 6.93548387096774e-05,
      "loss": 1.4984,
      "step": 2860
    },
    {
      "epoch": 7.680536912751678,
      "grad_norm": 0.10876505821943283,
      "learning_rate": 6.927419354838709e-05,
      "loss": 1.3842,
      "step": 2861
    },
    {
      "epoch": 7.6832214765100675,
      "grad_norm": 0.0873180404305458,
      "learning_rate": 6.919354838709677e-05,
      "loss": 1.4471,
      "step": 2862
    },
    {
      "epoch": 7.685906040268456,
      "grad_norm": 0.09959644079208374,
      "learning_rate": 6.911290322580644e-05,
      "loss": 1.3408,
      "step": 2863
    },
    {
      "epoch": 7.688590604026846,
      "grad_norm": 0.08563597500324249,
      "learning_rate": 6.903225806451613e-05,
      "loss": 1.5835,
      "step": 2864
    },
    {
      "epoch": 7.691275167785235,
      "grad_norm": 0.08301445096731186,
      "learning_rate": 6.89516129032258e-05,
      "loss": 1.5814,
      "step": 2865
    },
    {
      "epoch": 7.693959731543624,
      "grad_norm": 0.09457135200500488,
      "learning_rate": 6.887096774193548e-05,
      "loss": 1.5378,
      "step": 2866
    },
    {
      "epoch": 7.696644295302013,
      "grad_norm": 0.08750203996896744,
      "learning_rate": 6.879032258064516e-05,
      "loss": 1.6515,
      "step": 2867
    },
    {
      "epoch": 7.699328859060403,
      "grad_norm": 0.10243380814790726,
      "learning_rate": 6.870967741935483e-05,
      "loss": 1.514,
      "step": 2868
    },
    {
      "epoch": 7.702013422818792,
      "grad_norm": 0.0814373791217804,
      "learning_rate": 6.862903225806452e-05,
      "loss": 1.6169,
      "step": 2869
    },
    {
      "epoch": 7.704697986577181,
      "grad_norm": 0.0993405357003212,
      "learning_rate": 6.854838709677419e-05,
      "loss": 1.3799,
      "step": 2870
    },
    {
      "epoch": 7.707382550335571,
      "grad_norm": 0.09669725596904755,
      "learning_rate": 6.846774193548387e-05,
      "loss": 1.4118,
      "step": 2871
    },
    {
      "epoch": 7.71006711409396,
      "grad_norm": 0.09537263959646225,
      "learning_rate": 6.838709677419354e-05,
      "loss": 1.4947,
      "step": 2872
    },
    {
      "epoch": 7.712751677852349,
      "grad_norm": 0.09143438935279846,
      "learning_rate": 6.830645161290322e-05,
      "loss": 1.4497,
      "step": 2873
    },
    {
      "epoch": 7.715436241610738,
      "grad_norm": 0.09033547341823578,
      "learning_rate": 6.822580645161289e-05,
      "loss": 1.4526,
      "step": 2874
    },
    {
      "epoch": 7.718120805369128,
      "grad_norm": 0.09948844462633133,
      "learning_rate": 6.814516129032257e-05,
      "loss": 1.4161,
      "step": 2875
    },
    {
      "epoch": 7.7208053691275165,
      "grad_norm": 0.10279601812362671,
      "learning_rate": 6.806451612903224e-05,
      "loss": 1.4484,
      "step": 2876
    },
    {
      "epoch": 7.723489932885906,
      "grad_norm": 0.09207604080438614,
      "learning_rate": 6.798387096774193e-05,
      "loss": 1.4884,
      "step": 2877
    },
    {
      "epoch": 7.726174496644296,
      "grad_norm": 0.08940380811691284,
      "learning_rate": 6.790322580645161e-05,
      "loss": 1.4791,
      "step": 2878
    },
    {
      "epoch": 7.7288590604026846,
      "grad_norm": 0.09236045181751251,
      "learning_rate": 6.782258064516128e-05,
      "loss": 1.5483,
      "step": 2879
    },
    {
      "epoch": 7.731543624161074,
      "grad_norm": 0.0886126309633255,
      "learning_rate": 6.774193548387096e-05,
      "loss": 1.4032,
      "step": 2880
    },
    {
      "epoch": 7.734228187919463,
      "grad_norm": 0.09784022718667984,
      "learning_rate": 6.766129032258063e-05,
      "loss": 1.3368,
      "step": 2881
    },
    {
      "epoch": 7.736912751677853,
      "grad_norm": 0.10096829384565353,
      "learning_rate": 6.758064516129032e-05,
      "loss": 1.4179,
      "step": 2882
    },
    {
      "epoch": 7.739597315436241,
      "grad_norm": 0.10309542715549469,
      "learning_rate": 6.75e-05,
      "loss": 1.3366,
      "step": 2883
    },
    {
      "epoch": 7.742281879194631,
      "grad_norm": 0.08192179352045059,
      "learning_rate": 6.741935483870967e-05,
      "loss": 1.4904,
      "step": 2884
    },
    {
      "epoch": 7.74496644295302,
      "grad_norm": 0.0918494388461113,
      "learning_rate": 6.733870967741935e-05,
      "loss": 1.3687,
      "step": 2885
    },
    {
      "epoch": 7.7476510067114095,
      "grad_norm": 0.09071333706378937,
      "learning_rate": 6.725806451612902e-05,
      "loss": 1.4587,
      "step": 2886
    },
    {
      "epoch": 7.750335570469799,
      "grad_norm": 0.1058933436870575,
      "learning_rate": 6.717741935483871e-05,
      "loss": 1.4061,
      "step": 2887
    },
    {
      "epoch": 7.753020134228188,
      "grad_norm": 0.10195473581552505,
      "learning_rate": 6.709677419354838e-05,
      "loss": 1.3612,
      "step": 2888
    },
    {
      "epoch": 7.755704697986577,
      "grad_norm": 0.09875708073377609,
      "learning_rate": 6.701612903225806e-05,
      "loss": 1.5051,
      "step": 2889
    },
    {
      "epoch": 7.758389261744966,
      "grad_norm": 0.0922023355960846,
      "learning_rate": 6.693548387096773e-05,
      "loss": 1.5121,
      "step": 2890
    },
    {
      "epoch": 7.761073825503356,
      "grad_norm": 0.09353882819414139,
      "learning_rate": 6.685483870967741e-05,
      "loss": 1.4572,
      "step": 2891
    },
    {
      "epoch": 7.763758389261745,
      "grad_norm": 0.09819580614566803,
      "learning_rate": 6.67741935483871e-05,
      "loss": 1.5096,
      "step": 2892
    },
    {
      "epoch": 7.766442953020134,
      "grad_norm": 0.0981869250535965,
      "learning_rate": 6.669354838709677e-05,
      "loss": 1.5073,
      "step": 2893
    },
    {
      "epoch": 7.769127516778523,
      "grad_norm": 0.08560860902070999,
      "learning_rate": 6.661290322580645e-05,
      "loss": 1.5261,
      "step": 2894
    },
    {
      "epoch": 7.771812080536913,
      "grad_norm": 0.10615452378988266,
      "learning_rate": 6.653225806451612e-05,
      "loss": 1.4525,
      "step": 2895
    },
    {
      "epoch": 7.774496644295302,
      "grad_norm": 0.08617091923952103,
      "learning_rate": 6.64516129032258e-05,
      "loss": 1.5444,
      "step": 2896
    },
    {
      "epoch": 7.777181208053691,
      "grad_norm": 0.08801817148923874,
      "learning_rate": 6.637096774193547e-05,
      "loss": 1.4163,
      "step": 2897
    },
    {
      "epoch": 7.77986577181208,
      "grad_norm": 0.084379643201828,
      "learning_rate": 6.629032258064516e-05,
      "loss": 1.5722,
      "step": 2898
    },
    {
      "epoch": 7.78255033557047,
      "grad_norm": 0.10740359872579575,
      "learning_rate": 6.620967741935484e-05,
      "loss": 1.4062,
      "step": 2899
    },
    {
      "epoch": 7.785234899328859,
      "grad_norm": 0.08882416784763336,
      "learning_rate": 6.612903225806451e-05,
      "loss": 1.4731,
      "step": 2900
    },
    {
      "epoch": 7.787919463087248,
      "grad_norm": 0.09198886901140213,
      "learning_rate": 6.604838709677419e-05,
      "loss": 1.4181,
      "step": 2901
    },
    {
      "epoch": 7.790604026845638,
      "grad_norm": 0.0815117284655571,
      "learning_rate": 6.596774193548386e-05,
      "loss": 1.449,
      "step": 2902
    },
    {
      "epoch": 7.793288590604027,
      "grad_norm": 0.09389273077249527,
      "learning_rate": 6.588709677419355e-05,
      "loss": 1.5075,
      "step": 2903
    },
    {
      "epoch": 7.795973154362416,
      "grad_norm": 0.09524666517972946,
      "learning_rate": 6.580645161290322e-05,
      "loss": 1.429,
      "step": 2904
    },
    {
      "epoch": 7.798657718120805,
      "grad_norm": 0.08694223314523697,
      "learning_rate": 6.57258064516129e-05,
      "loss": 1.4842,
      "step": 2905
    },
    {
      "epoch": 7.801342281879195,
      "grad_norm": 0.07717917114496231,
      "learning_rate": 6.564516129032257e-05,
      "loss": 1.4599,
      "step": 2906
    },
    {
      "epoch": 7.804026845637583,
      "grad_norm": 0.09120559692382812,
      "learning_rate": 6.556451612903225e-05,
      "loss": 1.4252,
      "step": 2907
    },
    {
      "epoch": 7.806711409395973,
      "grad_norm": 0.08812445402145386,
      "learning_rate": 6.548387096774193e-05,
      "loss": 1.5723,
      "step": 2908
    },
    {
      "epoch": 7.809395973154363,
      "grad_norm": 0.1046266034245491,
      "learning_rate": 6.54032258064516e-05,
      "loss": 1.3693,
      "step": 2909
    },
    {
      "epoch": 7.8120805369127515,
      "grad_norm": 0.09760721772909164,
      "learning_rate": 6.532258064516129e-05,
      "loss": 1.4159,
      "step": 2910
    },
    {
      "epoch": 7.814765100671141,
      "grad_norm": 0.093525730073452,
      "learning_rate": 6.524193548387096e-05,
      "loss": 1.4702,
      "step": 2911
    },
    {
      "epoch": 7.81744966442953,
      "grad_norm": 0.10083571821451187,
      "learning_rate": 6.516129032258064e-05,
      "loss": 1.3949,
      "step": 2912
    },
    {
      "epoch": 7.82013422818792,
      "grad_norm": 0.11134649068117142,
      "learning_rate": 6.508064516129032e-05,
      "loss": 1.3843,
      "step": 2913
    },
    {
      "epoch": 7.822818791946308,
      "grad_norm": 0.09783631563186646,
      "learning_rate": 6.5e-05,
      "loss": 1.5217,
      "step": 2914
    },
    {
      "epoch": 7.825503355704698,
      "grad_norm": 0.08854605257511139,
      "learning_rate": 6.491935483870968e-05,
      "loss": 1.5096,
      "step": 2915
    },
    {
      "epoch": 7.828187919463087,
      "grad_norm": 0.09734935313463211,
      "learning_rate": 6.483870967741935e-05,
      "loss": 1.4185,
      "step": 2916
    },
    {
      "epoch": 7.830872483221476,
      "grad_norm": 0.09174905717372894,
      "learning_rate": 6.475806451612903e-05,
      "loss": 1.2699,
      "step": 2917
    },
    {
      "epoch": 7.833557046979866,
      "grad_norm": 0.10380113869905472,
      "learning_rate": 6.46774193548387e-05,
      "loss": 1.4409,
      "step": 2918
    },
    {
      "epoch": 7.836241610738255,
      "grad_norm": 0.08909523487091064,
      "learning_rate": 6.459677419354838e-05,
      "loss": 1.5214,
      "step": 2919
    },
    {
      "epoch": 7.8389261744966445,
      "grad_norm": 0.09745671600103378,
      "learning_rate": 6.451612903225805e-05,
      "loss": 1.4537,
      "step": 2920
    },
    {
      "epoch": 7.841610738255033,
      "grad_norm": 0.08811865746974945,
      "learning_rate": 6.443548387096774e-05,
      "loss": 1.5272,
      "step": 2921
    },
    {
      "epoch": 7.844295302013423,
      "grad_norm": 0.09219930320978165,
      "learning_rate": 6.43548387096774e-05,
      "loss": 1.4014,
      "step": 2922
    },
    {
      "epoch": 7.846979865771812,
      "grad_norm": 0.09305266290903091,
      "learning_rate": 6.427419354838709e-05,
      "loss": 1.3922,
      "step": 2923
    },
    {
      "epoch": 7.849664429530201,
      "grad_norm": 0.10596930235624313,
      "learning_rate": 6.419354838709677e-05,
      "loss": 1.441,
      "step": 2924
    },
    {
      "epoch": 7.85234899328859,
      "grad_norm": 0.1006690189242363,
      "learning_rate": 6.411290322580644e-05,
      "loss": 1.4065,
      "step": 2925
    },
    {
      "epoch": 7.85503355704698,
      "grad_norm": 0.08596097677946091,
      "learning_rate": 6.403225806451613e-05,
      "loss": 1.4662,
      "step": 2926
    },
    {
      "epoch": 7.8577181208053695,
      "grad_norm": 0.09428002685308456,
      "learning_rate": 6.39516129032258e-05,
      "loss": 1.5477,
      "step": 2927
    },
    {
      "epoch": 7.860402684563758,
      "grad_norm": 0.09307979047298431,
      "learning_rate": 6.387096774193548e-05,
      "loss": 1.4883,
      "step": 2928
    },
    {
      "epoch": 7.863087248322148,
      "grad_norm": 0.09627138078212738,
      "learning_rate": 6.379032258064516e-05,
      "loss": 1.4618,
      "step": 2929
    },
    {
      "epoch": 7.865771812080537,
      "grad_norm": 0.10319381207227707,
      "learning_rate": 6.370967741935483e-05,
      "loss": 1.479,
      "step": 2930
    },
    {
      "epoch": 7.868456375838926,
      "grad_norm": 0.08851804584264755,
      "learning_rate": 6.362903225806452e-05,
      "loss": 1.4773,
      "step": 2931
    },
    {
      "epoch": 7.871140939597315,
      "grad_norm": 0.10487981885671616,
      "learning_rate": 6.354838709677419e-05,
      "loss": 1.4861,
      "step": 2932
    },
    {
      "epoch": 7.873825503355705,
      "grad_norm": 0.08420391380786896,
      "learning_rate": 6.346774193548386e-05,
      "loss": 1.4174,
      "step": 2933
    },
    {
      "epoch": 7.8765100671140935,
      "grad_norm": 0.11009290814399719,
      "learning_rate": 6.338709677419355e-05,
      "loss": 1.3973,
      "step": 2934
    },
    {
      "epoch": 7.879194630872483,
      "grad_norm": 0.09577199071645737,
      "learning_rate": 6.330645161290322e-05,
      "loss": 1.4954,
      "step": 2935
    },
    {
      "epoch": 7.881879194630873,
      "grad_norm": 0.09761003404855728,
      "learning_rate": 6.322580645161289e-05,
      "loss": 1.4865,
      "step": 2936
    },
    {
      "epoch": 7.884563758389262,
      "grad_norm": 0.09233175963163376,
      "learning_rate": 6.314516129032258e-05,
      "loss": 1.4501,
      "step": 2937
    },
    {
      "epoch": 7.887248322147651,
      "grad_norm": 0.08680161088705063,
      "learning_rate": 6.306451612903225e-05,
      "loss": 1.5166,
      "step": 2938
    },
    {
      "epoch": 7.88993288590604,
      "grad_norm": 0.08925937861204147,
      "learning_rate": 6.298387096774193e-05,
      "loss": 1.4152,
      "step": 2939
    },
    {
      "epoch": 7.89261744966443,
      "grad_norm": 0.09176570177078247,
      "learning_rate": 6.290322580645161e-05,
      "loss": 1.4024,
      "step": 2940
    },
    {
      "epoch": 7.8953020134228185,
      "grad_norm": 0.08781624585390091,
      "learning_rate": 6.282258064516128e-05,
      "loss": 1.4664,
      "step": 2941
    },
    {
      "epoch": 7.897986577181208,
      "grad_norm": 0.09738806635141373,
      "learning_rate": 6.274193548387096e-05,
      "loss": 1.3553,
      "step": 2942
    },
    {
      "epoch": 7.900671140939597,
      "grad_norm": 0.10760033875703812,
      "learning_rate": 6.266129032258063e-05,
      "loss": 1.4072,
      "step": 2943
    },
    {
      "epoch": 7.9033557046979865,
      "grad_norm": 0.0901622325181961,
      "learning_rate": 6.258064516129032e-05,
      "loss": 1.4562,
      "step": 2944
    },
    {
      "epoch": 7.906040268456376,
      "grad_norm": 0.10059354454278946,
      "learning_rate": 6.25e-05,
      "loss": 1.4584,
      "step": 2945
    },
    {
      "epoch": 7.908724832214765,
      "grad_norm": 0.08650729060173035,
      "learning_rate": 6.241935483870967e-05,
      "loss": 1.4432,
      "step": 2946
    },
    {
      "epoch": 7.911409395973155,
      "grad_norm": 0.09734533727169037,
      "learning_rate": 6.233870967741935e-05,
      "loss": 1.351,
      "step": 2947
    },
    {
      "epoch": 7.914093959731543,
      "grad_norm": 0.08667679131031036,
      "learning_rate": 6.225806451612902e-05,
      "loss": 1.4076,
      "step": 2948
    },
    {
      "epoch": 7.916778523489933,
      "grad_norm": 0.10180570930242538,
      "learning_rate": 6.21774193548387e-05,
      "loss": 1.4317,
      "step": 2949
    },
    {
      "epoch": 7.919463087248322,
      "grad_norm": 0.09696045517921448,
      "learning_rate": 6.209677419354839e-05,
      "loss": 1.4229,
      "step": 2950
    },
    {
      "epoch": 7.9221476510067115,
      "grad_norm": 0.09373913705348969,
      "learning_rate": 6.201612903225806e-05,
      "loss": 1.4077,
      "step": 2951
    },
    {
      "epoch": 7.9248322147651,
      "grad_norm": 0.09983564168214798,
      "learning_rate": 6.193548387096773e-05,
      "loss": 1.4034,
      "step": 2952
    },
    {
      "epoch": 7.92751677852349,
      "grad_norm": 0.08981133252382278,
      "learning_rate": 6.185483870967741e-05,
      "loss": 1.4674,
      "step": 2953
    },
    {
      "epoch": 7.93020134228188,
      "grad_norm": 0.09000731259584427,
      "learning_rate": 6.177419354838708e-05,
      "loss": 1.4869,
      "step": 2954
    },
    {
      "epoch": 7.932885906040268,
      "grad_norm": 0.09368417412042618,
      "learning_rate": 6.169354838709677e-05,
      "loss": 1.4674,
      "step": 2955
    },
    {
      "epoch": 7.935570469798658,
      "grad_norm": 0.08997856080532074,
      "learning_rate": 6.161290322580645e-05,
      "loss": 1.4371,
      "step": 2956
    },
    {
      "epoch": 7.938255033557047,
      "grad_norm": 0.09682709723711014,
      "learning_rate": 6.153225806451612e-05,
      "loss": 1.4438,
      "step": 2957
    },
    {
      "epoch": 7.940939597315436,
      "grad_norm": 0.09940604120492935,
      "learning_rate": 6.14516129032258e-05,
      "loss": 1.5423,
      "step": 2958
    },
    {
      "epoch": 7.943624161073825,
      "grad_norm": 0.08893896639347076,
      "learning_rate": 6.137096774193547e-05,
      "loss": 1.5352,
      "step": 2959
    },
    {
      "epoch": 7.946308724832215,
      "grad_norm": 0.08923707902431488,
      "learning_rate": 6.129032258064516e-05,
      "loss": 1.4168,
      "step": 2960
    },
    {
      "epoch": 7.948993288590604,
      "grad_norm": 0.0903199166059494,
      "learning_rate": 6.120967741935484e-05,
      "loss": 1.3994,
      "step": 2961
    },
    {
      "epoch": 7.951677852348993,
      "grad_norm": 0.09736183285713196,
      "learning_rate": 6.112903225806451e-05,
      "loss": 1.378,
      "step": 2962
    },
    {
      "epoch": 7.954362416107383,
      "grad_norm": 0.08779634535312653,
      "learning_rate": 6.104838709677419e-05,
      "loss": 1.4894,
      "step": 2963
    },
    {
      "epoch": 7.957046979865772,
      "grad_norm": 0.0938786044716835,
      "learning_rate": 6.096774193548386e-05,
      "loss": 1.4362,
      "step": 2964
    },
    {
      "epoch": 7.959731543624161,
      "grad_norm": 0.09463975578546524,
      "learning_rate": 6.088709677419354e-05,
      "loss": 1.3725,
      "step": 2965
    },
    {
      "epoch": 7.96241610738255,
      "grad_norm": 0.08277148008346558,
      "learning_rate": 6.080645161290322e-05,
      "loss": 1.5146,
      "step": 2966
    },
    {
      "epoch": 7.96510067114094,
      "grad_norm": 0.09430092573165894,
      "learning_rate": 6.07258064516129e-05,
      "loss": 1.454,
      "step": 2967
    },
    {
      "epoch": 7.9677852348993286,
      "grad_norm": 0.09116699546575546,
      "learning_rate": 6.0645161290322576e-05,
      "loss": 1.489,
      "step": 2968
    },
    {
      "epoch": 7.970469798657718,
      "grad_norm": 0.08987240493297577,
      "learning_rate": 6.056451612903225e-05,
      "loss": 1.3503,
      "step": 2969
    },
    {
      "epoch": 7.973154362416107,
      "grad_norm": 0.09134499728679657,
      "learning_rate": 6.048387096774193e-05,
      "loss": 1.4104,
      "step": 2970
    },
    {
      "epoch": 7.975838926174497,
      "grad_norm": 0.09414692968130112,
      "learning_rate": 6.040322580645161e-05,
      "loss": 1.3843,
      "step": 2971
    },
    {
      "epoch": 7.978523489932886,
      "grad_norm": 0.09924964606761932,
      "learning_rate": 6.032258064516129e-05,
      "loss": 1.3341,
      "step": 2972
    },
    {
      "epoch": 7.981208053691275,
      "grad_norm": 0.09483303874731064,
      "learning_rate": 6.0241935483870965e-05,
      "loss": 1.3797,
      "step": 2973
    },
    {
      "epoch": 7.983892617449665,
      "grad_norm": 0.10075885057449341,
      "learning_rate": 6.0161290322580635e-05,
      "loss": 1.483,
      "step": 2974
    },
    {
      "epoch": 7.9865771812080535,
      "grad_norm": 0.08839251101016998,
      "learning_rate": 6.008064516129031e-05,
      "loss": 1.5385,
      "step": 2975
    },
    {
      "epoch": 7.989261744966443,
      "grad_norm": 0.08682653307914734,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.4078,
      "step": 2976
    },
    {
      "epoch": 7.991946308724832,
      "grad_norm": 0.09290864318609238,
      "learning_rate": 5.991935483870967e-05,
      "loss": 1.4396,
      "step": 2977
    },
    {
      "epoch": 7.994630872483222,
      "grad_norm": 0.09564715623855591,
      "learning_rate": 5.983870967741935e-05,
      "loss": 1.4564,
      "step": 2978
    },
    {
      "epoch": 7.99731543624161,
      "grad_norm": 0.08978605270385742,
      "learning_rate": 5.9758064516129024e-05,
      "loss": 1.3898,
      "step": 2979
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.09542316943407059,
      "learning_rate": 5.96774193548387e-05,
      "loss": 1.5073,
      "step": 2980
    },
    {
      "epoch": 8.00268456375839,
      "grad_norm": 0.09863811731338501,
      "learning_rate": 5.9596774193548384e-05,
      "loss": 1.4537,
      "step": 2981
    },
    {
      "epoch": 8.00536912751678,
      "grad_norm": 0.09239611029624939,
      "learning_rate": 5.951612903225806e-05,
      "loss": 1.4889,
      "step": 2982
    },
    {
      "epoch": 8.008053691275167,
      "grad_norm": 0.09574726969003677,
      "learning_rate": 5.943548387096774e-05,
      "loss": 1.4195,
      "step": 2983
    },
    {
      "epoch": 8.010738255033557,
      "grad_norm": 0.09134431928396225,
      "learning_rate": 5.9354838709677414e-05,
      "loss": 1.6381,
      "step": 2984
    },
    {
      "epoch": 8.013422818791947,
      "grad_norm": 0.10018911957740784,
      "learning_rate": 5.927419354838709e-05,
      "loss": 1.451,
      "step": 2985
    },
    {
      "epoch": 8.016107382550336,
      "grad_norm": 0.09068964421749115,
      "learning_rate": 5.9193548387096774e-05,
      "loss": 1.5012,
      "step": 2986
    },
    {
      "epoch": 8.018791946308724,
      "grad_norm": 0.09500107169151306,
      "learning_rate": 5.911290322580645e-05,
      "loss": 1.4778,
      "step": 2987
    },
    {
      "epoch": 8.021476510067114,
      "grad_norm": 0.09485157579183578,
      "learning_rate": 5.903225806451613e-05,
      "loss": 1.3924,
      "step": 2988
    },
    {
      "epoch": 8.024161073825503,
      "grad_norm": 0.08882471174001694,
      "learning_rate": 5.8951612903225803e-05,
      "loss": 1.5095,
      "step": 2989
    },
    {
      "epoch": 8.026845637583893,
      "grad_norm": 0.09696996212005615,
      "learning_rate": 5.887096774193547e-05,
      "loss": 1.3689,
      "step": 2990
    },
    {
      "epoch": 8.029530201342283,
      "grad_norm": 0.08782033622264862,
      "learning_rate": 5.879032258064515e-05,
      "loss": 1.5021,
      "step": 2991
    },
    {
      "epoch": 8.03221476510067,
      "grad_norm": 0.09410084038972855,
      "learning_rate": 5.870967741935483e-05,
      "loss": 1.5396,
      "step": 2992
    },
    {
      "epoch": 8.03489932885906,
      "grad_norm": 0.08748198300600052,
      "learning_rate": 5.862903225806451e-05,
      "loss": 1.587,
      "step": 2993
    },
    {
      "epoch": 8.03758389261745,
      "grad_norm": 0.09407491981983185,
      "learning_rate": 5.8548387096774186e-05,
      "loss": 1.5703,
      "step": 2994
    },
    {
      "epoch": 8.04026845637584,
      "grad_norm": 0.09532643854618073,
      "learning_rate": 5.846774193548386e-05,
      "loss": 1.3812,
      "step": 2995
    },
    {
      "epoch": 8.042953020134227,
      "grad_norm": 0.11860165745019913,
      "learning_rate": 5.838709677419354e-05,
      "loss": 1.4626,
      "step": 2996
    },
    {
      "epoch": 8.045637583892617,
      "grad_norm": 0.08039725571870804,
      "learning_rate": 5.830645161290322e-05,
      "loss": 1.531,
      "step": 2997
    },
    {
      "epoch": 8.048322147651007,
      "grad_norm": 0.08581988513469696,
      "learning_rate": 5.82258064516129e-05,
      "loss": 1.412,
      "step": 2998
    },
    {
      "epoch": 8.051006711409396,
      "grad_norm": 0.09409591555595398,
      "learning_rate": 5.8145161290322576e-05,
      "loss": 1.4952,
      "step": 2999
    },
    {
      "epoch": 8.053691275167786,
      "grad_norm": 0.08632458746433258,
      "learning_rate": 5.806451612903225e-05,
      "loss": 1.3693,
      "step": 3000
    },
    {
      "epoch": 8.056375838926174,
      "grad_norm": 0.09722333401441574,
      "learning_rate": 5.798387096774193e-05,
      "loss": 1.4159,
      "step": 3001
    },
    {
      "epoch": 8.059060402684564,
      "grad_norm": 0.11039704829454422,
      "learning_rate": 5.790322580645161e-05,
      "loss": 1.4011,
      "step": 3002
    },
    {
      "epoch": 8.061744966442953,
      "grad_norm": 0.10387755185365677,
      "learning_rate": 5.782258064516129e-05,
      "loss": 1.4321,
      "step": 3003
    },
    {
      "epoch": 8.064429530201343,
      "grad_norm": 0.08599665760993958,
      "learning_rate": 5.7741935483870965e-05,
      "loss": 1.4214,
      "step": 3004
    },
    {
      "epoch": 8.06711409395973,
      "grad_norm": 0.08607502281665802,
      "learning_rate": 5.766129032258064e-05,
      "loss": 1.4931,
      "step": 3005
    },
    {
      "epoch": 8.06979865771812,
      "grad_norm": 0.0872163400053978,
      "learning_rate": 5.758064516129031e-05,
      "loss": 1.5075,
      "step": 3006
    },
    {
      "epoch": 8.07248322147651,
      "grad_norm": 0.09046189486980438,
      "learning_rate": 5.75e-05,
      "loss": 1.4178,
      "step": 3007
    },
    {
      "epoch": 8.0751677852349,
      "grad_norm": 0.09982556104660034,
      "learning_rate": 5.741935483870967e-05,
      "loss": 1.5051,
      "step": 3008
    },
    {
      "epoch": 8.07785234899329,
      "grad_norm": 0.08463571965694427,
      "learning_rate": 5.733870967741935e-05,
      "loss": 1.495,
      "step": 3009
    },
    {
      "epoch": 8.080536912751677,
      "grad_norm": 0.09143121540546417,
      "learning_rate": 5.7258064516129025e-05,
      "loss": 1.4385,
      "step": 3010
    },
    {
      "epoch": 8.083221476510067,
      "grad_norm": 0.08576834201812744,
      "learning_rate": 5.71774193548387e-05,
      "loss": 1.6106,
      "step": 3011
    },
    {
      "epoch": 8.085906040268457,
      "grad_norm": 0.10654270648956299,
      "learning_rate": 5.709677419354838e-05,
      "loss": 1.2117,
      "step": 3012
    },
    {
      "epoch": 8.088590604026846,
      "grad_norm": 0.09436730295419693,
      "learning_rate": 5.701612903225806e-05,
      "loss": 1.4903,
      "step": 3013
    },
    {
      "epoch": 8.091275167785234,
      "grad_norm": 0.09550300240516663,
      "learning_rate": 5.693548387096774e-05,
      "loss": 1.415,
      "step": 3014
    },
    {
      "epoch": 8.093959731543624,
      "grad_norm": 0.10255610197782516,
      "learning_rate": 5.6854838709677414e-05,
      "loss": 1.4816,
      "step": 3015
    },
    {
      "epoch": 8.096644295302013,
      "grad_norm": 0.09170836210250854,
      "learning_rate": 5.677419354838709e-05,
      "loss": 1.4083,
      "step": 3016
    },
    {
      "epoch": 8.099328859060403,
      "grad_norm": 0.09872215241193771,
      "learning_rate": 5.669354838709677e-05,
      "loss": 1.4179,
      "step": 3017
    },
    {
      "epoch": 8.102013422818793,
      "grad_norm": 0.09433981031179428,
      "learning_rate": 5.661290322580645e-05,
      "loss": 1.3426,
      "step": 3018
    },
    {
      "epoch": 8.10469798657718,
      "grad_norm": 0.09678846597671509,
      "learning_rate": 5.653225806451613e-05,
      "loss": 1.3524,
      "step": 3019
    },
    {
      "epoch": 8.10738255033557,
      "grad_norm": 0.08883391320705414,
      "learning_rate": 5.6451612903225804e-05,
      "loss": 1.4305,
      "step": 3020
    },
    {
      "epoch": 8.11006711409396,
      "grad_norm": 0.11420546472072601,
      "learning_rate": 5.637096774193548e-05,
      "loss": 1.4243,
      "step": 3021
    },
    {
      "epoch": 8.11275167785235,
      "grad_norm": 0.08761858195066452,
      "learning_rate": 5.629032258064515e-05,
      "loss": 1.4412,
      "step": 3022
    },
    {
      "epoch": 8.115436241610738,
      "grad_norm": 0.11501619219779968,
      "learning_rate": 5.620967741935484e-05,
      "loss": 1.542,
      "step": 3023
    },
    {
      "epoch": 8.118120805369127,
      "grad_norm": 0.09496831148862839,
      "learning_rate": 5.612903225806451e-05,
      "loss": 1.3887,
      "step": 3024
    },
    {
      "epoch": 8.120805369127517,
      "grad_norm": 0.09219260513782501,
      "learning_rate": 5.6048387096774186e-05,
      "loss": 1.391,
      "step": 3025
    },
    {
      "epoch": 8.123489932885906,
      "grad_norm": 0.08978118747472763,
      "learning_rate": 5.596774193548386e-05,
      "loss": 1.4648,
      "step": 3026
    },
    {
      "epoch": 8.126174496644296,
      "grad_norm": 0.10307198017835617,
      "learning_rate": 5.588709677419354e-05,
      "loss": 1.509,
      "step": 3027
    },
    {
      "epoch": 8.128859060402684,
      "grad_norm": 0.0942654088139534,
      "learning_rate": 5.580645161290322e-05,
      "loss": 1.4616,
      "step": 3028
    },
    {
      "epoch": 8.131543624161074,
      "grad_norm": 0.10749860107898712,
      "learning_rate": 5.57258064516129e-05,
      "loss": 1.3209,
      "step": 3029
    },
    {
      "epoch": 8.134228187919463,
      "grad_norm": 0.08924258500337601,
      "learning_rate": 5.5645161290322576e-05,
      "loss": 1.4117,
      "step": 3030
    },
    {
      "epoch": 8.136912751677853,
      "grad_norm": 0.09385544061660767,
      "learning_rate": 5.556451612903225e-05,
      "loss": 1.3688,
      "step": 3031
    },
    {
      "epoch": 8.13959731543624,
      "grad_norm": 0.09818963706493378,
      "learning_rate": 5.548387096774193e-05,
      "loss": 1.4724,
      "step": 3032
    },
    {
      "epoch": 8.14228187919463,
      "grad_norm": 0.08129885047674179,
      "learning_rate": 5.540322580645161e-05,
      "loss": 1.561,
      "step": 3033
    },
    {
      "epoch": 8.14496644295302,
      "grad_norm": 0.09700249135494232,
      "learning_rate": 5.532258064516129e-05,
      "loss": 1.4841,
      "step": 3034
    },
    {
      "epoch": 8.14765100671141,
      "grad_norm": 0.09623537212610245,
      "learning_rate": 5.5241935483870966e-05,
      "loss": 1.5527,
      "step": 3035
    },
    {
      "epoch": 8.1503355704698,
      "grad_norm": 0.09773093461990356,
      "learning_rate": 5.516129032258064e-05,
      "loss": 1.5204,
      "step": 3036
    },
    {
      "epoch": 8.153020134228187,
      "grad_norm": 0.09468104690313339,
      "learning_rate": 5.508064516129032e-05,
      "loss": 1.4799,
      "step": 3037
    },
    {
      "epoch": 8.155704697986577,
      "grad_norm": 0.09484828263521194,
      "learning_rate": 5.499999999999999e-05,
      "loss": 1.4466,
      "step": 3038
    },
    {
      "epoch": 8.158389261744967,
      "grad_norm": 0.10274099558591843,
      "learning_rate": 5.491935483870968e-05,
      "loss": 1.3403,
      "step": 3039
    },
    {
      "epoch": 8.161073825503356,
      "grad_norm": 0.10011276602745056,
      "learning_rate": 5.483870967741935e-05,
      "loss": 1.5229,
      "step": 3040
    },
    {
      "epoch": 8.163758389261744,
      "grad_norm": 0.09558454900979996,
      "learning_rate": 5.4758064516129025e-05,
      "loss": 1.3363,
      "step": 3041
    },
    {
      "epoch": 8.166442953020134,
      "grad_norm": 0.09827824681997299,
      "learning_rate": 5.46774193548387e-05,
      "loss": 1.3944,
      "step": 3042
    },
    {
      "epoch": 8.169127516778524,
      "grad_norm": 0.11039692163467407,
      "learning_rate": 5.459677419354838e-05,
      "loss": 1.3553,
      "step": 3043
    },
    {
      "epoch": 8.171812080536913,
      "grad_norm": 0.0889822393655777,
      "learning_rate": 5.451612903225806e-05,
      "loss": 1.3706,
      "step": 3044
    },
    {
      "epoch": 8.174496644295303,
      "grad_norm": 0.0984526202082634,
      "learning_rate": 5.443548387096774e-05,
      "loss": 1.3987,
      "step": 3045
    },
    {
      "epoch": 8.17718120805369,
      "grad_norm": 0.08760125190019608,
      "learning_rate": 5.4354838709677414e-05,
      "loss": 1.4494,
      "step": 3046
    },
    {
      "epoch": 8.17986577181208,
      "grad_norm": 0.09820257127285004,
      "learning_rate": 5.427419354838709e-05,
      "loss": 1.4556,
      "step": 3047
    },
    {
      "epoch": 8.18255033557047,
      "grad_norm": 0.09724947810173035,
      "learning_rate": 5.419354838709677e-05,
      "loss": 1.4852,
      "step": 3048
    },
    {
      "epoch": 8.18523489932886,
      "grad_norm": 0.08243916183710098,
      "learning_rate": 5.411290322580645e-05,
      "loss": 1.5011,
      "step": 3049
    },
    {
      "epoch": 8.187919463087248,
      "grad_norm": 0.09734497219324112,
      "learning_rate": 5.403225806451613e-05,
      "loss": 1.402,
      "step": 3050
    },
    {
      "epoch": 8.190604026845637,
      "grad_norm": 0.08815394341945648,
      "learning_rate": 5.3951612903225804e-05,
      "loss": 1.4165,
      "step": 3051
    },
    {
      "epoch": 8.193288590604027,
      "grad_norm": 0.08736561238765717,
      "learning_rate": 5.387096774193548e-05,
      "loss": 1.5337,
      "step": 3052
    },
    {
      "epoch": 8.195973154362417,
      "grad_norm": 0.0899144783616066,
      "learning_rate": 5.379032258064516e-05,
      "loss": 1.6038,
      "step": 3053
    },
    {
      "epoch": 8.198657718120806,
      "grad_norm": 0.09426051378250122,
      "learning_rate": 5.370967741935484e-05,
      "loss": 1.4242,
      "step": 3054
    },
    {
      "epoch": 8.201342281879194,
      "grad_norm": 0.10317375510931015,
      "learning_rate": 5.362903225806452e-05,
      "loss": 1.3778,
      "step": 3055
    },
    {
      "epoch": 8.204026845637584,
      "grad_norm": 0.08381231129169464,
      "learning_rate": 5.354838709677419e-05,
      "loss": 1.5168,
      "step": 3056
    },
    {
      "epoch": 8.206711409395973,
      "grad_norm": 0.08774202316999435,
      "learning_rate": 5.346774193548386e-05,
      "loss": 1.5329,
      "step": 3057
    },
    {
      "epoch": 8.209395973154363,
      "grad_norm": 0.09785810858011246,
      "learning_rate": 5.338709677419354e-05,
      "loss": 1.4861,
      "step": 3058
    },
    {
      "epoch": 8.212080536912751,
      "grad_norm": 0.09256065636873245,
      "learning_rate": 5.3306451612903216e-05,
      "loss": 1.4607,
      "step": 3059
    },
    {
      "epoch": 8.21476510067114,
      "grad_norm": 0.10095484554767609,
      "learning_rate": 5.32258064516129e-05,
      "loss": 1.5089,
      "step": 3060
    },
    {
      "epoch": 8.21744966442953,
      "grad_norm": 0.09521409124135971,
      "learning_rate": 5.3145161290322576e-05,
      "loss": 1.449,
      "step": 3061
    },
    {
      "epoch": 8.22013422818792,
      "grad_norm": 0.09268032014369965,
      "learning_rate": 5.306451612903225e-05,
      "loss": 1.5497,
      "step": 3062
    },
    {
      "epoch": 8.22281879194631,
      "grad_norm": 0.09543568640947342,
      "learning_rate": 5.298387096774193e-05,
      "loss": 1.4881,
      "step": 3063
    },
    {
      "epoch": 8.225503355704697,
      "grad_norm": 0.10218708962202072,
      "learning_rate": 5.2903225806451606e-05,
      "loss": 1.351,
      "step": 3064
    },
    {
      "epoch": 8.228187919463087,
      "grad_norm": 0.08856482058763504,
      "learning_rate": 5.282258064516129e-05,
      "loss": 1.5331,
      "step": 3065
    },
    {
      "epoch": 8.230872483221477,
      "grad_norm": 0.08954982459545135,
      "learning_rate": 5.2741935483870966e-05,
      "loss": 1.4791,
      "step": 3066
    },
    {
      "epoch": 8.233557046979866,
      "grad_norm": 0.09160500764846802,
      "learning_rate": 5.266129032258064e-05,
      "loss": 1.5496,
      "step": 3067
    },
    {
      "epoch": 8.236241610738254,
      "grad_norm": 0.0857345461845398,
      "learning_rate": 5.258064516129032e-05,
      "loss": 1.5294,
      "step": 3068
    },
    {
      "epoch": 8.238926174496644,
      "grad_norm": 0.07951221615076065,
      "learning_rate": 5.2499999999999995e-05,
      "loss": 1.5181,
      "step": 3069
    },
    {
      "epoch": 8.241610738255034,
      "grad_norm": 0.09967558085918427,
      "learning_rate": 5.241935483870968e-05,
      "loss": 1.4735,
      "step": 3070
    },
    {
      "epoch": 8.244295302013423,
      "grad_norm": 0.08948690444231033,
      "learning_rate": 5.2338709677419355e-05,
      "loss": 1.4418,
      "step": 3071
    },
    {
      "epoch": 8.246979865771813,
      "grad_norm": 0.10095617175102234,
      "learning_rate": 5.2258064516129025e-05,
      "loss": 1.3962,
      "step": 3072
    },
    {
      "epoch": 8.2496644295302,
      "grad_norm": 0.08860302716493607,
      "learning_rate": 5.21774193548387e-05,
      "loss": 1.4404,
      "step": 3073
    },
    {
      "epoch": 8.25234899328859,
      "grad_norm": 0.09526866674423218,
      "learning_rate": 5.209677419354838e-05,
      "loss": 1.5422,
      "step": 3074
    },
    {
      "epoch": 8.25503355704698,
      "grad_norm": 0.10019080340862274,
      "learning_rate": 5.201612903225806e-05,
      "loss": 1.4215,
      "step": 3075
    },
    {
      "epoch": 8.25771812080537,
      "grad_norm": 0.09111117571592331,
      "learning_rate": 5.193548387096774e-05,
      "loss": 1.5115,
      "step": 3076
    },
    {
      "epoch": 8.260402684563758,
      "grad_norm": 0.08925880491733551,
      "learning_rate": 5.1854838709677415e-05,
      "loss": 1.4916,
      "step": 3077
    },
    {
      "epoch": 8.263087248322147,
      "grad_norm": 0.094078928232193,
      "learning_rate": 5.177419354838709e-05,
      "loss": 1.3809,
      "step": 3078
    },
    {
      "epoch": 8.265771812080537,
      "grad_norm": 0.09300051629543304,
      "learning_rate": 5.169354838709677e-05,
      "loss": 1.41,
      "step": 3079
    },
    {
      "epoch": 8.268456375838927,
      "grad_norm": 0.08043276518583298,
      "learning_rate": 5.161290322580645e-05,
      "loss": 1.419,
      "step": 3080
    },
    {
      "epoch": 8.271140939597316,
      "grad_norm": 0.0942733883857727,
      "learning_rate": 5.153225806451613e-05,
      "loss": 1.4557,
      "step": 3081
    },
    {
      "epoch": 8.273825503355704,
      "grad_norm": 0.09099181741476059,
      "learning_rate": 5.1451612903225804e-05,
      "loss": 1.3504,
      "step": 3082
    },
    {
      "epoch": 8.276510067114094,
      "grad_norm": 0.09272432327270508,
      "learning_rate": 5.137096774193548e-05,
      "loss": 1.3862,
      "step": 3083
    },
    {
      "epoch": 8.279194630872484,
      "grad_norm": 0.08879640698432922,
      "learning_rate": 5.129032258064516e-05,
      "loss": 1.5041,
      "step": 3084
    },
    {
      "epoch": 8.281879194630873,
      "grad_norm": 0.09059035032987595,
      "learning_rate": 5.120967741935483e-05,
      "loss": 1.4549,
      "step": 3085
    },
    {
      "epoch": 8.284563758389261,
      "grad_norm": 0.10317770391702652,
      "learning_rate": 5.112903225806452e-05,
      "loss": 1.3444,
      "step": 3086
    },
    {
      "epoch": 8.28724832214765,
      "grad_norm": 0.08447645604610443,
      "learning_rate": 5.1048387096774194e-05,
      "loss": 1.4236,
      "step": 3087
    },
    {
      "epoch": 8.28993288590604,
      "grad_norm": 0.09040424227714539,
      "learning_rate": 5.0967741935483863e-05,
      "loss": 1.5476,
      "step": 3088
    },
    {
      "epoch": 8.29261744966443,
      "grad_norm": 0.10354790836572647,
      "learning_rate": 5.088709677419354e-05,
      "loss": 1.5255,
      "step": 3089
    },
    {
      "epoch": 8.29530201342282,
      "grad_norm": 0.09296563267707825,
      "learning_rate": 5.0806451612903217e-05,
      "loss": 1.4584,
      "step": 3090
    },
    {
      "epoch": 8.297986577181208,
      "grad_norm": 0.07972726970911026,
      "learning_rate": 5.07258064516129e-05,
      "loss": 1.6241,
      "step": 3091
    },
    {
      "epoch": 8.300671140939597,
      "grad_norm": 0.09727546572685242,
      "learning_rate": 5.0645161290322576e-05,
      "loss": 1.4942,
      "step": 3092
    },
    {
      "epoch": 8.303355704697987,
      "grad_norm": 0.10035283118486404,
      "learning_rate": 5.056451612903225e-05,
      "loss": 1.5578,
      "step": 3093
    },
    {
      "epoch": 8.306040268456377,
      "grad_norm": 0.0985746756196022,
      "learning_rate": 5.048387096774193e-05,
      "loss": 1.3966,
      "step": 3094
    },
    {
      "epoch": 8.308724832214764,
      "grad_norm": 0.09441592544317245,
      "learning_rate": 5.0403225806451606e-05,
      "loss": 1.4646,
      "step": 3095
    },
    {
      "epoch": 8.311409395973154,
      "grad_norm": 0.08554814010858536,
      "learning_rate": 5.032258064516129e-05,
      "loss": 1.4953,
      "step": 3096
    },
    {
      "epoch": 8.314093959731544,
      "grad_norm": 0.08845514059066772,
      "learning_rate": 5.0241935483870966e-05,
      "loss": 1.4978,
      "step": 3097
    },
    {
      "epoch": 8.316778523489933,
      "grad_norm": 0.10279908031225204,
      "learning_rate": 5.016129032258064e-05,
      "loss": 1.394,
      "step": 3098
    },
    {
      "epoch": 8.319463087248323,
      "grad_norm": 0.09688553214073181,
      "learning_rate": 5.008064516129032e-05,
      "loss": 1.5103,
      "step": 3099
    },
    {
      "epoch": 8.322147651006711,
      "grad_norm": 0.08867964148521423,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 1.5362,
      "step": 3100
    },
    {
      "epoch": 8.3248322147651,
      "grad_norm": 0.09390518814325333,
      "learning_rate": 4.991935483870968e-05,
      "loss": 1.4244,
      "step": 3101
    },
    {
      "epoch": 8.32751677852349,
      "grad_norm": 0.09399539977312088,
      "learning_rate": 4.9838709677419356e-05,
      "loss": 1.614,
      "step": 3102
    },
    {
      "epoch": 8.33020134228188,
      "grad_norm": 0.08194833993911743,
      "learning_rate": 4.975806451612903e-05,
      "loss": 1.4303,
      "step": 3103
    },
    {
      "epoch": 8.332885906040268,
      "grad_norm": 0.08940904587507248,
      "learning_rate": 4.96774193548387e-05,
      "loss": 1.4561,
      "step": 3104
    },
    {
      "epoch": 8.335570469798657,
      "grad_norm": 0.08802124857902527,
      "learning_rate": 4.959677419354838e-05,
      "loss": 1.4238,
      "step": 3105
    },
    {
      "epoch": 8.338255033557047,
      "grad_norm": 0.10525226593017578,
      "learning_rate": 4.9516129032258055e-05,
      "loss": 1.5646,
      "step": 3106
    },
    {
      "epoch": 8.340939597315437,
      "grad_norm": 0.11544063687324524,
      "learning_rate": 4.943548387096774e-05,
      "loss": 1.4166,
      "step": 3107
    },
    {
      "epoch": 8.343624161073825,
      "grad_norm": 0.09530371427536011,
      "learning_rate": 4.9354838709677415e-05,
      "loss": 1.4129,
      "step": 3108
    },
    {
      "epoch": 8.346308724832214,
      "grad_norm": 0.10360333323478699,
      "learning_rate": 4.927419354838709e-05,
      "loss": 1.4714,
      "step": 3109
    },
    {
      "epoch": 8.348993288590604,
      "grad_norm": 0.10027451068162918,
      "learning_rate": 4.919354838709677e-05,
      "loss": 1.4164,
      "step": 3110
    },
    {
      "epoch": 8.351677852348994,
      "grad_norm": 0.10410752147436142,
      "learning_rate": 4.9112903225806444e-05,
      "loss": 1.3698,
      "step": 3111
    },
    {
      "epoch": 8.354362416107383,
      "grad_norm": 0.0918835997581482,
      "learning_rate": 4.903225806451613e-05,
      "loss": 1.4552,
      "step": 3112
    },
    {
      "epoch": 8.357046979865771,
      "grad_norm": 0.08372712880373001,
      "learning_rate": 4.8951612903225804e-05,
      "loss": 1.4884,
      "step": 3113
    },
    {
      "epoch": 8.35973154362416,
      "grad_norm": 0.09594332426786423,
      "learning_rate": 4.887096774193548e-05,
      "loss": 1.5046,
      "step": 3114
    },
    {
      "epoch": 8.36241610738255,
      "grad_norm": 0.0826299712061882,
      "learning_rate": 4.879032258064516e-05,
      "loss": 1.5094,
      "step": 3115
    },
    {
      "epoch": 8.36510067114094,
      "grad_norm": 0.0986771509051323,
      "learning_rate": 4.8709677419354834e-05,
      "loss": 1.4165,
      "step": 3116
    },
    {
      "epoch": 8.367785234899328,
      "grad_norm": 0.10698797553777695,
      "learning_rate": 4.862903225806452e-05,
      "loss": 1.4708,
      "step": 3117
    },
    {
      "epoch": 8.370469798657718,
      "grad_norm": 0.1044021025300026,
      "learning_rate": 4.8548387096774194e-05,
      "loss": 1.4892,
      "step": 3118
    },
    {
      "epoch": 8.373154362416107,
      "grad_norm": 0.09090697020292282,
      "learning_rate": 4.8467741935483864e-05,
      "loss": 1.5241,
      "step": 3119
    },
    {
      "epoch": 8.375838926174497,
      "grad_norm": 0.09943164885044098,
      "learning_rate": 4.838709677419354e-05,
      "loss": 1.4499,
      "step": 3120
    },
    {
      "epoch": 8.378523489932887,
      "grad_norm": 0.08823990821838379,
      "learning_rate": 4.830645161290322e-05,
      "loss": 1.4628,
      "step": 3121
    },
    {
      "epoch": 8.381208053691275,
      "grad_norm": 0.09775196015834808,
      "learning_rate": 4.82258064516129e-05,
      "loss": 1.4741,
      "step": 3122
    },
    {
      "epoch": 8.383892617449664,
      "grad_norm": 0.09005533158779144,
      "learning_rate": 4.814516129032258e-05,
      "loss": 1.4456,
      "step": 3123
    },
    {
      "epoch": 8.386577181208054,
      "grad_norm": 0.09209604561328888,
      "learning_rate": 4.806451612903225e-05,
      "loss": 1.4266,
      "step": 3124
    },
    {
      "epoch": 8.389261744966444,
      "grad_norm": 0.09160726517438889,
      "learning_rate": 4.798387096774193e-05,
      "loss": 1.4351,
      "step": 3125
    },
    {
      "epoch": 8.391946308724831,
      "grad_norm": 0.09320540726184845,
      "learning_rate": 4.7903225806451606e-05,
      "loss": 1.4024,
      "step": 3126
    },
    {
      "epoch": 8.394630872483221,
      "grad_norm": 0.08531501889228821,
      "learning_rate": 4.782258064516129e-05,
      "loss": 1.293,
      "step": 3127
    },
    {
      "epoch": 8.39731543624161,
      "grad_norm": 0.09398499131202698,
      "learning_rate": 4.7741935483870966e-05,
      "loss": 1.4701,
      "step": 3128
    },
    {
      "epoch": 8.4,
      "grad_norm": 0.0908646509051323,
      "learning_rate": 4.766129032258064e-05,
      "loss": 1.6223,
      "step": 3129
    },
    {
      "epoch": 8.40268456375839,
      "grad_norm": 0.09649556130170822,
      "learning_rate": 4.758064516129032e-05,
      "loss": 1.5408,
      "step": 3130
    },
    {
      "epoch": 8.405369127516778,
      "grad_norm": 0.0884602814912796,
      "learning_rate": 4.7499999999999996e-05,
      "loss": 1.4388,
      "step": 3131
    },
    {
      "epoch": 8.408053691275168,
      "grad_norm": 0.10004481673240662,
      "learning_rate": 4.741935483870967e-05,
      "loss": 1.4153,
      "step": 3132
    },
    {
      "epoch": 8.410738255033557,
      "grad_norm": 0.09126445651054382,
      "learning_rate": 4.7338709677419356e-05,
      "loss": 1.4532,
      "step": 3133
    },
    {
      "epoch": 8.413422818791947,
      "grad_norm": 0.08834600448608398,
      "learning_rate": 4.725806451612903e-05,
      "loss": 1.4927,
      "step": 3134
    },
    {
      "epoch": 8.416107382550335,
      "grad_norm": 0.09582794457674026,
      "learning_rate": 4.71774193548387e-05,
      "loss": 1.3845,
      "step": 3135
    },
    {
      "epoch": 8.418791946308724,
      "grad_norm": 0.09775838255882263,
      "learning_rate": 4.709677419354838e-05,
      "loss": 1.4764,
      "step": 3136
    },
    {
      "epoch": 8.421476510067114,
      "grad_norm": 0.09167902171611786,
      "learning_rate": 4.7016129032258055e-05,
      "loss": 1.3975,
      "step": 3137
    },
    {
      "epoch": 8.424161073825504,
      "grad_norm": 0.08826553821563721,
      "learning_rate": 4.693548387096774e-05,
      "loss": 1.5114,
      "step": 3138
    },
    {
      "epoch": 8.426845637583893,
      "grad_norm": 0.10022567212581635,
      "learning_rate": 4.6854838709677415e-05,
      "loss": 1.4084,
      "step": 3139
    },
    {
      "epoch": 8.429530201342281,
      "grad_norm": 0.09495177119970322,
      "learning_rate": 4.677419354838709e-05,
      "loss": 1.3712,
      "step": 3140
    },
    {
      "epoch": 8.432214765100671,
      "grad_norm": 0.09422556310892105,
      "learning_rate": 4.669354838709677e-05,
      "loss": 1.4835,
      "step": 3141
    },
    {
      "epoch": 8.43489932885906,
      "grad_norm": 0.08745929598808289,
      "learning_rate": 4.6612903225806445e-05,
      "loss": 1.4574,
      "step": 3142
    },
    {
      "epoch": 8.43758389261745,
      "grad_norm": 0.09881628304719925,
      "learning_rate": 4.653225806451613e-05,
      "loss": 1.4486,
      "step": 3143
    },
    {
      "epoch": 8.440268456375838,
      "grad_norm": 0.09855936467647552,
      "learning_rate": 4.6451612903225805e-05,
      "loss": 1.3879,
      "step": 3144
    },
    {
      "epoch": 8.442953020134228,
      "grad_norm": 0.0965537428855896,
      "learning_rate": 4.637096774193548e-05,
      "loss": 1.4361,
      "step": 3145
    },
    {
      "epoch": 8.445637583892617,
      "grad_norm": 0.09305085241794586,
      "learning_rate": 4.629032258064516e-05,
      "loss": 1.4216,
      "step": 3146
    },
    {
      "epoch": 8.448322147651007,
      "grad_norm": 0.10679084062576294,
      "learning_rate": 4.6209677419354834e-05,
      "loss": 1.4742,
      "step": 3147
    },
    {
      "epoch": 8.451006711409397,
      "grad_norm": 0.09985250234603882,
      "learning_rate": 4.612903225806452e-05,
      "loss": 1.5105,
      "step": 3148
    },
    {
      "epoch": 8.453691275167785,
      "grad_norm": 0.1007838323712349,
      "learning_rate": 4.6048387096774194e-05,
      "loss": 1.4255,
      "step": 3149
    },
    {
      "epoch": 8.456375838926174,
      "grad_norm": 0.09452944248914719,
      "learning_rate": 4.596774193548387e-05,
      "loss": 1.4765,
      "step": 3150
    },
    {
      "epoch": 8.459060402684564,
      "grad_norm": 0.09332634508609772,
      "learning_rate": 4.588709677419354e-05,
      "loss": 1.4593,
      "step": 3151
    },
    {
      "epoch": 8.461744966442954,
      "grad_norm": 0.08667111396789551,
      "learning_rate": 4.580645161290322e-05,
      "loss": 1.4486,
      "step": 3152
    },
    {
      "epoch": 8.464429530201341,
      "grad_norm": 0.0928269699215889,
      "learning_rate": 4.5725806451612894e-05,
      "loss": 1.4446,
      "step": 3153
    },
    {
      "epoch": 8.467114093959731,
      "grad_norm": 0.11016292124986649,
      "learning_rate": 4.564516129032258e-05,
      "loss": 1.3706,
      "step": 3154
    },
    {
      "epoch": 8.46979865771812,
      "grad_norm": 0.09047362208366394,
      "learning_rate": 4.5564516129032253e-05,
      "loss": 1.4896,
      "step": 3155
    },
    {
      "epoch": 8.47248322147651,
      "grad_norm": 0.09024333208799362,
      "learning_rate": 4.548387096774193e-05,
      "loss": 1.4056,
      "step": 3156
    },
    {
      "epoch": 8.4751677852349,
      "grad_norm": 0.10238926857709885,
      "learning_rate": 4.5403225806451607e-05,
      "loss": 1.4153,
      "step": 3157
    },
    {
      "epoch": 8.477852348993288,
      "grad_norm": 0.09456968307495117,
      "learning_rate": 4.532258064516128e-05,
      "loss": 1.3538,
      "step": 3158
    },
    {
      "epoch": 8.480536912751678,
      "grad_norm": 0.09444987028837204,
      "learning_rate": 4.5241935483870966e-05,
      "loss": 1.4458,
      "step": 3159
    },
    {
      "epoch": 8.483221476510067,
      "grad_norm": 0.096663698554039,
      "learning_rate": 4.516129032258064e-05,
      "loss": 1.3821,
      "step": 3160
    },
    {
      "epoch": 8.485906040268457,
      "grad_norm": 0.0960526242852211,
      "learning_rate": 4.508064516129032e-05,
      "loss": 1.3904,
      "step": 3161
    },
    {
      "epoch": 8.488590604026845,
      "grad_norm": 0.08680884540081024,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 1.4687,
      "step": 3162
    },
    {
      "epoch": 8.491275167785235,
      "grad_norm": 0.09364085644483566,
      "learning_rate": 4.491935483870967e-05,
      "loss": 1.4575,
      "step": 3163
    },
    {
      "epoch": 8.493959731543624,
      "grad_norm": 0.09906543791294098,
      "learning_rate": 4.4838709677419356e-05,
      "loss": 1.4836,
      "step": 3164
    },
    {
      "epoch": 8.496644295302014,
      "grad_norm": 0.09676092118024826,
      "learning_rate": 4.475806451612903e-05,
      "loss": 1.4409,
      "step": 3165
    },
    {
      "epoch": 8.499328859060403,
      "grad_norm": 0.10097189992666245,
      "learning_rate": 4.467741935483871e-05,
      "loss": 1.3745,
      "step": 3166
    },
    {
      "epoch": 8.502013422818791,
      "grad_norm": 0.11488229036331177,
      "learning_rate": 4.459677419354838e-05,
      "loss": 1.1672,
      "step": 3167
    },
    {
      "epoch": 8.504697986577181,
      "grad_norm": 0.08859609067440033,
      "learning_rate": 4.4516129032258055e-05,
      "loss": 1.4233,
      "step": 3168
    },
    {
      "epoch": 8.50738255033557,
      "grad_norm": 0.0945950448513031,
      "learning_rate": 4.443548387096774e-05,
      "loss": 1.4442,
      "step": 3169
    },
    {
      "epoch": 8.51006711409396,
      "grad_norm": 0.09288953989744186,
      "learning_rate": 4.4354838709677415e-05,
      "loss": 1.4012,
      "step": 3170
    },
    {
      "epoch": 8.512751677852348,
      "grad_norm": 0.10022744536399841,
      "learning_rate": 4.427419354838709e-05,
      "loss": 1.4884,
      "step": 3171
    },
    {
      "epoch": 8.515436241610738,
      "grad_norm": 0.09902528673410416,
      "learning_rate": 4.419354838709677e-05,
      "loss": 1.4471,
      "step": 3172
    },
    {
      "epoch": 8.518120805369128,
      "grad_norm": 0.09398268908262253,
      "learning_rate": 4.4112903225806445e-05,
      "loss": 1.3584,
      "step": 3173
    },
    {
      "epoch": 8.520805369127517,
      "grad_norm": 0.10080571472644806,
      "learning_rate": 4.403225806451612e-05,
      "loss": 1.4656,
      "step": 3174
    },
    {
      "epoch": 8.523489932885907,
      "grad_norm": 0.0858190730214119,
      "learning_rate": 4.3951612903225805e-05,
      "loss": 1.4685,
      "step": 3175
    },
    {
      "epoch": 8.526174496644295,
      "grad_norm": 0.09385284036397934,
      "learning_rate": 4.387096774193548e-05,
      "loss": 1.6112,
      "step": 3176
    },
    {
      "epoch": 8.528859060402684,
      "grad_norm": 0.0919162929058075,
      "learning_rate": 4.379032258064516e-05,
      "loss": 1.3025,
      "step": 3177
    },
    {
      "epoch": 8.531543624161074,
      "grad_norm": 0.09274789690971375,
      "learning_rate": 4.3709677419354834e-05,
      "loss": 1.5114,
      "step": 3178
    },
    {
      "epoch": 8.534228187919464,
      "grad_norm": 0.09811879694461823,
      "learning_rate": 4.362903225806451e-05,
      "loss": 1.4705,
      "step": 3179
    },
    {
      "epoch": 8.536912751677852,
      "grad_norm": 0.09871678799390793,
      "learning_rate": 4.3548387096774194e-05,
      "loss": 1.3838,
      "step": 3180
    },
    {
      "epoch": 8.539597315436241,
      "grad_norm": 0.09664291143417358,
      "learning_rate": 4.346774193548387e-05,
      "loss": 1.4258,
      "step": 3181
    },
    {
      "epoch": 8.542281879194631,
      "grad_norm": 0.08223256468772888,
      "learning_rate": 4.338709677419355e-05,
      "loss": 1.468,
      "step": 3182
    },
    {
      "epoch": 8.54496644295302,
      "grad_norm": 0.10130548477172852,
      "learning_rate": 4.330645161290322e-05,
      "loss": 1.3785,
      "step": 3183
    },
    {
      "epoch": 8.54765100671141,
      "grad_norm": 0.09763625264167786,
      "learning_rate": 4.3225806451612894e-05,
      "loss": 1.4522,
      "step": 3184
    },
    {
      "epoch": 8.550335570469798,
      "grad_norm": 0.09275045245885849,
      "learning_rate": 4.314516129032258e-05,
      "loss": 1.3692,
      "step": 3185
    },
    {
      "epoch": 8.553020134228188,
      "grad_norm": 0.0861440896987915,
      "learning_rate": 4.3064516129032254e-05,
      "loss": 1.407,
      "step": 3186
    },
    {
      "epoch": 8.555704697986577,
      "grad_norm": 0.10793978720903397,
      "learning_rate": 4.298387096774193e-05,
      "loss": 1.4257,
      "step": 3187
    },
    {
      "epoch": 8.558389261744967,
      "grad_norm": 0.09869957715272903,
      "learning_rate": 4.290322580645161e-05,
      "loss": 1.4255,
      "step": 3188
    },
    {
      "epoch": 8.561073825503355,
      "grad_norm": 0.08860033005475998,
      "learning_rate": 4.282258064516128e-05,
      "loss": 1.3759,
      "step": 3189
    },
    {
      "epoch": 8.563758389261745,
      "grad_norm": 0.10519210249185562,
      "learning_rate": 4.274193548387097e-05,
      "loss": 1.4252,
      "step": 3190
    },
    {
      "epoch": 8.566442953020134,
      "grad_norm": 0.09729231148958206,
      "learning_rate": 4.266129032258064e-05,
      "loss": 1.4099,
      "step": 3191
    },
    {
      "epoch": 8.569127516778524,
      "grad_norm": 0.0957256630063057,
      "learning_rate": 4.258064516129032e-05,
      "loss": 1.3906,
      "step": 3192
    },
    {
      "epoch": 8.571812080536914,
      "grad_norm": 0.10685072094202042,
      "learning_rate": 4.2499999999999996e-05,
      "loss": 1.3565,
      "step": 3193
    },
    {
      "epoch": 8.574496644295301,
      "grad_norm": 0.09286502748727798,
      "learning_rate": 4.241935483870967e-05,
      "loss": 1.4317,
      "step": 3194
    },
    {
      "epoch": 8.577181208053691,
      "grad_norm": 0.0994953140616417,
      "learning_rate": 4.2338709677419356e-05,
      "loss": 1.4558,
      "step": 3195
    },
    {
      "epoch": 8.57986577181208,
      "grad_norm": 0.0914851501584053,
      "learning_rate": 4.225806451612903e-05,
      "loss": 1.4167,
      "step": 3196
    },
    {
      "epoch": 8.58255033557047,
      "grad_norm": 0.0945170596241951,
      "learning_rate": 4.217741935483871e-05,
      "loss": 1.5368,
      "step": 3197
    },
    {
      "epoch": 8.585234899328858,
      "grad_norm": 0.09347561001777649,
      "learning_rate": 4.2096774193548386e-05,
      "loss": 1.5162,
      "step": 3198
    },
    {
      "epoch": 8.587919463087248,
      "grad_norm": 0.08926736563444138,
      "learning_rate": 4.2016129032258056e-05,
      "loss": 1.4175,
      "step": 3199
    },
    {
      "epoch": 8.590604026845638,
      "grad_norm": 0.10085295140743256,
      "learning_rate": 4.193548387096773e-05,
      "loss": 1.5394,
      "step": 3200
    },
    {
      "epoch": 8.593288590604027,
      "grad_norm": 0.08541710674762726,
      "learning_rate": 4.1854838709677415e-05,
      "loss": 1.55,
      "step": 3201
    },
    {
      "epoch": 8.595973154362417,
      "grad_norm": 0.09678060561418533,
      "learning_rate": 4.177419354838709e-05,
      "loss": 1.5177,
      "step": 3202
    },
    {
      "epoch": 8.598657718120805,
      "grad_norm": 0.09265903383493423,
      "learning_rate": 4.169354838709677e-05,
      "loss": 1.3933,
      "step": 3203
    },
    {
      "epoch": 8.601342281879194,
      "grad_norm": 0.09639904648065567,
      "learning_rate": 4.1612903225806445e-05,
      "loss": 1.5029,
      "step": 3204
    },
    {
      "epoch": 8.604026845637584,
      "grad_norm": 0.08636409789323807,
      "learning_rate": 4.153225806451612e-05,
      "loss": 1.4348,
      "step": 3205
    },
    {
      "epoch": 8.606711409395974,
      "grad_norm": 0.09391187876462936,
      "learning_rate": 4.1451612903225805e-05,
      "loss": 1.4663,
      "step": 3206
    },
    {
      "epoch": 8.609395973154362,
      "grad_norm": 0.0940098911523819,
      "learning_rate": 4.137096774193548e-05,
      "loss": 1.4203,
      "step": 3207
    },
    {
      "epoch": 8.612080536912751,
      "grad_norm": 0.09117566794157028,
      "learning_rate": 4.129032258064516e-05,
      "loss": 1.4265,
      "step": 3208
    },
    {
      "epoch": 8.614765100671141,
      "grad_norm": 0.0986625924706459,
      "learning_rate": 4.1209677419354835e-05,
      "loss": 1.4747,
      "step": 3209
    },
    {
      "epoch": 8.61744966442953,
      "grad_norm": 0.09523756057024002,
      "learning_rate": 4.112903225806451e-05,
      "loss": 1.4062,
      "step": 3210
    },
    {
      "epoch": 8.620134228187919,
      "grad_norm": 0.0906400978565216,
      "learning_rate": 4.1048387096774195e-05,
      "loss": 1.5332,
      "step": 3211
    },
    {
      "epoch": 8.622818791946308,
      "grad_norm": 0.09183168411254883,
      "learning_rate": 4.096774193548387e-05,
      "loss": 1.4587,
      "step": 3212
    },
    {
      "epoch": 8.625503355704698,
      "grad_norm": 0.08838005363941193,
      "learning_rate": 4.088709677419355e-05,
      "loss": 1.4467,
      "step": 3213
    },
    {
      "epoch": 8.628187919463087,
      "grad_norm": 0.08322552591562271,
      "learning_rate": 4.0806451612903224e-05,
      "loss": 1.4904,
      "step": 3214
    },
    {
      "epoch": 8.630872483221477,
      "grad_norm": 0.09414006769657135,
      "learning_rate": 4.0725806451612894e-05,
      "loss": 1.426,
      "step": 3215
    },
    {
      "epoch": 8.633557046979865,
      "grad_norm": 0.09584540128707886,
      "learning_rate": 4.0645161290322584e-05,
      "loss": 1.4567,
      "step": 3216
    },
    {
      "epoch": 8.636241610738255,
      "grad_norm": 0.09348224103450775,
      "learning_rate": 4.0564516129032254e-05,
      "loss": 1.3751,
      "step": 3217
    },
    {
      "epoch": 8.638926174496644,
      "grad_norm": 0.09405846148729324,
      "learning_rate": 4.048387096774193e-05,
      "loss": 1.4269,
      "step": 3218
    },
    {
      "epoch": 8.641610738255034,
      "grad_norm": 0.09197147935628891,
      "learning_rate": 4.040322580645161e-05,
      "loss": 1.5248,
      "step": 3219
    },
    {
      "epoch": 8.644295302013422,
      "grad_norm": 0.08934608846902847,
      "learning_rate": 4.0322580645161284e-05,
      "loss": 1.5891,
      "step": 3220
    },
    {
      "epoch": 8.646979865771812,
      "grad_norm": 0.09127368032932281,
      "learning_rate": 4.024193548387096e-05,
      "loss": 1.4814,
      "step": 3221
    },
    {
      "epoch": 8.649664429530201,
      "grad_norm": 0.09358083456754684,
      "learning_rate": 4.0161290322580643e-05,
      "loss": 1.5485,
      "step": 3222
    },
    {
      "epoch": 8.65234899328859,
      "grad_norm": 0.09781703352928162,
      "learning_rate": 4.008064516129032e-05,
      "loss": 1.5096,
      "step": 3223
    },
    {
      "epoch": 8.65503355704698,
      "grad_norm": 0.09889128059148788,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 1.4481,
      "step": 3224
    },
    {
      "epoch": 8.657718120805368,
      "grad_norm": 0.10514426231384277,
      "learning_rate": 3.991935483870967e-05,
      "loss": 1.3791,
      "step": 3225
    },
    {
      "epoch": 8.660402684563758,
      "grad_norm": 0.09712469577789307,
      "learning_rate": 3.983870967741935e-05,
      "loss": 1.3826,
      "step": 3226
    },
    {
      "epoch": 8.663087248322148,
      "grad_norm": 0.09723390638828278,
      "learning_rate": 3.975806451612903e-05,
      "loss": 1.5504,
      "step": 3227
    },
    {
      "epoch": 8.665771812080537,
      "grad_norm": 0.09852392226457596,
      "learning_rate": 3.967741935483871e-05,
      "loss": 1.4175,
      "step": 3228
    },
    {
      "epoch": 8.668456375838925,
      "grad_norm": 0.10160036385059357,
      "learning_rate": 3.9596774193548386e-05,
      "loss": 1.4211,
      "step": 3229
    },
    {
      "epoch": 8.671140939597315,
      "grad_norm": 0.0946158915758133,
      "learning_rate": 3.951612903225806e-05,
      "loss": 1.4368,
      "step": 3230
    },
    {
      "epoch": 8.673825503355705,
      "grad_norm": 0.10405661165714264,
      "learning_rate": 3.943548387096773e-05,
      "loss": 1.3391,
      "step": 3231
    },
    {
      "epoch": 8.676510067114094,
      "grad_norm": 0.08649641275405884,
      "learning_rate": 3.935483870967742e-05,
      "loss": 1.4252,
      "step": 3232
    },
    {
      "epoch": 8.679194630872484,
      "grad_norm": 0.08672873675823212,
      "learning_rate": 3.927419354838709e-05,
      "loss": 1.4511,
      "step": 3233
    },
    {
      "epoch": 8.681879194630872,
      "grad_norm": 0.09223335981369019,
      "learning_rate": 3.919354838709677e-05,
      "loss": 1.4593,
      "step": 3234
    },
    {
      "epoch": 8.684563758389261,
      "grad_norm": 0.0872262567281723,
      "learning_rate": 3.9112903225806445e-05,
      "loss": 1.5063,
      "step": 3235
    },
    {
      "epoch": 8.687248322147651,
      "grad_norm": 0.10093309730291367,
      "learning_rate": 3.903225806451612e-05,
      "loss": 1.4316,
      "step": 3236
    },
    {
      "epoch": 8.68993288590604,
      "grad_norm": 0.0932748094201088,
      "learning_rate": 3.8951612903225805e-05,
      "loss": 1.4149,
      "step": 3237
    },
    {
      "epoch": 8.692617449664429,
      "grad_norm": 0.09238830953836441,
      "learning_rate": 3.887096774193548e-05,
      "loss": 1.4502,
      "step": 3238
    },
    {
      "epoch": 8.695302013422818,
      "grad_norm": 0.09416225552558899,
      "learning_rate": 3.879032258064516e-05,
      "loss": 1.4524,
      "step": 3239
    },
    {
      "epoch": 8.697986577181208,
      "grad_norm": 0.08589273691177368,
      "learning_rate": 3.8709677419354835e-05,
      "loss": 1.5625,
      "step": 3240
    },
    {
      "epoch": 8.700671140939598,
      "grad_norm": 0.08668211847543716,
      "learning_rate": 3.862903225806451e-05,
      "loss": 1.4765,
      "step": 3241
    },
    {
      "epoch": 8.703355704697987,
      "grad_norm": 0.1022607684135437,
      "learning_rate": 3.8548387096774195e-05,
      "loss": 1.4679,
      "step": 3242
    },
    {
      "epoch": 8.706040268456375,
      "grad_norm": 0.08610614389181137,
      "learning_rate": 3.846774193548387e-05,
      "loss": 1.5458,
      "step": 3243
    },
    {
      "epoch": 8.708724832214765,
      "grad_norm": 0.09576623141765594,
      "learning_rate": 3.838709677419355e-05,
      "loss": 1.4127,
      "step": 3244
    },
    {
      "epoch": 8.711409395973154,
      "grad_norm": 0.0931750163435936,
      "learning_rate": 3.8306451612903224e-05,
      "loss": 1.5076,
      "step": 3245
    },
    {
      "epoch": 8.714093959731544,
      "grad_norm": 0.09343397617340088,
      "learning_rate": 3.8225806451612894e-05,
      "loss": 1.4335,
      "step": 3246
    },
    {
      "epoch": 8.716778523489932,
      "grad_norm": 0.09331904351711273,
      "learning_rate": 3.814516129032257e-05,
      "loss": 1.457,
      "step": 3247
    },
    {
      "epoch": 8.719463087248322,
      "grad_norm": 0.10241834819316864,
      "learning_rate": 3.806451612903226e-05,
      "loss": 1.497,
      "step": 3248
    },
    {
      "epoch": 8.722147651006711,
      "grad_norm": 0.10424723476171494,
      "learning_rate": 3.798387096774193e-05,
      "loss": 1.3306,
      "step": 3249
    },
    {
      "epoch": 8.724832214765101,
      "grad_norm": 0.09228238463401794,
      "learning_rate": 3.790322580645161e-05,
      "loss": 1.429,
      "step": 3250
    },
    {
      "epoch": 8.72751677852349,
      "grad_norm": 0.10973688960075378,
      "learning_rate": 3.7822580645161284e-05,
      "loss": 1.3624,
      "step": 3251
    },
    {
      "epoch": 8.730201342281878,
      "grad_norm": 0.09449731558561325,
      "learning_rate": 3.774193548387096e-05,
      "loss": 1.404,
      "step": 3252
    },
    {
      "epoch": 8.732885906040268,
      "grad_norm": 0.10094578564167023,
      "learning_rate": 3.7661290322580644e-05,
      "loss": 1.3261,
      "step": 3253
    },
    {
      "epoch": 8.735570469798658,
      "grad_norm": 0.09755903482437134,
      "learning_rate": 3.758064516129032e-05,
      "loss": 1.4351,
      "step": 3254
    },
    {
      "epoch": 8.738255033557047,
      "grad_norm": 0.0950995683670044,
      "learning_rate": 3.75e-05,
      "loss": 1.4792,
      "step": 3255
    },
    {
      "epoch": 8.740939597315435,
      "grad_norm": 0.09864787012338638,
      "learning_rate": 3.741935483870967e-05,
      "loss": 1.4197,
      "step": 3256
    },
    {
      "epoch": 8.743624161073825,
      "grad_norm": 0.10151447355747223,
      "learning_rate": 3.733870967741935e-05,
      "loss": 1.4754,
      "step": 3257
    },
    {
      "epoch": 8.746308724832215,
      "grad_norm": 0.09268958866596222,
      "learning_rate": 3.7258064516129026e-05,
      "loss": 1.4956,
      "step": 3258
    },
    {
      "epoch": 8.748993288590604,
      "grad_norm": 0.0919404849410057,
      "learning_rate": 3.717741935483871e-05,
      "loss": 1.5116,
      "step": 3259
    },
    {
      "epoch": 8.751677852348994,
      "grad_norm": 0.08513502031564713,
      "learning_rate": 3.7096774193548386e-05,
      "loss": 1.4859,
      "step": 3260
    },
    {
      "epoch": 8.754362416107382,
      "grad_norm": 0.09805700182914734,
      "learning_rate": 3.701612903225806e-05,
      "loss": 1.4269,
      "step": 3261
    },
    {
      "epoch": 8.757046979865772,
      "grad_norm": 0.09172728657722473,
      "learning_rate": 3.693548387096774e-05,
      "loss": 1.5647,
      "step": 3262
    },
    {
      "epoch": 8.759731543624161,
      "grad_norm": 0.10663951933383942,
      "learning_rate": 3.6854838709677416e-05,
      "loss": 1.4513,
      "step": 3263
    },
    {
      "epoch": 8.76241610738255,
      "grad_norm": 0.09702452272176743,
      "learning_rate": 3.677419354838709e-05,
      "loss": 1.4638,
      "step": 3264
    },
    {
      "epoch": 8.765100671140939,
      "grad_norm": 0.11819620430469513,
      "learning_rate": 3.669354838709677e-05,
      "loss": 1.2991,
      "step": 3265
    },
    {
      "epoch": 8.767785234899328,
      "grad_norm": 0.09527378529310226,
      "learning_rate": 3.6612903225806446e-05,
      "loss": 1.4655,
      "step": 3266
    },
    {
      "epoch": 8.770469798657718,
      "grad_norm": 0.10292436927556992,
      "learning_rate": 3.653225806451613e-05,
      "loss": 1.3577,
      "step": 3267
    },
    {
      "epoch": 8.773154362416108,
      "grad_norm": 0.09169462323188782,
      "learning_rate": 3.6451612903225805e-05,
      "loss": 1.432,
      "step": 3268
    },
    {
      "epoch": 8.775838926174497,
      "grad_norm": 0.1046154797077179,
      "learning_rate": 3.637096774193548e-05,
      "loss": 1.4013,
      "step": 3269
    },
    {
      "epoch": 8.778523489932885,
      "grad_norm": 0.11670834571123123,
      "learning_rate": 3.629032258064516e-05,
      "loss": 1.4073,
      "step": 3270
    },
    {
      "epoch": 8.781208053691275,
      "grad_norm": 0.096376433968544,
      "learning_rate": 3.6209677419354835e-05,
      "loss": 1.5192,
      "step": 3271
    },
    {
      "epoch": 8.783892617449665,
      "grad_norm": 0.08847209811210632,
      "learning_rate": 3.612903225806451e-05,
      "loss": 1.4909,
      "step": 3272
    },
    {
      "epoch": 8.786577181208054,
      "grad_norm": 0.09705258905887604,
      "learning_rate": 3.604838709677419e-05,
      "loss": 1.3823,
      "step": 3273
    },
    {
      "epoch": 8.789261744966442,
      "grad_norm": 0.09401097148656845,
      "learning_rate": 3.5967741935483865e-05,
      "loss": 1.4076,
      "step": 3274
    },
    {
      "epoch": 8.791946308724832,
      "grad_norm": 0.08694740384817123,
      "learning_rate": 3.588709677419355e-05,
      "loss": 1.5331,
      "step": 3275
    },
    {
      "epoch": 8.794630872483221,
      "grad_norm": 0.09567277878522873,
      "learning_rate": 3.5806451612903225e-05,
      "loss": 1.5206,
      "step": 3276
    },
    {
      "epoch": 8.797315436241611,
      "grad_norm": 0.0934499129652977,
      "learning_rate": 3.57258064516129e-05,
      "loss": 1.3919,
      "step": 3277
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.08924303203821182,
      "learning_rate": 3.564516129032258e-05,
      "loss": 1.5411,
      "step": 3278
    },
    {
      "epoch": 8.802684563758389,
      "grad_norm": 0.10607729107141495,
      "learning_rate": 3.5564516129032254e-05,
      "loss": 1.3137,
      "step": 3279
    },
    {
      "epoch": 8.805369127516778,
      "grad_norm": 0.09559696912765503,
      "learning_rate": 3.548387096774193e-05,
      "loss": 1.507,
      "step": 3280
    },
    {
      "epoch": 8.808053691275168,
      "grad_norm": 0.09645311534404755,
      "learning_rate": 3.540322580645161e-05,
      "loss": 1.3487,
      "step": 3281
    },
    {
      "epoch": 8.810738255033558,
      "grad_norm": 0.09193920344114304,
      "learning_rate": 3.5322580645161284e-05,
      "loss": 1.5029,
      "step": 3282
    },
    {
      "epoch": 8.813422818791945,
      "grad_norm": 0.10121393948793411,
      "learning_rate": 3.524193548387097e-05,
      "loss": 1.4033,
      "step": 3283
    },
    {
      "epoch": 8.816107382550335,
      "grad_norm": 0.09476502239704132,
      "learning_rate": 3.5161290322580644e-05,
      "loss": 1.5708,
      "step": 3284
    },
    {
      "epoch": 8.818791946308725,
      "grad_norm": 0.09671561419963837,
      "learning_rate": 3.508064516129032e-05,
      "loss": 1.4493,
      "step": 3285
    },
    {
      "epoch": 8.821476510067114,
      "grad_norm": 0.10943640768527985,
      "learning_rate": 3.5e-05,
      "loss": 1.3872,
      "step": 3286
    },
    {
      "epoch": 8.824161073825504,
      "grad_norm": 0.08580826967954636,
      "learning_rate": 3.4919354838709673e-05,
      "loss": 1.4232,
      "step": 3287
    },
    {
      "epoch": 8.826845637583892,
      "grad_norm": 0.08880149573087692,
      "learning_rate": 3.483870967741935e-05,
      "loss": 1.5154,
      "step": 3288
    },
    {
      "epoch": 8.829530201342282,
      "grad_norm": 0.09939050674438477,
      "learning_rate": 3.4758064516129027e-05,
      "loss": 1.4446,
      "step": 3289
    },
    {
      "epoch": 8.832214765100671,
      "grad_norm": 0.11279751360416412,
      "learning_rate": 3.46774193548387e-05,
      "loss": 1.409,
      "step": 3290
    },
    {
      "epoch": 8.834899328859061,
      "grad_norm": 0.10438551008701324,
      "learning_rate": 3.4596774193548386e-05,
      "loss": 1.3582,
      "step": 3291
    },
    {
      "epoch": 8.837583892617449,
      "grad_norm": 0.09742490202188492,
      "learning_rate": 3.451612903225806e-05,
      "loss": 1.4399,
      "step": 3292
    },
    {
      "epoch": 8.840268456375838,
      "grad_norm": 0.09319416433572769,
      "learning_rate": 3.443548387096774e-05,
      "loss": 1.2657,
      "step": 3293
    },
    {
      "epoch": 8.842953020134228,
      "grad_norm": 0.10530108213424683,
      "learning_rate": 3.4354838709677416e-05,
      "loss": 1.5394,
      "step": 3294
    },
    {
      "epoch": 8.845637583892618,
      "grad_norm": 0.09375704079866409,
      "learning_rate": 3.427419354838709e-05,
      "loss": 1.4523,
      "step": 3295
    },
    {
      "epoch": 8.848322147651007,
      "grad_norm": 0.09275157004594803,
      "learning_rate": 3.419354838709677e-05,
      "loss": 1.5278,
      "step": 3296
    },
    {
      "epoch": 8.851006711409395,
      "grad_norm": 0.10338970273733139,
      "learning_rate": 3.4112903225806446e-05,
      "loss": 1.407,
      "step": 3297
    },
    {
      "epoch": 8.853691275167785,
      "grad_norm": 0.09736590087413788,
      "learning_rate": 3.403225806451612e-05,
      "loss": 1.5137,
      "step": 3298
    },
    {
      "epoch": 8.856375838926175,
      "grad_norm": 0.09550166875123978,
      "learning_rate": 3.3951612903225806e-05,
      "loss": 1.454,
      "step": 3299
    },
    {
      "epoch": 8.859060402684564,
      "grad_norm": 0.09618939459323883,
      "learning_rate": 3.387096774193548e-05,
      "loss": 1.3644,
      "step": 3300
    },
    {
      "epoch": 8.861744966442952,
      "grad_norm": 0.09160218387842178,
      "learning_rate": 3.379032258064516e-05,
      "loss": 1.43,
      "step": 3301
    },
    {
      "epoch": 8.864429530201342,
      "grad_norm": 0.08847614377737045,
      "learning_rate": 3.3709677419354835e-05,
      "loss": 1.4612,
      "step": 3302
    },
    {
      "epoch": 8.867114093959731,
      "grad_norm": 0.08825657516717911,
      "learning_rate": 3.362903225806451e-05,
      "loss": 1.4794,
      "step": 3303
    },
    {
      "epoch": 8.869798657718121,
      "grad_norm": 0.09291362017393112,
      "learning_rate": 3.354838709677419e-05,
      "loss": 1.5721,
      "step": 3304
    },
    {
      "epoch": 8.87248322147651,
      "grad_norm": 0.10545022785663605,
      "learning_rate": 3.3467741935483865e-05,
      "loss": 1.3919,
      "step": 3305
    },
    {
      "epoch": 8.875167785234899,
      "grad_norm": 0.11171681433916092,
      "learning_rate": 3.338709677419355e-05,
      "loss": 1.423,
      "step": 3306
    },
    {
      "epoch": 8.877852348993288,
      "grad_norm": 0.10317724943161011,
      "learning_rate": 3.3306451612903225e-05,
      "loss": 1.4088,
      "step": 3307
    },
    {
      "epoch": 8.880536912751678,
      "grad_norm": 0.11285144835710526,
      "learning_rate": 3.32258064516129e-05,
      "loss": 1.2925,
      "step": 3308
    },
    {
      "epoch": 8.883221476510068,
      "grad_norm": 0.09588152915239334,
      "learning_rate": 3.314516129032258e-05,
      "loss": 1.4064,
      "step": 3309
    },
    {
      "epoch": 8.885906040268456,
      "grad_norm": 0.10440490394830704,
      "learning_rate": 3.3064516129032255e-05,
      "loss": 1.4594,
      "step": 3310
    },
    {
      "epoch": 8.888590604026845,
      "grad_norm": 0.10674012452363968,
      "learning_rate": 3.298387096774193e-05,
      "loss": 1.4154,
      "step": 3311
    },
    {
      "epoch": 8.891275167785235,
      "grad_norm": 0.10116652399301529,
      "learning_rate": 3.290322580645161e-05,
      "loss": 1.4792,
      "step": 3312
    },
    {
      "epoch": 8.893959731543625,
      "grad_norm": 0.0836956575512886,
      "learning_rate": 3.2822580645161284e-05,
      "loss": 1.5431,
      "step": 3313
    },
    {
      "epoch": 8.896644295302014,
      "grad_norm": 0.09956232458353043,
      "learning_rate": 3.274193548387097e-05,
      "loss": 1.4603,
      "step": 3314
    },
    {
      "epoch": 8.899328859060402,
      "grad_norm": 0.10594089329242706,
      "learning_rate": 3.2661290322580644e-05,
      "loss": 1.4965,
      "step": 3315
    },
    {
      "epoch": 8.902013422818792,
      "grad_norm": 0.09435036033391953,
      "learning_rate": 3.258064516129032e-05,
      "loss": 1.4958,
      "step": 3316
    },
    {
      "epoch": 8.904697986577181,
      "grad_norm": 0.10099297016859055,
      "learning_rate": 3.25e-05,
      "loss": 1.3767,
      "step": 3317
    },
    {
      "epoch": 8.907382550335571,
      "grad_norm": 0.0982748493552208,
      "learning_rate": 3.2419354838709674e-05,
      "loss": 1.4585,
      "step": 3318
    },
    {
      "epoch": 8.910067114093959,
      "grad_norm": 0.08238261938095093,
      "learning_rate": 3.233870967741935e-05,
      "loss": 1.505,
      "step": 3319
    },
    {
      "epoch": 8.912751677852349,
      "grad_norm": 0.09248784184455872,
      "learning_rate": 3.225806451612903e-05,
      "loss": 1.6409,
      "step": 3320
    },
    {
      "epoch": 8.915436241610738,
      "grad_norm": 0.09002387523651123,
      "learning_rate": 3.21774193548387e-05,
      "loss": 1.5005,
      "step": 3321
    },
    {
      "epoch": 8.918120805369128,
      "grad_norm": 0.0977778285741806,
      "learning_rate": 3.209677419354839e-05,
      "loss": 1.3993,
      "step": 3322
    },
    {
      "epoch": 8.920805369127518,
      "grad_norm": 0.0897379145026207,
      "learning_rate": 3.201612903225806e-05,
      "loss": 1.4759,
      "step": 3323
    },
    {
      "epoch": 8.923489932885905,
      "grad_norm": 0.1010027751326561,
      "learning_rate": 3.193548387096774e-05,
      "loss": 1.4749,
      "step": 3324
    },
    {
      "epoch": 8.926174496644295,
      "grad_norm": 0.09743721038103104,
      "learning_rate": 3.1854838709677416e-05,
      "loss": 1.4285,
      "step": 3325
    },
    {
      "epoch": 8.928859060402685,
      "grad_norm": 0.09330356866121292,
      "learning_rate": 3.177419354838709e-05,
      "loss": 1.4426,
      "step": 3326
    },
    {
      "epoch": 8.931543624161074,
      "grad_norm": 0.08579612523317337,
      "learning_rate": 3.1693548387096776e-05,
      "loss": 1.5412,
      "step": 3327
    },
    {
      "epoch": 8.934228187919462,
      "grad_norm": 0.09513258934020996,
      "learning_rate": 3.1612903225806446e-05,
      "loss": 1.4391,
      "step": 3328
    },
    {
      "epoch": 8.936912751677852,
      "grad_norm": 0.0948210209608078,
      "learning_rate": 3.153225806451612e-05,
      "loss": 1.4656,
      "step": 3329
    },
    {
      "epoch": 8.939597315436242,
      "grad_norm": 0.08974344283342361,
      "learning_rate": 3.1451612903225806e-05,
      "loss": 1.3457,
      "step": 3330
    },
    {
      "epoch": 8.942281879194631,
      "grad_norm": 0.09458904713392258,
      "learning_rate": 3.137096774193548e-05,
      "loss": 1.4405,
      "step": 3331
    },
    {
      "epoch": 8.944966442953021,
      "grad_norm": 0.09880674630403519,
      "learning_rate": 3.129032258064516e-05,
      "loss": 1.3775,
      "step": 3332
    },
    {
      "epoch": 8.947651006711409,
      "grad_norm": 0.08178868889808655,
      "learning_rate": 3.1209677419354836e-05,
      "loss": 1.5629,
      "step": 3333
    },
    {
      "epoch": 8.950335570469798,
      "grad_norm": 0.0941295400261879,
      "learning_rate": 3.112903225806451e-05,
      "loss": 1.4193,
      "step": 3334
    },
    {
      "epoch": 8.953020134228188,
      "grad_norm": 0.0900832936167717,
      "learning_rate": 3.1048387096774195e-05,
      "loss": 1.4875,
      "step": 3335
    },
    {
      "epoch": 8.955704697986578,
      "grad_norm": 0.1023406982421875,
      "learning_rate": 3.0967741935483865e-05,
      "loss": 1.3406,
      "step": 3336
    },
    {
      "epoch": 8.958389261744966,
      "grad_norm": 0.08681508898735046,
      "learning_rate": 3.088709677419354e-05,
      "loss": 1.4759,
      "step": 3337
    },
    {
      "epoch": 8.961073825503355,
      "grad_norm": 0.0974709689617157,
      "learning_rate": 3.0806451612903225e-05,
      "loss": 1.4719,
      "step": 3338
    },
    {
      "epoch": 8.963758389261745,
      "grad_norm": 0.08885912597179413,
      "learning_rate": 3.07258064516129e-05,
      "loss": 1.4791,
      "step": 3339
    },
    {
      "epoch": 8.966442953020135,
      "grad_norm": 0.08495999872684479,
      "learning_rate": 3.064516129032258e-05,
      "loss": 1.5275,
      "step": 3340
    },
    {
      "epoch": 8.969127516778524,
      "grad_norm": 0.09042219072580338,
      "learning_rate": 3.0564516129032255e-05,
      "loss": 1.4912,
      "step": 3341
    },
    {
      "epoch": 8.971812080536912,
      "grad_norm": 0.09030316770076752,
      "learning_rate": 3.048387096774193e-05,
      "loss": 1.4544,
      "step": 3342
    },
    {
      "epoch": 8.974496644295302,
      "grad_norm": 0.08801145851612091,
      "learning_rate": 3.040322580645161e-05,
      "loss": 1.4351,
      "step": 3343
    },
    {
      "epoch": 8.977181208053691,
      "grad_norm": 0.10270121693611145,
      "learning_rate": 3.0322580645161288e-05,
      "loss": 1.34,
      "step": 3344
    },
    {
      "epoch": 8.979865771812081,
      "grad_norm": 0.08533000200986862,
      "learning_rate": 3.0241935483870964e-05,
      "loss": 1.5328,
      "step": 3345
    },
    {
      "epoch": 8.982550335570469,
      "grad_norm": 0.08834477514028549,
      "learning_rate": 3.0161290322580644e-05,
      "loss": 1.5138,
      "step": 3346
    },
    {
      "epoch": 8.985234899328859,
      "grad_norm": 0.08790010213851929,
      "learning_rate": 3.0080645161290317e-05,
      "loss": 1.4354,
      "step": 3347
    },
    {
      "epoch": 8.987919463087248,
      "grad_norm": 0.10797532647848129,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.3429,
      "step": 3348
    },
    {
      "epoch": 8.990604026845638,
      "grad_norm": 0.09215761721134186,
      "learning_rate": 2.9919354838709674e-05,
      "loss": 1.5422,
      "step": 3349
    },
    {
      "epoch": 8.993288590604028,
      "grad_norm": 0.09280902147293091,
      "learning_rate": 2.983870967741935e-05,
      "loss": 1.5457,
      "step": 3350
    },
    {
      "epoch": 8.995973154362416,
      "grad_norm": 0.10339868068695068,
      "learning_rate": 2.975806451612903e-05,
      "loss": 1.4881,
      "step": 3351
    },
    {
      "epoch": 8.998657718120805,
      "grad_norm": 0.08920740336179733,
      "learning_rate": 2.9677419354838707e-05,
      "loss": 1.53,
      "step": 3352
    },
    {
      "epoch": 9.001342281879195,
      "grad_norm": 0.10125336796045303,
      "learning_rate": 2.9596774193548387e-05,
      "loss": 1.4833,
      "step": 3353
    },
    {
      "epoch": 9.004026845637584,
      "grad_norm": 0.1113290935754776,
      "learning_rate": 2.9516129032258063e-05,
      "loss": 1.4959,
      "step": 3354
    },
    {
      "epoch": 9.006711409395972,
      "grad_norm": 0.09552459418773651,
      "learning_rate": 2.9435483870967737e-05,
      "loss": 1.5763,
      "step": 3355
    },
    {
      "epoch": 9.009395973154362,
      "grad_norm": 0.09250519424676895,
      "learning_rate": 2.9354838709677417e-05,
      "loss": 1.4983,
      "step": 3356
    },
    {
      "epoch": 9.012080536912752,
      "grad_norm": 0.10919945687055588,
      "learning_rate": 2.9274193548387093e-05,
      "loss": 1.3951,
      "step": 3357
    },
    {
      "epoch": 9.014765100671141,
      "grad_norm": 0.10477890074253082,
      "learning_rate": 2.919354838709677e-05,
      "loss": 1.3063,
      "step": 3358
    },
    {
      "epoch": 9.017449664429531,
      "grad_norm": 0.09243518859148026,
      "learning_rate": 2.911290322580645e-05,
      "loss": 1.5062,
      "step": 3359
    },
    {
      "epoch": 9.020134228187919,
      "grad_norm": 0.09707822650671005,
      "learning_rate": 2.9032258064516126e-05,
      "loss": 1.4873,
      "step": 3360
    },
    {
      "epoch": 9.022818791946309,
      "grad_norm": 0.09671064466238022,
      "learning_rate": 2.8951612903225806e-05,
      "loss": 1.4492,
      "step": 3361
    },
    {
      "epoch": 9.025503355704698,
      "grad_norm": 0.09127439558506012,
      "learning_rate": 2.8870967741935483e-05,
      "loss": 1.4994,
      "step": 3362
    },
    {
      "epoch": 9.028187919463088,
      "grad_norm": 0.09946024417877197,
      "learning_rate": 2.8790322580645156e-05,
      "loss": 1.4751,
      "step": 3363
    },
    {
      "epoch": 9.030872483221476,
      "grad_norm": 0.10558080673217773,
      "learning_rate": 2.8709677419354836e-05,
      "loss": 1.4538,
      "step": 3364
    },
    {
      "epoch": 9.033557046979865,
      "grad_norm": 0.08904752135276794,
      "learning_rate": 2.8629032258064512e-05,
      "loss": 1.5928,
      "step": 3365
    },
    {
      "epoch": 9.036241610738255,
      "grad_norm": 0.09757561981678009,
      "learning_rate": 2.854838709677419e-05,
      "loss": 1.3637,
      "step": 3366
    },
    {
      "epoch": 9.038926174496645,
      "grad_norm": 0.09640330821275711,
      "learning_rate": 2.846774193548387e-05,
      "loss": 1.4229,
      "step": 3367
    },
    {
      "epoch": 9.041610738255034,
      "grad_norm": 0.09650007635354996,
      "learning_rate": 2.8387096774193545e-05,
      "loss": 1.4253,
      "step": 3368
    },
    {
      "epoch": 9.044295302013422,
      "grad_norm": 0.09138926863670349,
      "learning_rate": 2.8306451612903225e-05,
      "loss": 1.4325,
      "step": 3369
    },
    {
      "epoch": 9.046979865771812,
      "grad_norm": 0.08201885968446732,
      "learning_rate": 2.8225806451612902e-05,
      "loss": 1.504,
      "step": 3370
    },
    {
      "epoch": 9.049664429530202,
      "grad_norm": 0.0930318683385849,
      "learning_rate": 2.8145161290322575e-05,
      "loss": 1.4918,
      "step": 3371
    },
    {
      "epoch": 9.052348993288591,
      "grad_norm": 0.09404178708791733,
      "learning_rate": 2.8064516129032255e-05,
      "loss": 1.5208,
      "step": 3372
    },
    {
      "epoch": 9.055033557046979,
      "grad_norm": 0.10325151681900024,
      "learning_rate": 2.798387096774193e-05,
      "loss": 1.515,
      "step": 3373
    },
    {
      "epoch": 9.057718120805369,
      "grad_norm": 0.09441368281841278,
      "learning_rate": 2.790322580645161e-05,
      "loss": 1.3906,
      "step": 3374
    },
    {
      "epoch": 9.060402684563758,
      "grad_norm": 0.09396148473024368,
      "learning_rate": 2.7822580645161288e-05,
      "loss": 1.459,
      "step": 3375
    },
    {
      "epoch": 9.063087248322148,
      "grad_norm": 0.09892367571592331,
      "learning_rate": 2.7741935483870965e-05,
      "loss": 1.5002,
      "step": 3376
    },
    {
      "epoch": 9.065771812080538,
      "grad_norm": 0.09252110868692398,
      "learning_rate": 2.7661290322580644e-05,
      "loss": 1.438,
      "step": 3377
    },
    {
      "epoch": 9.068456375838926,
      "grad_norm": 0.08523678034543991,
      "learning_rate": 2.758064516129032e-05,
      "loss": 1.4208,
      "step": 3378
    },
    {
      "epoch": 9.071140939597315,
      "grad_norm": 0.10282532870769501,
      "learning_rate": 2.7499999999999994e-05,
      "loss": 1.4737,
      "step": 3379
    },
    {
      "epoch": 9.073825503355705,
      "grad_norm": 0.0960964635014534,
      "learning_rate": 2.7419354838709674e-05,
      "loss": 1.4724,
      "step": 3380
    },
    {
      "epoch": 9.076510067114095,
      "grad_norm": 0.08313233405351639,
      "learning_rate": 2.733870967741935e-05,
      "loss": 1.5986,
      "step": 3381
    },
    {
      "epoch": 9.079194630872482,
      "grad_norm": 0.0973910391330719,
      "learning_rate": 2.725806451612903e-05,
      "loss": 1.5263,
      "step": 3382
    },
    {
      "epoch": 9.081879194630872,
      "grad_norm": 0.09622343629598618,
      "learning_rate": 2.7177419354838707e-05,
      "loss": 1.4849,
      "step": 3383
    },
    {
      "epoch": 9.084563758389262,
      "grad_norm": 0.10128512233495712,
      "learning_rate": 2.7096774193548384e-05,
      "loss": 1.5478,
      "step": 3384
    },
    {
      "epoch": 9.087248322147651,
      "grad_norm": 0.09832268953323364,
      "learning_rate": 2.7016129032258064e-05,
      "loss": 1.5416,
      "step": 3385
    },
    {
      "epoch": 9.089932885906041,
      "grad_norm": 0.09104938805103302,
      "learning_rate": 2.693548387096774e-05,
      "loss": 1.4175,
      "step": 3386
    },
    {
      "epoch": 9.092617449664429,
      "grad_norm": 0.09262008219957352,
      "learning_rate": 2.685483870967742e-05,
      "loss": 1.4507,
      "step": 3387
    },
    {
      "epoch": 9.095302013422819,
      "grad_norm": 0.0895199328660965,
      "learning_rate": 2.6774193548387093e-05,
      "loss": 1.439,
      "step": 3388
    },
    {
      "epoch": 9.097986577181208,
      "grad_norm": 0.09303204715251923,
      "learning_rate": 2.669354838709677e-05,
      "loss": 1.3957,
      "step": 3389
    },
    {
      "epoch": 9.100671140939598,
      "grad_norm": 0.09293287992477417,
      "learning_rate": 2.661290322580645e-05,
      "loss": 1.4134,
      "step": 3390
    },
    {
      "epoch": 9.103355704697986,
      "grad_norm": 0.09094355255365372,
      "learning_rate": 2.6532258064516126e-05,
      "loss": 1.4855,
      "step": 3391
    },
    {
      "epoch": 9.106040268456375,
      "grad_norm": 0.09718787670135498,
      "learning_rate": 2.6451612903225803e-05,
      "loss": 1.3253,
      "step": 3392
    },
    {
      "epoch": 9.108724832214765,
      "grad_norm": 0.08818309754133224,
      "learning_rate": 2.6370967741935483e-05,
      "loss": 1.5265,
      "step": 3393
    },
    {
      "epoch": 9.111409395973155,
      "grad_norm": 0.09431859105825424,
      "learning_rate": 2.629032258064516e-05,
      "loss": 1.432,
      "step": 3394
    },
    {
      "epoch": 9.114093959731544,
      "grad_norm": 0.09002631902694702,
      "learning_rate": 2.620967741935484e-05,
      "loss": 1.4779,
      "step": 3395
    },
    {
      "epoch": 9.116778523489932,
      "grad_norm": 0.09522673487663269,
      "learning_rate": 2.6129032258064513e-05,
      "loss": 1.3559,
      "step": 3396
    },
    {
      "epoch": 9.119463087248322,
      "grad_norm": 0.08593372255563736,
      "learning_rate": 2.604838709677419e-05,
      "loss": 1.4308,
      "step": 3397
    },
    {
      "epoch": 9.122147651006712,
      "grad_norm": 0.08553901314735413,
      "learning_rate": 2.596774193548387e-05,
      "loss": 1.5028,
      "step": 3398
    },
    {
      "epoch": 9.124832214765101,
      "grad_norm": 0.09256401658058167,
      "learning_rate": 2.5887096774193546e-05,
      "loss": 1.6298,
      "step": 3399
    },
    {
      "epoch": 9.12751677852349,
      "grad_norm": 0.08964172005653381,
      "learning_rate": 2.5806451612903226e-05,
      "loss": 1.5716,
      "step": 3400
    }
  ],
  "logging_steps": 1,
  "max_steps": 3720,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 200,
  "total_flos": 5.072194828472156e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
