{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.49933598937583,
  "eval_steps": 500,
  "global_step": 800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010624169986719787,
      "grad_norm": 0.3279721736907959,
      "learning_rate": 0.00029968085106382974,
      "loss": 3.1781,
      "step": 1
    },
    {
      "epoch": 0.021248339973439574,
      "grad_norm": 0.36583197116851807,
      "learning_rate": 0.00029936170212765956,
      "loss": 3.4337,
      "step": 2
    },
    {
      "epoch": 0.03187250996015936,
      "grad_norm": 0.3695668578147888,
      "learning_rate": 0.0002990425531914893,
      "loss": 3.1011,
      "step": 3
    },
    {
      "epoch": 0.04249667994687915,
      "grad_norm": 0.42307087779045105,
      "learning_rate": 0.00029872340425531914,
      "loss": 3.0929,
      "step": 4
    },
    {
      "epoch": 0.05312084993359894,
      "grad_norm": 0.49101606011390686,
      "learning_rate": 0.0002984042553191489,
      "loss": 3.1404,
      "step": 5
    },
    {
      "epoch": 0.06374501992031872,
      "grad_norm": 0.6288477182388306,
      "learning_rate": 0.0002980851063829787,
      "loss": 3.0252,
      "step": 6
    },
    {
      "epoch": 0.07436918990703852,
      "grad_norm": 0.7282926440238953,
      "learning_rate": 0.0002977659574468085,
      "loss": 3.0258,
      "step": 7
    },
    {
      "epoch": 0.0849933598937583,
      "grad_norm": 0.6832836270332336,
      "learning_rate": 0.00029744680851063825,
      "loss": 2.9132,
      "step": 8
    },
    {
      "epoch": 0.09561752988047809,
      "grad_norm": 0.44626370072364807,
      "learning_rate": 0.00029712765957446807,
      "loss": 2.8467,
      "step": 9
    },
    {
      "epoch": 0.10624169986719788,
      "grad_norm": 0.2913471758365631,
      "learning_rate": 0.00029680851063829784,
      "loss": 2.7505,
      "step": 10
    },
    {
      "epoch": 0.11686586985391766,
      "grad_norm": 0.20893539488315582,
      "learning_rate": 0.00029648936170212765,
      "loss": 2.7182,
      "step": 11
    },
    {
      "epoch": 0.12749003984063745,
      "grad_norm": 0.29709652066230774,
      "learning_rate": 0.0002961702127659574,
      "loss": 2.8614,
      "step": 12
    },
    {
      "epoch": 0.13811420982735723,
      "grad_norm": 0.2982236444950104,
      "learning_rate": 0.0002958510638297872,
      "loss": 2.7424,
      "step": 13
    },
    {
      "epoch": 0.14873837981407703,
      "grad_norm": 0.35820886492729187,
      "learning_rate": 0.000295531914893617,
      "loss": 2.6307,
      "step": 14
    },
    {
      "epoch": 0.1593625498007968,
      "grad_norm": 0.47862401604652405,
      "learning_rate": 0.00029521276595744677,
      "loss": 2.71,
      "step": 15
    },
    {
      "epoch": 0.1699867197875166,
      "grad_norm": 0.44093382358551025,
      "learning_rate": 0.0002948936170212766,
      "loss": 2.6968,
      "step": 16
    },
    {
      "epoch": 0.1806108897742364,
      "grad_norm": 0.4603285491466522,
      "learning_rate": 0.00029457446808510635,
      "loss": 2.6827,
      "step": 17
    },
    {
      "epoch": 0.19123505976095617,
      "grad_norm": 0.5755305886268616,
      "learning_rate": 0.00029425531914893617,
      "loss": 2.7388,
      "step": 18
    },
    {
      "epoch": 0.20185922974767595,
      "grad_norm": 0.5908556580543518,
      "learning_rate": 0.00029393617021276593,
      "loss": 2.6262,
      "step": 19
    },
    {
      "epoch": 0.21248339973439576,
      "grad_norm": 0.5639699101448059,
      "learning_rate": 0.0002936170212765957,
      "loss": 2.4773,
      "step": 20
    },
    {
      "epoch": 0.22310756972111553,
      "grad_norm": 0.5304442644119263,
      "learning_rate": 0.0002932978723404255,
      "loss": 2.6283,
      "step": 21
    },
    {
      "epoch": 0.2337317397078353,
      "grad_norm": 0.3991209864616394,
      "learning_rate": 0.0002929787234042553,
      "loss": 2.5711,
      "step": 22
    },
    {
      "epoch": 0.24435590969455512,
      "grad_norm": 0.31719374656677246,
      "learning_rate": 0.0002926595744680851,
      "loss": 2.3263,
      "step": 23
    },
    {
      "epoch": 0.2549800796812749,
      "grad_norm": 0.28653839230537415,
      "learning_rate": 0.00029234042553191486,
      "loss": 2.4867,
      "step": 24
    },
    {
      "epoch": 0.2656042496679947,
      "grad_norm": 0.2587491273880005,
      "learning_rate": 0.0002920212765957447,
      "loss": 2.4734,
      "step": 25
    },
    {
      "epoch": 0.27622841965471445,
      "grad_norm": 0.2265876680612564,
      "learning_rate": 0.00029170212765957445,
      "loss": 2.4225,
      "step": 26
    },
    {
      "epoch": 0.2868525896414343,
      "grad_norm": 0.23168301582336426,
      "learning_rate": 0.0002913829787234042,
      "loss": 2.2656,
      "step": 27
    },
    {
      "epoch": 0.29747675962815406,
      "grad_norm": 0.23805607855319977,
      "learning_rate": 0.00029106382978723403,
      "loss": 2.346,
      "step": 28
    },
    {
      "epoch": 0.30810092961487384,
      "grad_norm": 0.2342095971107483,
      "learning_rate": 0.0002907446808510638,
      "loss": 2.2262,
      "step": 29
    },
    {
      "epoch": 0.3187250996015936,
      "grad_norm": 0.24530459940433502,
      "learning_rate": 0.0002904255319148936,
      "loss": 2.3789,
      "step": 30
    },
    {
      "epoch": 0.3293492695883134,
      "grad_norm": 0.2885332703590393,
      "learning_rate": 0.0002901063829787234,
      "loss": 2.3045,
      "step": 31
    },
    {
      "epoch": 0.3399734395750332,
      "grad_norm": 0.23279331624507904,
      "learning_rate": 0.00028978723404255314,
      "loss": 2.1952,
      "step": 32
    },
    {
      "epoch": 0.350597609561753,
      "grad_norm": NaN,
      "learning_rate": 0.00028978723404255314,
      "loss": 2.255,
      "step": 33
    },
    {
      "epoch": 0.3612217795484728,
      "grad_norm": 0.2687067687511444,
      "learning_rate": 0.00028946808510638296,
      "loss": 2.3048,
      "step": 34
    },
    {
      "epoch": 0.37184594953519257,
      "grad_norm": 0.2492797076702118,
      "learning_rate": 0.0002891489361702127,
      "loss": 2.0936,
      "step": 35
    },
    {
      "epoch": 0.38247011952191234,
      "grad_norm": 0.2148616909980774,
      "learning_rate": 0.00028882978723404254,
      "loss": 2.2092,
      "step": 36
    },
    {
      "epoch": 0.3930942895086321,
      "grad_norm": 0.2083529829978943,
      "learning_rate": 0.0002885106382978723,
      "loss": 2.0691,
      "step": 37
    },
    {
      "epoch": 0.4037184594953519,
      "grad_norm": 0.19042588770389557,
      "learning_rate": 0.0002881914893617021,
      "loss": 2.1434,
      "step": 38
    },
    {
      "epoch": 0.41434262948207173,
      "grad_norm": 0.16303813457489014,
      "learning_rate": 0.0002878723404255319,
      "loss": 2.1201,
      "step": 39
    },
    {
      "epoch": 0.4249667994687915,
      "grad_norm": 0.16852618753910065,
      "learning_rate": 0.00028755319148936166,
      "loss": 2.1736,
      "step": 40
    },
    {
      "epoch": 0.4355909694555113,
      "grad_norm": 0.17100104689598083,
      "learning_rate": 0.0002872340425531915,
      "loss": 2.1187,
      "step": 41
    },
    {
      "epoch": 0.44621513944223107,
      "grad_norm": 0.14468209445476532,
      "learning_rate": 0.00028691489361702124,
      "loss": 2.1342,
      "step": 42
    },
    {
      "epoch": 0.45683930942895085,
      "grad_norm": 0.1329849511384964,
      "learning_rate": 0.00028659574468085106,
      "loss": 2.27,
      "step": 43
    },
    {
      "epoch": 0.4674634794156706,
      "grad_norm": 0.129038468003273,
      "learning_rate": 0.0002862765957446808,
      "loss": 2.1306,
      "step": 44
    },
    {
      "epoch": 0.47808764940239046,
      "grad_norm": 0.12817080318927765,
      "learning_rate": 0.00028595744680851064,
      "loss": 2.0742,
      "step": 45
    },
    {
      "epoch": 0.48871181938911024,
      "grad_norm": 0.1258813440799713,
      "learning_rate": 0.0002856382978723404,
      "loss": 2.048,
      "step": 46
    },
    {
      "epoch": 0.49933598937583,
      "grad_norm": 0.12358162552118301,
      "learning_rate": 0.00028531914893617017,
      "loss": 1.9928,
      "step": 47
    },
    {
      "epoch": 0.5099601593625498,
      "grad_norm": 0.11360735446214676,
      "learning_rate": 0.000285,
      "loss": 2.097,
      "step": 48
    },
    {
      "epoch": 0.5205843293492696,
      "grad_norm": 0.10145996510982513,
      "learning_rate": 0.00028468085106382975,
      "loss": 2.0697,
      "step": 49
    },
    {
      "epoch": 0.5312084993359893,
      "grad_norm": 0.1071072593331337,
      "learning_rate": 0.00028436170212765957,
      "loss": 2.0554,
      "step": 50
    },
    {
      "epoch": 0.5418326693227091,
      "grad_norm": 0.12286775559186935,
      "learning_rate": 0.00028404255319148934,
      "loss": 1.978,
      "step": 51
    },
    {
      "epoch": 0.5524568393094289,
      "grad_norm": 0.11012357473373413,
      "learning_rate": 0.0002837234042553191,
      "loss": 2.001,
      "step": 52
    },
    {
      "epoch": 0.5630810092961488,
      "grad_norm": 0.09948943555355072,
      "learning_rate": 0.0002834042553191489,
      "loss": 1.9874,
      "step": 53
    },
    {
      "epoch": 0.5737051792828686,
      "grad_norm": 0.10768385231494904,
      "learning_rate": 0.0002830851063829787,
      "loss": 2.0452,
      "step": 54
    },
    {
      "epoch": 0.5843293492695883,
      "grad_norm": 0.10479564964771271,
      "learning_rate": 0.0002827659574468085,
      "loss": 1.9089,
      "step": 55
    },
    {
      "epoch": 0.5949535192563081,
      "grad_norm": 0.1082344502210617,
      "learning_rate": 0.00028244680851063827,
      "loss": 2.0174,
      "step": 56
    },
    {
      "epoch": 0.6055776892430279,
      "grad_norm": 0.091640904545784,
      "learning_rate": 0.0002821276595744681,
      "loss": 2.0409,
      "step": 57
    },
    {
      "epoch": 0.6162018592297477,
      "grad_norm": 0.09326059371232986,
      "learning_rate": 0.00028180851063829785,
      "loss": 1.9285,
      "step": 58
    },
    {
      "epoch": 0.6268260292164675,
      "grad_norm": 0.0900297462940216,
      "learning_rate": 0.0002814893617021276,
      "loss": 1.9659,
      "step": 59
    },
    {
      "epoch": 0.6374501992031872,
      "grad_norm": 0.0886436328291893,
      "learning_rate": 0.00028117021276595743,
      "loss": 1.9451,
      "step": 60
    },
    {
      "epoch": 0.648074369189907,
      "grad_norm": 0.08444812893867493,
      "learning_rate": 0.0002808510638297872,
      "loss": 1.8422,
      "step": 61
    },
    {
      "epoch": 0.6586985391766268,
      "grad_norm": 0.08084605634212494,
      "learning_rate": 0.000280531914893617,
      "loss": 2.0873,
      "step": 62
    },
    {
      "epoch": 0.6693227091633466,
      "grad_norm": 0.0878792256116867,
      "learning_rate": 0.0002802127659574468,
      "loss": 1.9139,
      "step": 63
    },
    {
      "epoch": 0.6799468791500664,
      "grad_norm": 0.07596878707408905,
      "learning_rate": 0.0002798936170212766,
      "loss": 1.9647,
      "step": 64
    },
    {
      "epoch": 0.6905710491367862,
      "grad_norm": 0.08034836500883102,
      "learning_rate": 0.00027957446808510636,
      "loss": 1.9566,
      "step": 65
    },
    {
      "epoch": 0.701195219123506,
      "grad_norm": 0.11635452508926392,
      "learning_rate": 0.00027925531914893613,
      "loss": 1.9928,
      "step": 66
    },
    {
      "epoch": 0.7118193891102258,
      "grad_norm": 0.08397679775953293,
      "learning_rate": 0.0002789361702127659,
      "loss": 2.0184,
      "step": 67
    },
    {
      "epoch": 0.7224435590969456,
      "grad_norm": 0.0884801596403122,
      "learning_rate": 0.0002786170212765957,
      "loss": 2.0727,
      "step": 68
    },
    {
      "epoch": 0.7330677290836654,
      "grad_norm": 0.09047926962375641,
      "learning_rate": 0.00027829787234042553,
      "loss": 1.861,
      "step": 69
    },
    {
      "epoch": 0.7436918990703851,
      "grad_norm": 0.08517676591873169,
      "learning_rate": 0.0002779787234042553,
      "loss": 1.9289,
      "step": 70
    },
    {
      "epoch": 0.7543160690571049,
      "grad_norm": 0.07628200203180313,
      "learning_rate": 0.00027765957446808506,
      "loss": 2.0881,
      "step": 71
    },
    {
      "epoch": 0.7649402390438247,
      "grad_norm": 0.07643327116966248,
      "learning_rate": 0.0002773404255319149,
      "loss": 1.9156,
      "step": 72
    },
    {
      "epoch": 0.7755644090305445,
      "grad_norm": 0.07732241600751877,
      "learning_rate": 0.00027702127659574464,
      "loss": 1.9646,
      "step": 73
    },
    {
      "epoch": 0.7861885790172642,
      "grad_norm": 0.0736071914434433,
      "learning_rate": 0.00027670212765957446,
      "loss": 1.9724,
      "step": 74
    },
    {
      "epoch": 0.796812749003984,
      "grad_norm": 0.08032434433698654,
      "learning_rate": 0.0002763829787234042,
      "loss": 1.8697,
      "step": 75
    },
    {
      "epoch": 0.8074369189907038,
      "grad_norm": 0.07973580062389374,
      "learning_rate": 0.00027606382978723404,
      "loss": 1.9202,
      "step": 76
    },
    {
      "epoch": 0.8180610889774237,
      "grad_norm": 0.07916183769702911,
      "learning_rate": 0.0002757446808510638,
      "loss": 1.8844,
      "step": 77
    },
    {
      "epoch": 0.8286852589641435,
      "grad_norm": 0.09364909678697586,
      "learning_rate": 0.0002754255319148936,
      "loss": 1.9145,
      "step": 78
    },
    {
      "epoch": 0.8393094289508632,
      "grad_norm": 0.08988624811172485,
      "learning_rate": 0.0002751063829787234,
      "loss": 1.8949,
      "step": 79
    },
    {
      "epoch": 0.849933598937583,
      "grad_norm": 0.07826817035675049,
      "learning_rate": 0.00027478723404255316,
      "loss": 1.8919,
      "step": 80
    },
    {
      "epoch": 0.8605577689243028,
      "grad_norm": 0.0911497101187706,
      "learning_rate": 0.000274468085106383,
      "loss": 1.8728,
      "step": 81
    },
    {
      "epoch": 0.8711819389110226,
      "grad_norm": 0.07091208547353745,
      "learning_rate": 0.00027414893617021274,
      "loss": 1.9424,
      "step": 82
    },
    {
      "epoch": 0.8818061088977424,
      "grad_norm": 0.07147558778524399,
      "learning_rate": 0.00027382978723404256,
      "loss": 1.8044,
      "step": 83
    },
    {
      "epoch": 0.8924302788844621,
      "grad_norm": 0.08481091260910034,
      "learning_rate": 0.0002735106382978723,
      "loss": 1.9595,
      "step": 84
    },
    {
      "epoch": 0.9030544488711819,
      "grad_norm": 0.0827057957649231,
      "learning_rate": 0.0002731914893617021,
      "loss": 2.0251,
      "step": 85
    },
    {
      "epoch": 0.9136786188579017,
      "grad_norm": 0.08022807538509369,
      "learning_rate": 0.00027287234042553185,
      "loss": 1.8807,
      "step": 86
    },
    {
      "epoch": 0.9243027888446215,
      "grad_norm": 0.07993966341018677,
      "learning_rate": 0.00027255319148936167,
      "loss": 1.8167,
      "step": 87
    },
    {
      "epoch": 0.9349269588313412,
      "grad_norm": 0.07699655741453171,
      "learning_rate": 0.0002722340425531915,
      "loss": 1.8585,
      "step": 88
    },
    {
      "epoch": 0.9455511288180611,
      "grad_norm": 0.08736340701580048,
      "learning_rate": 0.00027191489361702125,
      "loss": 1.8147,
      "step": 89
    },
    {
      "epoch": 0.9561752988047809,
      "grad_norm": 0.08588510006666183,
      "learning_rate": 0.000271595744680851,
      "loss": 1.87,
      "step": 90
    },
    {
      "epoch": 0.9667994687915007,
      "grad_norm": 0.07963236421346664,
      "learning_rate": 0.00027127659574468084,
      "loss": 1.9313,
      "step": 91
    },
    {
      "epoch": 0.9774236387782205,
      "grad_norm": 0.07762040197849274,
      "learning_rate": 0.0002709574468085106,
      "loss": 1.8222,
      "step": 92
    },
    {
      "epoch": 0.9880478087649402,
      "grad_norm": 0.08581358939409256,
      "learning_rate": 0.00027063829787234037,
      "loss": 1.932,
      "step": 93
    },
    {
      "epoch": 0.99867197875166,
      "grad_norm": 0.0864376500248909,
      "learning_rate": 0.0002703191489361702,
      "loss": 1.8961,
      "step": 94
    },
    {
      "epoch": 1.0092961487383798,
      "grad_norm": 0.11332442611455917,
      "learning_rate": 0.00027,
      "loss": 1.916,
      "step": 95
    },
    {
      "epoch": 1.0199203187250996,
      "grad_norm": 0.09344201534986496,
      "learning_rate": 0.00026968085106382977,
      "loss": 1.8664,
      "step": 96
    },
    {
      "epoch": 1.0305444887118194,
      "grad_norm": 0.07747912406921387,
      "learning_rate": 0.00026936170212765953,
      "loss": 1.7572,
      "step": 97
    },
    {
      "epoch": 1.0411686586985391,
      "grad_norm": 0.10123267769813538,
      "learning_rate": 0.00026904255319148935,
      "loss": 1.959,
      "step": 98
    },
    {
      "epoch": 1.051792828685259,
      "grad_norm": 0.1068844199180603,
      "learning_rate": 0.0002687234042553191,
      "loss": 1.8538,
      "step": 99
    },
    {
      "epoch": 1.0624169986719787,
      "grad_norm": 0.09686190634965897,
      "learning_rate": 0.00026840425531914893,
      "loss": 1.8548,
      "step": 100
    },
    {
      "epoch": 1.0730411686586985,
      "grad_norm": 0.09034443646669388,
      "learning_rate": 0.0002680851063829787,
      "loss": 1.9828,
      "step": 101
    },
    {
      "epoch": 1.0836653386454183,
      "grad_norm": 0.08811964839696884,
      "learning_rate": 0.0002677659574468085,
      "loss": 1.9387,
      "step": 102
    },
    {
      "epoch": 1.094289508632138,
      "grad_norm": 0.08579233288764954,
      "learning_rate": 0.0002674468085106383,
      "loss": 1.8647,
      "step": 103
    },
    {
      "epoch": 1.1049136786188578,
      "grad_norm": 0.09809018671512604,
      "learning_rate": 0.00026712765957446805,
      "loss": 1.8738,
      "step": 104
    },
    {
      "epoch": 1.1155378486055776,
      "grad_norm": 0.10105618834495544,
      "learning_rate": 0.0002668085106382978,
      "loss": 1.8174,
      "step": 105
    },
    {
      "epoch": 1.1261620185922974,
      "grad_norm": 0.08844725042581558,
      "learning_rate": 0.00026648936170212763,
      "loss": 1.8353,
      "step": 106
    },
    {
      "epoch": 1.1367861885790171,
      "grad_norm": 0.09560899436473846,
      "learning_rate": 0.00026617021276595745,
      "loss": 1.7933,
      "step": 107
    },
    {
      "epoch": 1.1474103585657371,
      "grad_norm": 0.08059749007225037,
      "learning_rate": 0.0002658510638297872,
      "loss": 1.9151,
      "step": 108
    },
    {
      "epoch": 1.158034528552457,
      "grad_norm": 0.08130941540002823,
      "learning_rate": 0.000265531914893617,
      "loss": 1.9007,
      "step": 109
    },
    {
      "epoch": 1.1686586985391767,
      "grad_norm": 0.08468231558799744,
      "learning_rate": 0.0002652127659574468,
      "loss": 1.8042,
      "step": 110
    },
    {
      "epoch": 1.1792828685258965,
      "grad_norm": 0.11069625616073608,
      "learning_rate": 0.00026489361702127656,
      "loss": 1.7738,
      "step": 111
    },
    {
      "epoch": 1.1899070385126163,
      "grad_norm": 0.12281065434217453,
      "learning_rate": 0.0002645744680851063,
      "loss": 1.7945,
      "step": 112
    },
    {
      "epoch": 1.200531208499336,
      "grad_norm": 0.1001247763633728,
      "learning_rate": 0.00026425531914893614,
      "loss": 1.9107,
      "step": 113
    },
    {
      "epoch": 1.2111553784860558,
      "grad_norm": 0.09952205419540405,
      "learning_rate": 0.00026393617021276596,
      "loss": 1.7804,
      "step": 114
    },
    {
      "epoch": 1.2217795484727756,
      "grad_norm": 0.09466720372438431,
      "learning_rate": 0.0002636170212765957,
      "loss": 1.7719,
      "step": 115
    },
    {
      "epoch": 1.2324037184594954,
      "grad_norm": 0.09038345515727997,
      "learning_rate": 0.0002632978723404255,
      "loss": 1.889,
      "step": 116
    },
    {
      "epoch": 1.2430278884462151,
      "grad_norm": 0.11160929501056671,
      "learning_rate": 0.0002629787234042553,
      "loss": 1.8395,
      "step": 117
    },
    {
      "epoch": 1.253652058432935,
      "grad_norm": 0.08024296909570694,
      "learning_rate": 0.0002626595744680851,
      "loss": 1.9484,
      "step": 118
    },
    {
      "epoch": 1.2642762284196547,
      "grad_norm": 0.07904856652021408,
      "learning_rate": 0.0002623404255319149,
      "loss": 1.8523,
      "step": 119
    },
    {
      "epoch": 1.2749003984063745,
      "grad_norm": 0.11055313050746918,
      "learning_rate": 0.00026202127659574466,
      "loss": 1.9068,
      "step": 120
    },
    {
      "epoch": 1.2855245683930943,
      "grad_norm": 0.09166239947080612,
      "learning_rate": 0.0002617021276595745,
      "loss": 1.7134,
      "step": 121
    },
    {
      "epoch": 1.296148738379814,
      "grad_norm": 0.09509138762950897,
      "learning_rate": 0.00026138297872340424,
      "loss": 1.8696,
      "step": 122
    },
    {
      "epoch": 1.3067729083665338,
      "grad_norm": 0.09571313858032227,
      "learning_rate": 0.000261063829787234,
      "loss": 1.7169,
      "step": 123
    },
    {
      "epoch": 1.3173970783532536,
      "grad_norm": 0.11394093185663223,
      "learning_rate": 0.00026074468085106377,
      "loss": 1.7415,
      "step": 124
    },
    {
      "epoch": 1.3280212483399734,
      "grad_norm": 0.10052936524152756,
      "learning_rate": 0.0002604255319148936,
      "loss": 1.7528,
      "step": 125
    },
    {
      "epoch": 1.3386454183266931,
      "grad_norm": 0.08510251343250275,
      "learning_rate": 0.0002601063829787234,
      "loss": 1.9736,
      "step": 126
    },
    {
      "epoch": 1.3492695883134131,
      "grad_norm": 0.09326912462711334,
      "learning_rate": 0.00025978723404255317,
      "loss": 1.7843,
      "step": 127
    },
    {
      "epoch": 1.359893758300133,
      "grad_norm": 0.08329076319932938,
      "learning_rate": 0.000259468085106383,
      "loss": 1.8339,
      "step": 128
    },
    {
      "epoch": 1.3705179282868527,
      "grad_norm": 0.08713129907846451,
      "learning_rate": 0.00025914893617021275,
      "loss": 1.8178,
      "step": 129
    },
    {
      "epoch": 1.3811420982735725,
      "grad_norm": 0.09457555413246155,
      "learning_rate": 0.0002588297872340425,
      "loss": 1.7859,
      "step": 130
    },
    {
      "epoch": 1.3917662682602923,
      "grad_norm": 0.07503700256347656,
      "learning_rate": 0.0002585106382978723,
      "loss": 1.7856,
      "step": 131
    },
    {
      "epoch": 1.402390438247012,
      "grad_norm": 0.08907203376293182,
      "learning_rate": 0.0002581914893617021,
      "loss": 1.8329,
      "step": 132
    },
    {
      "epoch": 1.4130146082337318,
      "grad_norm": 0.08010651171207428,
      "learning_rate": 0.0002578723404255319,
      "loss": 1.8576,
      "step": 133
    },
    {
      "epoch": 1.4236387782204516,
      "grad_norm": 0.12659108638763428,
      "learning_rate": 0.0002575531914893617,
      "loss": 1.7183,
      "step": 134
    },
    {
      "epoch": 1.4342629482071714,
      "grad_norm": 0.09661834686994553,
      "learning_rate": 0.00025723404255319145,
      "loss": 1.7908,
      "step": 135
    },
    {
      "epoch": 1.4448871181938912,
      "grad_norm": 0.07461525499820709,
      "learning_rate": 0.00025691489361702127,
      "loss": 1.6864,
      "step": 136
    },
    {
      "epoch": 1.455511288180611,
      "grad_norm": 0.12449926882982254,
      "learning_rate": 0.00025659574468085103,
      "loss": 1.8382,
      "step": 137
    },
    {
      "epoch": 1.4661354581673307,
      "grad_norm": 0.09183940291404724,
      "learning_rate": 0.0002562765957446808,
      "loss": 1.7148,
      "step": 138
    },
    {
      "epoch": 1.4767596281540505,
      "grad_norm": 0.07938866317272186,
      "learning_rate": 0.0002559574468085106,
      "loss": 1.6908,
      "step": 139
    },
    {
      "epoch": 1.4873837981407703,
      "grad_norm": 0.08909526467323303,
      "learning_rate": 0.00025563829787234044,
      "loss": 1.7754,
      "step": 140
    },
    {
      "epoch": 1.49800796812749,
      "grad_norm": 0.08712038397789001,
      "learning_rate": 0.0002553191489361702,
      "loss": 1.8905,
      "step": 141
    },
    {
      "epoch": 1.5086321381142098,
      "grad_norm": 0.10690370947122574,
      "learning_rate": 0.00025499999999999996,
      "loss": 1.8162,
      "step": 142
    },
    {
      "epoch": 1.5192563081009296,
      "grad_norm": 0.09010189026594162,
      "learning_rate": 0.00025468085106382973,
      "loss": 1.8599,
      "step": 143
    },
    {
      "epoch": 1.5298804780876494,
      "grad_norm": 0.07990174740552902,
      "learning_rate": 0.00025436170212765955,
      "loss": 1.9413,
      "step": 144
    },
    {
      "epoch": 1.5405046480743692,
      "grad_norm": 0.08070138096809387,
      "learning_rate": 0.00025404255319148937,
      "loss": 1.7159,
      "step": 145
    },
    {
      "epoch": 1.551128818061089,
      "grad_norm": 0.09988098591566086,
      "learning_rate": 0.00025372340425531913,
      "loss": 1.7364,
      "step": 146
    },
    {
      "epoch": 1.5617529880478087,
      "grad_norm": 0.08340666443109512,
      "learning_rate": 0.00025340425531914895,
      "loss": 1.7531,
      "step": 147
    },
    {
      "epoch": 1.5723771580345285,
      "grad_norm": 0.09032858908176422,
      "learning_rate": 0.0002530851063829787,
      "loss": 1.7892,
      "step": 148
    },
    {
      "epoch": 1.5830013280212483,
      "grad_norm": 0.07185827940702438,
      "learning_rate": 0.0002527659574468085,
      "loss": 1.7886,
      "step": 149
    },
    {
      "epoch": 1.593625498007968,
      "grad_norm": 0.08283240348100662,
      "learning_rate": 0.00025244680851063824,
      "loss": 1.7764,
      "step": 150
    },
    {
      "epoch": 1.6042496679946878,
      "grad_norm": 0.08560474961996078,
      "learning_rate": 0.00025212765957446806,
      "loss": 1.7738,
      "step": 151
    },
    {
      "epoch": 1.6148738379814076,
      "grad_norm": 0.08574458211660385,
      "learning_rate": 0.0002518085106382979,
      "loss": 1.7116,
      "step": 152
    },
    {
      "epoch": 1.6254980079681274,
      "grad_norm": 0.09498518705368042,
      "learning_rate": 0.00025148936170212764,
      "loss": 1.8842,
      "step": 153
    },
    {
      "epoch": 1.6361221779548472,
      "grad_norm": 0.0901661217212677,
      "learning_rate": 0.0002511702127659574,
      "loss": 1.8358,
      "step": 154
    },
    {
      "epoch": 1.646746347941567,
      "grad_norm": 0.08419694751501083,
      "learning_rate": 0.00025085106382978723,
      "loss": 1.7922,
      "step": 155
    },
    {
      "epoch": 1.6573705179282867,
      "grad_norm": 0.08117412775754929,
      "learning_rate": 0.000250531914893617,
      "loss": 1.7338,
      "step": 156
    },
    {
      "epoch": 1.6679946879150065,
      "grad_norm": 0.08474418520927429,
      "learning_rate": 0.00025021276595744676,
      "loss": 1.8122,
      "step": 157
    },
    {
      "epoch": 1.6786188579017263,
      "grad_norm": 0.07888034731149673,
      "learning_rate": 0.0002498936170212766,
      "loss": 1.7806,
      "step": 158
    },
    {
      "epoch": 1.6892430278884463,
      "grad_norm": 0.1085740402340889,
      "learning_rate": 0.0002495744680851064,
      "loss": 1.5875,
      "step": 159
    },
    {
      "epoch": 1.699867197875166,
      "grad_norm": 0.08953169733285904,
      "learning_rate": 0.00024925531914893616,
      "loss": 1.6759,
      "step": 160
    },
    {
      "epoch": 1.7104913678618858,
      "grad_norm": 0.0773085504770279,
      "learning_rate": 0.0002489361702127659,
      "loss": 1.9008,
      "step": 161
    },
    {
      "epoch": 1.7211155378486056,
      "grad_norm": NaN,
      "learning_rate": 0.0002489361702127659,
      "loss": 1.7039,
      "step": 162
    },
    {
      "epoch": 1.7317397078353254,
      "grad_norm": 0.07997316122055054,
      "learning_rate": 0.0002486170212765957,
      "loss": 1.8294,
      "step": 163
    },
    {
      "epoch": 1.7423638778220452,
      "grad_norm": 0.08575142920017242,
      "learning_rate": 0.0002482978723404255,
      "loss": 1.7801,
      "step": 164
    },
    {
      "epoch": 1.752988047808765,
      "grad_norm": 0.08479725569486618,
      "learning_rate": 0.00024797872340425527,
      "loss": 1.8322,
      "step": 165
    },
    {
      "epoch": 1.7636122177954847,
      "grad_norm": 0.08855168521404266,
      "learning_rate": 0.0002476595744680851,
      "loss": 1.7303,
      "step": 166
    },
    {
      "epoch": 1.7742363877822045,
      "grad_norm": 0.07387498766183853,
      "learning_rate": 0.0002473404255319149,
      "loss": 1.757,
      "step": 167
    },
    {
      "epoch": 1.7848605577689243,
      "grad_norm": 0.07716386020183563,
      "learning_rate": 0.00024702127659574467,
      "loss": 1.7622,
      "step": 168
    },
    {
      "epoch": 1.795484727755644,
      "grad_norm": 0.08597090095281601,
      "learning_rate": 0.00024670212765957444,
      "loss": 1.6555,
      "step": 169
    },
    {
      "epoch": 1.8061088977423638,
      "grad_norm": 0.07917674630880356,
      "learning_rate": 0.0002463829787234042,
      "loss": 1.7271,
      "step": 170
    },
    {
      "epoch": 1.8167330677290838,
      "grad_norm": 0.09325148910284042,
      "learning_rate": 0.000246063829787234,
      "loss": 1.7083,
      "step": 171
    },
    {
      "epoch": 1.8273572377158036,
      "grad_norm": 0.0872533991932869,
      "learning_rate": 0.00024574468085106384,
      "loss": 1.6893,
      "step": 172
    },
    {
      "epoch": 1.8379814077025234,
      "grad_norm": 0.1366048902273178,
      "learning_rate": 0.0002454255319148936,
      "loss": 1.6688,
      "step": 173
    },
    {
      "epoch": 1.8486055776892432,
      "grad_norm": 0.09800561517477036,
      "learning_rate": 0.00024510638297872337,
      "loss": 1.6156,
      "step": 174
    },
    {
      "epoch": 1.859229747675963,
      "grad_norm": 0.08141856640577316,
      "learning_rate": 0.0002447872340425532,
      "loss": 1.6937,
      "step": 175
    },
    {
      "epoch": 1.8698539176626827,
      "grad_norm": 0.08697675913572311,
      "learning_rate": 0.00024446808510638295,
      "loss": 1.5902,
      "step": 176
    },
    {
      "epoch": 1.8804780876494025,
      "grad_norm": 0.08771288394927979,
      "learning_rate": 0.0002441489361702127,
      "loss": 1.7277,
      "step": 177
    },
    {
      "epoch": 1.8911022576361223,
      "grad_norm": 0.09685193002223969,
      "learning_rate": 0.0002438297872340425,
      "loss": 1.7337,
      "step": 178
    },
    {
      "epoch": 1.901726427622842,
      "grad_norm": 0.08651173114776611,
      "learning_rate": 0.00024351063829787233,
      "loss": 1.6189,
      "step": 179
    },
    {
      "epoch": 1.9123505976095618,
      "grad_norm": 0.08489224314689636,
      "learning_rate": 0.00024319148936170212,
      "loss": 1.7176,
      "step": 180
    },
    {
      "epoch": 1.9229747675962816,
      "grad_norm": 0.07669105380773544,
      "learning_rate": 0.00024287234042553188,
      "loss": 1.7302,
      "step": 181
    },
    {
      "epoch": 1.9335989375830014,
      "grad_norm": 0.09143587201833725,
      "learning_rate": 0.00024255319148936167,
      "loss": 1.8211,
      "step": 182
    },
    {
      "epoch": 1.9442231075697212,
      "grad_norm": 0.08110936731100082,
      "learning_rate": 0.00024223404255319146,
      "loss": 1.716,
      "step": 183
    },
    {
      "epoch": 1.954847277556441,
      "grad_norm": 0.0984715074300766,
      "learning_rate": 0.00024191489361702126,
      "loss": 1.6478,
      "step": 184
    },
    {
      "epoch": 1.9654714475431607,
      "grad_norm": 0.09660723060369492,
      "learning_rate": 0.00024159574468085102,
      "loss": 1.7089,
      "step": 185
    },
    {
      "epoch": 1.9760956175298805,
      "grad_norm": 0.08946020901203156,
      "learning_rate": 0.00024127659574468084,
      "loss": 1.7724,
      "step": 186
    },
    {
      "epoch": 1.9867197875166003,
      "grad_norm": 0.14478392899036407,
      "learning_rate": 0.00024095744680851063,
      "loss": 1.6734,
      "step": 187
    },
    {
      "epoch": 1.99734395750332,
      "grad_norm": 0.11967744678258896,
      "learning_rate": 0.0002406382978723404,
      "loss": 1.7309,
      "step": 188
    },
    {
      "epoch": 2.00796812749004,
      "grad_norm": 0.09738278388977051,
      "learning_rate": 0.0002403191489361702,
      "loss": 1.6882,
      "step": 189
    },
    {
      "epoch": 2.0185922974767596,
      "grad_norm": 0.09475640952587128,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.6396,
      "step": 190
    },
    {
      "epoch": 2.0292164674634794,
      "grad_norm": 0.08827947080135345,
      "learning_rate": 0.00023968085106382977,
      "loss": 1.7115,
      "step": 191
    },
    {
      "epoch": 2.039840637450199,
      "grad_norm": 0.08507775515317917,
      "learning_rate": 0.00023936170212765956,
      "loss": 1.7459,
      "step": 192
    },
    {
      "epoch": 2.050464807436919,
      "grad_norm": 0.09584052860736847,
      "learning_rate": 0.00023904255319148933,
      "loss": 1.6531,
      "step": 193
    },
    {
      "epoch": 2.0610889774236387,
      "grad_norm": 0.11834631860256195,
      "learning_rate": 0.00023872340425531915,
      "loss": 1.6297,
      "step": 194
    },
    {
      "epoch": 2.0717131474103585,
      "grad_norm": 0.07898686081171036,
      "learning_rate": 0.0002384042553191489,
      "loss": 1.7375,
      "step": 195
    },
    {
      "epoch": 2.0823373173970783,
      "grad_norm": 0.07584560662508011,
      "learning_rate": 0.0002380851063829787,
      "loss": 1.8002,
      "step": 196
    },
    {
      "epoch": 2.092961487383798,
      "grad_norm": 0.08059314638376236,
      "learning_rate": 0.00023776595744680847,
      "loss": 1.6246,
      "step": 197
    },
    {
      "epoch": 2.103585657370518,
      "grad_norm": 0.08974593877792358,
      "learning_rate": 0.00023744680851063828,
      "loss": 1.7615,
      "step": 198
    },
    {
      "epoch": 2.1142098273572376,
      "grad_norm": 0.0682586058974266,
      "learning_rate": 0.00023712765957446808,
      "loss": 1.7291,
      "step": 199
    },
    {
      "epoch": 2.1248339973439574,
      "grad_norm": 0.08665736019611359,
      "learning_rate": 0.00023680851063829784,
      "loss": 1.6848,
      "step": 200
    },
    {
      "epoch": 2.135458167330677,
      "grad_norm": 0.08664016425609589,
      "learning_rate": 0.00023648936170212766,
      "loss": 1.7101,
      "step": 201
    },
    {
      "epoch": 2.146082337317397,
      "grad_norm": 0.07344760000705719,
      "learning_rate": 0.00023617021276595742,
      "loss": 1.7299,
      "step": 202
    },
    {
      "epoch": 2.1567065073041167,
      "grad_norm": 0.08983156085014343,
      "learning_rate": 0.00023585106382978722,
      "loss": 1.7298,
      "step": 203
    },
    {
      "epoch": 2.1673306772908365,
      "grad_norm": 0.08902983367443085,
      "learning_rate": 0.00023553191489361698,
      "loss": 1.6294,
      "step": 204
    },
    {
      "epoch": 2.1779548472775563,
      "grad_norm": 0.07833916693925858,
      "learning_rate": 0.0002352127659574468,
      "loss": 1.7166,
      "step": 205
    },
    {
      "epoch": 2.188579017264276,
      "grad_norm": 0.07126647979021072,
      "learning_rate": 0.0002348936170212766,
      "loss": 1.7621,
      "step": 206
    },
    {
      "epoch": 2.199203187250996,
      "grad_norm": 0.09007761627435684,
      "learning_rate": 0.00023457446808510635,
      "loss": 1.7359,
      "step": 207
    },
    {
      "epoch": 2.2098273572377156,
      "grad_norm": 0.07404829561710358,
      "learning_rate": 0.00023425531914893615,
      "loss": 1.6925,
      "step": 208
    },
    {
      "epoch": 2.2204515272244354,
      "grad_norm": 0.0897846668958664,
      "learning_rate": 0.00023393617021276594,
      "loss": 1.676,
      "step": 209
    },
    {
      "epoch": 2.231075697211155,
      "grad_norm": 0.12007264047861099,
      "learning_rate": 0.00023361702127659573,
      "loss": 1.6935,
      "step": 210
    },
    {
      "epoch": 2.241699867197875,
      "grad_norm": 0.08360671997070312,
      "learning_rate": 0.0002332978723404255,
      "loss": 1.8298,
      "step": 211
    },
    {
      "epoch": 2.2523240371845947,
      "grad_norm": 0.08342460542917252,
      "learning_rate": 0.00023297872340425529,
      "loss": 1.6155,
      "step": 212
    },
    {
      "epoch": 2.2629482071713145,
      "grad_norm": 0.13088785111904144,
      "learning_rate": 0.0002326595744680851,
      "loss": 1.742,
      "step": 213
    },
    {
      "epoch": 2.2735723771580343,
      "grad_norm": 0.06997595727443695,
      "learning_rate": 0.00023234042553191487,
      "loss": 1.7811,
      "step": 214
    },
    {
      "epoch": 2.2841965471447545,
      "grad_norm": 0.09555305540561676,
      "learning_rate": 0.00023202127659574466,
      "loss": 1.6566,
      "step": 215
    },
    {
      "epoch": 2.2948207171314743,
      "grad_norm": 0.09280280023813248,
      "learning_rate": 0.00023170212765957442,
      "loss": 1.8613,
      "step": 216
    },
    {
      "epoch": 2.305444887118194,
      "grad_norm": 0.09588853269815445,
      "learning_rate": 0.00023138297872340424,
      "loss": 1.6329,
      "step": 217
    },
    {
      "epoch": 2.316069057104914,
      "grad_norm": 0.12264272570610046,
      "learning_rate": 0.00023106382978723403,
      "loss": 1.7587,
      "step": 218
    },
    {
      "epoch": 2.3266932270916336,
      "grad_norm": 0.07793453335762024,
      "learning_rate": 0.0002307446808510638,
      "loss": 1.8145,
      "step": 219
    },
    {
      "epoch": 2.3373173970783534,
      "grad_norm": 0.08428662270307541,
      "learning_rate": 0.00023042553191489362,
      "loss": 1.7085,
      "step": 220
    },
    {
      "epoch": 2.347941567065073,
      "grad_norm": 0.087140753865242,
      "learning_rate": 0.00023010638297872338,
      "loss": 1.6293,
      "step": 221
    },
    {
      "epoch": 2.358565737051793,
      "grad_norm": 0.09934718161821365,
      "learning_rate": 0.00022978723404255317,
      "loss": 1.7531,
      "step": 222
    },
    {
      "epoch": 2.3691899070385127,
      "grad_norm": 0.06589512526988983,
      "learning_rate": 0.00022946808510638294,
      "loss": 1.7958,
      "step": 223
    },
    {
      "epoch": 2.3798140770252325,
      "grad_norm": 0.0838281661272049,
      "learning_rate": 0.00022914893617021276,
      "loss": 1.6561,
      "step": 224
    },
    {
      "epoch": 2.3904382470119523,
      "grad_norm": 0.09797434508800507,
      "learning_rate": 0.00022882978723404255,
      "loss": 1.6978,
      "step": 225
    },
    {
      "epoch": 2.401062416998672,
      "grad_norm": 0.07465117424726486,
      "learning_rate": 0.0002285106382978723,
      "loss": 1.7352,
      "step": 226
    },
    {
      "epoch": 2.411686586985392,
      "grad_norm": 0.0840086117386818,
      "learning_rate": 0.0002281914893617021,
      "loss": 1.7001,
      "step": 227
    },
    {
      "epoch": 2.4223107569721116,
      "grad_norm": 0.10479357838630676,
      "learning_rate": 0.0002278723404255319,
      "loss": 1.7412,
      "step": 228
    },
    {
      "epoch": 2.4329349269588314,
      "grad_norm": 0.08956945687532425,
      "learning_rate": 0.0002275531914893617,
      "loss": 1.5557,
      "step": 229
    },
    {
      "epoch": 2.443559096945551,
      "grad_norm": 0.09703324735164642,
      "learning_rate": 0.00022723404255319145,
      "loss": 1.6794,
      "step": 230
    },
    {
      "epoch": 2.454183266932271,
      "grad_norm": 0.07648909091949463,
      "learning_rate": 0.00022691489361702124,
      "loss": 1.6674,
      "step": 231
    },
    {
      "epoch": 2.4648074369189907,
      "grad_norm": 0.0889127179980278,
      "learning_rate": 0.00022659574468085106,
      "loss": 1.5726,
      "step": 232
    },
    {
      "epoch": 2.4754316069057105,
      "grad_norm": 0.08079401403665543,
      "learning_rate": 0.00022627659574468083,
      "loss": 1.6954,
      "step": 233
    },
    {
      "epoch": 2.4860557768924303,
      "grad_norm": 0.09639470279216766,
      "learning_rate": 0.00022595744680851062,
      "loss": 1.6593,
      "step": 234
    },
    {
      "epoch": 2.49667994687915,
      "grad_norm": 0.07117421180009842,
      "learning_rate": 0.00022563829787234038,
      "loss": 1.6092,
      "step": 235
    },
    {
      "epoch": 2.50730411686587,
      "grad_norm": 0.0780269131064415,
      "learning_rate": 0.0002253191489361702,
      "loss": 1.8341,
      "step": 236
    },
    {
      "epoch": 2.5179282868525896,
      "grad_norm": 0.07999501377344131,
      "learning_rate": 0.000225,
      "loss": 1.7194,
      "step": 237
    },
    {
      "epoch": 2.5285524568393094,
      "grad_norm": 0.07721162587404251,
      "learning_rate": 0.00022468085106382976,
      "loss": 1.7815,
      "step": 238
    },
    {
      "epoch": 2.539176626826029,
      "grad_norm": 0.10254112631082535,
      "learning_rate": 0.00022436170212765958,
      "loss": 1.6956,
      "step": 239
    },
    {
      "epoch": 2.549800796812749,
      "grad_norm": 0.10284124314785004,
      "learning_rate": 0.00022404255319148934,
      "loss": 1.611,
      "step": 240
    },
    {
      "epoch": 2.5604249667994687,
      "grad_norm": 0.08098829537630081,
      "learning_rate": 0.00022372340425531913,
      "loss": 1.7928,
      "step": 241
    },
    {
      "epoch": 2.5710491367861885,
      "grad_norm": 0.08569381386041641,
      "learning_rate": 0.0002234042553191489,
      "loss": 1.5545,
      "step": 242
    },
    {
      "epoch": 2.5816733067729083,
      "grad_norm": 0.09452007710933685,
      "learning_rate": 0.00022308510638297872,
      "loss": 1.6381,
      "step": 243
    },
    {
      "epoch": 2.592297476759628,
      "grad_norm": 0.07188697159290314,
      "learning_rate": 0.0002227659574468085,
      "loss": 1.6543,
      "step": 244
    },
    {
      "epoch": 2.602921646746348,
      "grad_norm": 0.08207941055297852,
      "learning_rate": 0.00022244680851063827,
      "loss": 1.5765,
      "step": 245
    },
    {
      "epoch": 2.6135458167330676,
      "grad_norm": 0.08475076407194138,
      "learning_rate": 0.00022212765957446806,
      "loss": 1.7066,
      "step": 246
    },
    {
      "epoch": 2.6241699867197874,
      "grad_norm": 0.09421990811824799,
      "learning_rate": 0.00022180851063829786,
      "loss": 1.7546,
      "step": 247
    },
    {
      "epoch": 2.634794156706507,
      "grad_norm": 0.0933796837925911,
      "learning_rate": 0.00022148936170212765,
      "loss": 1.7268,
      "step": 248
    },
    {
      "epoch": 2.645418326693227,
      "grad_norm": 0.06919679790735245,
      "learning_rate": 0.0002211702127659574,
      "loss": 1.7648,
      "step": 249
    },
    {
      "epoch": 2.6560424966799467,
      "grad_norm": 0.09467025846242905,
      "learning_rate": 0.0002208510638297872,
      "loss": 1.5967,
      "step": 250
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.11812198907136917,
      "learning_rate": 0.00022053191489361702,
      "loss": 1.8652,
      "step": 251
    },
    {
      "epoch": 2.6772908366533863,
      "grad_norm": 0.08837858587503433,
      "learning_rate": 0.00022021276595744679,
      "loss": 1.6884,
      "step": 252
    },
    {
      "epoch": 2.6879150066401065,
      "grad_norm": 0.0790489912033081,
      "learning_rate": 0.00021989361702127658,
      "loss": 1.6242,
      "step": 253
    },
    {
      "epoch": 2.6985391766268263,
      "grad_norm": 0.08768492937088013,
      "learning_rate": 0.00021957446808510634,
      "loss": 1.6315,
      "step": 254
    },
    {
      "epoch": 2.709163346613546,
      "grad_norm": 0.08749332278966904,
      "learning_rate": 0.00021925531914893616,
      "loss": 1.6819,
      "step": 255
    },
    {
      "epoch": 2.719787516600266,
      "grad_norm": 0.08293814957141876,
      "learning_rate": 0.00021893617021276593,
      "loss": 1.5584,
      "step": 256
    },
    {
      "epoch": 2.7304116865869856,
      "grad_norm": 0.08642102777957916,
      "learning_rate": 0.00021861702127659572,
      "loss": 1.7375,
      "step": 257
    },
    {
      "epoch": 2.7410358565737054,
      "grad_norm": 0.10397759079933167,
      "learning_rate": 0.00021829787234042554,
      "loss": 1.5949,
      "step": 258
    },
    {
      "epoch": 2.751660026560425,
      "grad_norm": 0.08011938631534576,
      "learning_rate": 0.0002179787234042553,
      "loss": 1.5589,
      "step": 259
    },
    {
      "epoch": 2.762284196547145,
      "grad_norm": 0.08378328382968903,
      "learning_rate": 0.0002176595744680851,
      "loss": 1.5813,
      "step": 260
    },
    {
      "epoch": 2.7729083665338647,
      "grad_norm": 0.08823984861373901,
      "learning_rate": 0.00021734042553191486,
      "loss": 1.6607,
      "step": 261
    },
    {
      "epoch": 2.7835325365205845,
      "grad_norm": 0.09370078146457672,
      "learning_rate": 0.00021702127659574468,
      "loss": 1.626,
      "step": 262
    },
    {
      "epoch": 2.7941567065073043,
      "grad_norm": 0.08523169904947281,
      "learning_rate": 0.00021670212765957447,
      "loss": 1.6056,
      "step": 263
    },
    {
      "epoch": 2.804780876494024,
      "grad_norm": 0.08150960505008698,
      "learning_rate": 0.00021638297872340423,
      "loss": 1.6709,
      "step": 264
    },
    {
      "epoch": 2.815405046480744,
      "grad_norm": 0.09529219567775726,
      "learning_rate": 0.00021606382978723402,
      "loss": 1.7535,
      "step": 265
    },
    {
      "epoch": 2.8260292164674636,
      "grad_norm": 0.10047151148319244,
      "learning_rate": 0.00021574468085106381,
      "loss": 1.6211,
      "step": 266
    },
    {
      "epoch": 2.8366533864541834,
      "grad_norm": 0.07933830469846725,
      "learning_rate": 0.0002154255319148936,
      "loss": 1.6962,
      "step": 267
    },
    {
      "epoch": 2.847277556440903,
      "grad_norm": 0.07997510582208633,
      "learning_rate": 0.00021510638297872337,
      "loss": 1.71,
      "step": 268
    },
    {
      "epoch": 2.857901726427623,
      "grad_norm": 0.07891733944416046,
      "learning_rate": 0.00021478723404255316,
      "loss": 1.7304,
      "step": 269
    },
    {
      "epoch": 2.8685258964143427,
      "grad_norm": 0.09876938909292221,
      "learning_rate": 0.00021446808510638298,
      "loss": 1.6115,
      "step": 270
    },
    {
      "epoch": 2.8791500664010625,
      "grad_norm": 0.08463148027658463,
      "learning_rate": 0.00021414893617021275,
      "loss": 1.8059,
      "step": 271
    },
    {
      "epoch": 2.8897742363877823,
      "grad_norm": 0.08177047967910767,
      "learning_rate": 0.00021382978723404254,
      "loss": 1.8978,
      "step": 272
    },
    {
      "epoch": 2.900398406374502,
      "grad_norm": 0.09995210915803909,
      "learning_rate": 0.0002135106382978723,
      "loss": 1.6608,
      "step": 273
    },
    {
      "epoch": 2.911022576361222,
      "grad_norm": 0.09493739157915115,
      "learning_rate": 0.00021319148936170212,
      "loss": 1.705,
      "step": 274
    },
    {
      "epoch": 2.9216467463479416,
      "grad_norm": 0.08499748259782791,
      "learning_rate": 0.00021287234042553188,
      "loss": 1.7787,
      "step": 275
    },
    {
      "epoch": 2.9322709163346614,
      "grad_norm": 0.08179622143507004,
      "learning_rate": 0.00021255319148936168,
      "loss": 1.7791,
      "step": 276
    },
    {
      "epoch": 2.942895086321381,
      "grad_norm": 0.09231356531381607,
      "learning_rate": 0.0002122340425531915,
      "loss": 1.7487,
      "step": 277
    },
    {
      "epoch": 2.953519256308101,
      "grad_norm": 0.0808577686548233,
      "learning_rate": 0.00021191489361702126,
      "loss": 1.6811,
      "step": 278
    },
    {
      "epoch": 2.9641434262948207,
      "grad_norm": 0.09352324157953262,
      "learning_rate": 0.00021159574468085105,
      "loss": 1.6342,
      "step": 279
    },
    {
      "epoch": 2.9747675962815405,
      "grad_norm": 0.09281177818775177,
      "learning_rate": 0.00021127659574468082,
      "loss": 1.7799,
      "step": 280
    },
    {
      "epoch": 2.9853917662682603,
      "grad_norm": 0.08891172707080841,
      "learning_rate": 0.00021095744680851063,
      "loss": 1.634,
      "step": 281
    },
    {
      "epoch": 2.99601593625498,
      "grad_norm": 0.08253594487905502,
      "learning_rate": 0.0002106382978723404,
      "loss": 1.7004,
      "step": 282
    },
    {
      "epoch": 3.0066401062417,
      "grad_norm": 0.09366695582866669,
      "learning_rate": 0.0002103191489361702,
      "loss": 1.733,
      "step": 283
    },
    {
      "epoch": 3.0172642762284196,
      "grad_norm": 0.10267122834920883,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.7925,
      "step": 284
    },
    {
      "epoch": 3.0278884462151394,
      "grad_norm": 0.0999959409236908,
      "learning_rate": 0.00020968085106382977,
      "loss": 1.7719,
      "step": 285
    },
    {
      "epoch": 3.038512616201859,
      "grad_norm": 0.10565171390771866,
      "learning_rate": 0.00020936170212765956,
      "loss": 1.6925,
      "step": 286
    },
    {
      "epoch": 3.049136786188579,
      "grad_norm": 0.08860759437084198,
      "learning_rate": 0.00020904255319148933,
      "loss": 1.6002,
      "step": 287
    },
    {
      "epoch": 3.0597609561752988,
      "grad_norm": 0.08897780627012253,
      "learning_rate": 0.00020872340425531912,
      "loss": 1.798,
      "step": 288
    },
    {
      "epoch": 3.0703851261620185,
      "grad_norm": 0.09204239398241043,
      "learning_rate": 0.00020840425531914894,
      "loss": 1.5878,
      "step": 289
    },
    {
      "epoch": 3.0810092961487383,
      "grad_norm": 0.08298034220933914,
      "learning_rate": 0.0002080851063829787,
      "loss": 1.6352,
      "step": 290
    },
    {
      "epoch": 3.091633466135458,
      "grad_norm": 0.09675175696611404,
      "learning_rate": 0.0002077659574468085,
      "loss": 1.5069,
      "step": 291
    },
    {
      "epoch": 3.102257636122178,
      "grad_norm": 0.08254816383123398,
      "learning_rate": 0.0002074468085106383,
      "loss": 1.6554,
      "step": 292
    },
    {
      "epoch": 3.1128818061088976,
      "grad_norm": 0.09097104519605637,
      "learning_rate": 0.00020712765957446808,
      "loss": 1.6019,
      "step": 293
    },
    {
      "epoch": 3.1235059760956174,
      "grad_norm": 0.08379288762807846,
      "learning_rate": 0.00020680851063829784,
      "loss": 1.6252,
      "step": 294
    },
    {
      "epoch": 3.134130146082337,
      "grad_norm": 0.08946876972913742,
      "learning_rate": 0.00020648936170212763,
      "loss": 1.6431,
      "step": 295
    },
    {
      "epoch": 3.144754316069057,
      "grad_norm": 0.08441557735204697,
      "learning_rate": 0.00020617021276595745,
      "loss": 1.661,
      "step": 296
    },
    {
      "epoch": 3.1553784860557768,
      "grad_norm": 0.07564915716648102,
      "learning_rate": 0.00020585106382978722,
      "loss": 1.7631,
      "step": 297
    },
    {
      "epoch": 3.1660026560424965,
      "grad_norm": 0.09821760654449463,
      "learning_rate": 0.000205531914893617,
      "loss": 1.6719,
      "step": 298
    },
    {
      "epoch": 3.1766268260292163,
      "grad_norm": 0.11012808978557587,
      "learning_rate": 0.00020521276595744677,
      "loss": 1.6196,
      "step": 299
    },
    {
      "epoch": 3.187250996015936,
      "grad_norm": 0.09707041084766388,
      "learning_rate": 0.0002048936170212766,
      "loss": 1.5907,
      "step": 300
    },
    {
      "epoch": 3.197875166002656,
      "grad_norm": 0.08371146768331528,
      "learning_rate": 0.00020457446808510636,
      "loss": 1.6867,
      "step": 301
    },
    {
      "epoch": 3.2084993359893756,
      "grad_norm": 0.09941984713077545,
      "learning_rate": 0.00020425531914893615,
      "loss": 1.6007,
      "step": 302
    },
    {
      "epoch": 3.2191235059760954,
      "grad_norm": 0.0759904533624649,
      "learning_rate": 0.0002039361702127659,
      "loss": 1.6543,
      "step": 303
    },
    {
      "epoch": 3.229747675962815,
      "grad_norm": 0.0830073282122612,
      "learning_rate": 0.00020361702127659573,
      "loss": 1.5972,
      "step": 304
    },
    {
      "epoch": 3.240371845949535,
      "grad_norm": 0.09076899290084839,
      "learning_rate": 0.00020329787234042552,
      "loss": 1.5952,
      "step": 305
    },
    {
      "epoch": 3.2509960159362548,
      "grad_norm": 0.08167530596256256,
      "learning_rate": 0.0002029787234042553,
      "loss": 1.6717,
      "step": 306
    },
    {
      "epoch": 3.2616201859229745,
      "grad_norm": 0.0959407389163971,
      "learning_rate": 0.00020265957446808508,
      "loss": 1.712,
      "step": 307
    },
    {
      "epoch": 3.2722443559096943,
      "grad_norm": 0.0792761743068695,
      "learning_rate": 0.00020234042553191487,
      "loss": 1.7032,
      "step": 308
    },
    {
      "epoch": 3.2828685258964145,
      "grad_norm": 0.07530815899372101,
      "learning_rate": 0.00020202127659574466,
      "loss": 1.6597,
      "step": 309
    },
    {
      "epoch": 3.2934926958831343,
      "grad_norm": 0.0784522294998169,
      "learning_rate": 0.00020170212765957445,
      "loss": 1.7831,
      "step": 310
    },
    {
      "epoch": 3.304116865869854,
      "grad_norm": 0.08888989686965942,
      "learning_rate": 0.00020138297872340425,
      "loss": 1.5844,
      "step": 311
    },
    {
      "epoch": 3.314741035856574,
      "grad_norm": 0.07850754261016846,
      "learning_rate": 0.00020106382978723404,
      "loss": 1.748,
      "step": 312
    },
    {
      "epoch": 3.3253652058432936,
      "grad_norm": 0.08939934521913528,
      "learning_rate": 0.0002007446808510638,
      "loss": 1.6944,
      "step": 313
    },
    {
      "epoch": 3.3359893758300134,
      "grad_norm": 0.07742386311292648,
      "learning_rate": 0.0002004255319148936,
      "loss": 1.7363,
      "step": 314
    },
    {
      "epoch": 3.346613545816733,
      "grad_norm": 0.0808059349656105,
      "learning_rate": 0.0002001063829787234,
      "loss": 1.6749,
      "step": 315
    },
    {
      "epoch": 3.357237715803453,
      "grad_norm": 0.08157669007778168,
      "learning_rate": 0.00019978723404255318,
      "loss": 1.6972,
      "step": 316
    },
    {
      "epoch": 3.3678618857901728,
      "grad_norm": 0.08394844084978104,
      "learning_rate": 0.00019946808510638297,
      "loss": 1.698,
      "step": 317
    },
    {
      "epoch": 3.3784860557768925,
      "grad_norm": 0.09135328978300095,
      "learning_rate": 0.00019914893617021273,
      "loss": 1.5575,
      "step": 318
    },
    {
      "epoch": 3.3891102257636123,
      "grad_norm": 0.10290763527154922,
      "learning_rate": 0.00019882978723404255,
      "loss": 1.5663,
      "step": 319
    },
    {
      "epoch": 3.399734395750332,
      "grad_norm": 0.0871923640370369,
      "learning_rate": 0.00019851063829787232,
      "loss": 1.7381,
      "step": 320
    },
    {
      "epoch": 3.410358565737052,
      "grad_norm": 0.07524427026510239,
      "learning_rate": 0.0001981914893617021,
      "loss": 1.8222,
      "step": 321
    },
    {
      "epoch": 3.4209827357237717,
      "grad_norm": 0.10004841536283493,
      "learning_rate": 0.00019787234042553187,
      "loss": 1.75,
      "step": 322
    },
    {
      "epoch": 3.4316069057104914,
      "grad_norm": 0.08908538520336151,
      "learning_rate": 0.0001975531914893617,
      "loss": 1.5443,
      "step": 323
    },
    {
      "epoch": 3.442231075697211,
      "grad_norm": 0.07355914264917374,
      "learning_rate": 0.00019723404255319148,
      "loss": 1.6106,
      "step": 324
    },
    {
      "epoch": 3.452855245683931,
      "grad_norm": 0.0758935958147049,
      "learning_rate": 0.00019691489361702125,
      "loss": 1.701,
      "step": 325
    },
    {
      "epoch": 3.4634794156706508,
      "grad_norm": 0.0822303518652916,
      "learning_rate": 0.00019659574468085104,
      "loss": 1.6965,
      "step": 326
    },
    {
      "epoch": 3.4741035856573705,
      "grad_norm": 0.07645487785339355,
      "learning_rate": 0.00019627659574468083,
      "loss": 1.6589,
      "step": 327
    },
    {
      "epoch": 3.4847277556440903,
      "grad_norm": 0.08638538420200348,
      "learning_rate": 0.00019595744680851062,
      "loss": 1.707,
      "step": 328
    },
    {
      "epoch": 3.49535192563081,
      "grad_norm": 0.10435963422060013,
      "learning_rate": 0.00019563829787234039,
      "loss": 1.5927,
      "step": 329
    },
    {
      "epoch": 3.50597609561753,
      "grad_norm": 0.07387281209230423,
      "learning_rate": 0.0001953191489361702,
      "loss": 1.6915,
      "step": 330
    },
    {
      "epoch": 3.5166002656042497,
      "grad_norm": 0.0987783744931221,
      "learning_rate": 0.000195,
      "loss": 1.644,
      "step": 331
    },
    {
      "epoch": 3.5272244355909694,
      "grad_norm": 0.07262059301137924,
      "learning_rate": 0.00019468085106382976,
      "loss": 1.5643,
      "step": 332
    },
    {
      "epoch": 3.537848605577689,
      "grad_norm": 0.07891940325498581,
      "learning_rate": 0.00019436170212765955,
      "loss": 1.7723,
      "step": 333
    },
    {
      "epoch": 3.548472775564409,
      "grad_norm": 0.08632764965295792,
      "learning_rate": 0.00019404255319148937,
      "loss": 1.5846,
      "step": 334
    },
    {
      "epoch": 3.5590969455511288,
      "grad_norm": 0.08078315109014511,
      "learning_rate": 0.00019372340425531914,
      "loss": 1.6557,
      "step": 335
    },
    {
      "epoch": 3.5697211155378485,
      "grad_norm": 0.09099909663200378,
      "learning_rate": 0.00019340425531914893,
      "loss": 1.7008,
      "step": 336
    },
    {
      "epoch": 3.5803452855245683,
      "grad_norm": 0.088044174015522,
      "learning_rate": 0.0001930851063829787,
      "loss": 1.6846,
      "step": 337
    },
    {
      "epoch": 3.590969455511288,
      "grad_norm": 0.07687050104141235,
      "learning_rate": 0.0001927659574468085,
      "loss": 1.5898,
      "step": 338
    },
    {
      "epoch": 3.601593625498008,
      "grad_norm": 0.09619280695915222,
      "learning_rate": 0.00019244680851063827,
      "loss": 1.705,
      "step": 339
    },
    {
      "epoch": 3.6122177954847277,
      "grad_norm": 0.09241889417171478,
      "learning_rate": 0.00019212765957446807,
      "loss": 1.5238,
      "step": 340
    },
    {
      "epoch": 3.6228419654714474,
      "grad_norm": 0.08457780629396439,
      "learning_rate": 0.00019180851063829783,
      "loss": 1.5858,
      "step": 341
    },
    {
      "epoch": 3.633466135458167,
      "grad_norm": 0.07253050804138184,
      "learning_rate": 0.00019148936170212765,
      "loss": 1.727,
      "step": 342
    },
    {
      "epoch": 3.644090305444887,
      "grad_norm": 0.07820379734039307,
      "learning_rate": 0.00019117021276595744,
      "loss": 1.7316,
      "step": 343
    },
    {
      "epoch": 3.6547144754316068,
      "grad_norm": 0.10435035824775696,
      "learning_rate": 0.0001908510638297872,
      "loss": 1.6405,
      "step": 344
    },
    {
      "epoch": 3.6653386454183265,
      "grad_norm": 0.08662481606006622,
      "learning_rate": 0.000190531914893617,
      "loss": 1.65,
      "step": 345
    },
    {
      "epoch": 3.6759628154050463,
      "grad_norm": 0.08520755171775818,
      "learning_rate": 0.0001902127659574468,
      "loss": 1.652,
      "step": 346
    },
    {
      "epoch": 3.686586985391766,
      "grad_norm": 0.08171600848436356,
      "learning_rate": 0.00018989361702127658,
      "loss": 1.7095,
      "step": 347
    },
    {
      "epoch": 3.6972111553784863,
      "grad_norm": 0.09085745364427567,
      "learning_rate": 0.00018957446808510635,
      "loss": 1.6281,
      "step": 348
    },
    {
      "epoch": 3.707835325365206,
      "grad_norm": 0.08756884187459946,
      "learning_rate": 0.00018925531914893616,
      "loss": 1.6361,
      "step": 349
    },
    {
      "epoch": 3.718459495351926,
      "grad_norm": 0.07827677577733994,
      "learning_rate": 0.00018893617021276596,
      "loss": 1.5985,
      "step": 350
    },
    {
      "epoch": 3.7290836653386457,
      "grad_norm": 0.0832931324839592,
      "learning_rate": 0.00018861702127659572,
      "loss": 1.7779,
      "step": 351
    },
    {
      "epoch": 3.7397078353253654,
      "grad_norm": 0.08075178414583206,
      "learning_rate": 0.0001882978723404255,
      "loss": 1.8099,
      "step": 352
    },
    {
      "epoch": 3.750332005312085,
      "grad_norm": 0.09760818630456924,
      "learning_rate": 0.0001879787234042553,
      "loss": 1.6193,
      "step": 353
    },
    {
      "epoch": 3.760956175298805,
      "grad_norm": 0.08703875541687012,
      "learning_rate": 0.0001876595744680851,
      "loss": 1.7948,
      "step": 354
    },
    {
      "epoch": 3.7715803452855248,
      "grad_norm": 0.11842148005962372,
      "learning_rate": 0.00018734042553191489,
      "loss": 1.6747,
      "step": 355
    },
    {
      "epoch": 3.7822045152722445,
      "grad_norm": 0.0856247991323471,
      "learning_rate": 0.00018702127659574465,
      "loss": 1.6507,
      "step": 356
    },
    {
      "epoch": 3.7928286852589643,
      "grad_norm": 0.07454843819141388,
      "learning_rate": 0.00018670212765957447,
      "loss": 1.5874,
      "step": 357
    },
    {
      "epoch": 3.803452855245684,
      "grad_norm": 0.0889597162604332,
      "learning_rate": 0.00018638297872340423,
      "loss": 1.5999,
      "step": 358
    },
    {
      "epoch": 3.814077025232404,
      "grad_norm": 0.12103632837533951,
      "learning_rate": 0.00018606382978723403,
      "loss": 1.6603,
      "step": 359
    },
    {
      "epoch": 3.8247011952191237,
      "grad_norm": 0.11040086299180984,
      "learning_rate": 0.0001857446808510638,
      "loss": 1.638,
      "step": 360
    },
    {
      "epoch": 3.8353253652058434,
      "grad_norm": 0.09968622028827667,
      "learning_rate": 0.0001854255319148936,
      "loss": 1.5713,
      "step": 361
    },
    {
      "epoch": 3.845949535192563,
      "grad_norm": 0.08552873879671097,
      "learning_rate": 0.0001851063829787234,
      "loss": 1.7143,
      "step": 362
    },
    {
      "epoch": 3.856573705179283,
      "grad_norm": 0.10283885151147842,
      "learning_rate": 0.00018478723404255316,
      "loss": 1.7684,
      "step": 363
    },
    {
      "epoch": 3.8671978751660028,
      "grad_norm": 0.08209744840860367,
      "learning_rate": 0.00018446808510638298,
      "loss": 1.5932,
      "step": 364
    },
    {
      "epoch": 3.8778220451527226,
      "grad_norm": 0.08333166688680649,
      "learning_rate": 0.00018414893617021275,
      "loss": 1.6268,
      "step": 365
    },
    {
      "epoch": 3.8884462151394423,
      "grad_norm": 0.09842249006032944,
      "learning_rate": 0.00018382978723404254,
      "loss": 1.5001,
      "step": 366
    },
    {
      "epoch": 3.899070385126162,
      "grad_norm": 0.08397795259952545,
      "learning_rate": 0.0001835106382978723,
      "loss": 1.6056,
      "step": 367
    },
    {
      "epoch": 3.909694555112882,
      "grad_norm": 0.09174256026744843,
      "learning_rate": 0.00018319148936170212,
      "loss": 1.5076,
      "step": 368
    },
    {
      "epoch": 3.9203187250996017,
      "grad_norm": 0.08138716965913773,
      "learning_rate": 0.00018287234042553191,
      "loss": 1.7641,
      "step": 369
    },
    {
      "epoch": 3.9309428950863214,
      "grad_norm": 0.09375933557748795,
      "learning_rate": 0.00018255319148936168,
      "loss": 1.6196,
      "step": 370
    },
    {
      "epoch": 3.941567065073041,
      "grad_norm": 0.08882960677146912,
      "learning_rate": 0.00018223404255319147,
      "loss": 1.6398,
      "step": 371
    },
    {
      "epoch": 3.952191235059761,
      "grad_norm": 0.1019798070192337,
      "learning_rate": 0.00018191489361702126,
      "loss": 1.7349,
      "step": 372
    },
    {
      "epoch": 3.9628154050464808,
      "grad_norm": 0.09740639477968216,
      "learning_rate": 0.00018159574468085105,
      "loss": 1.5809,
      "step": 373
    },
    {
      "epoch": 3.9734395750332006,
      "grad_norm": 0.10073769092559814,
      "learning_rate": 0.00018127659574468082,
      "loss": 1.6242,
      "step": 374
    },
    {
      "epoch": 3.9840637450199203,
      "grad_norm": 0.08801964670419693,
      "learning_rate": 0.0001809574468085106,
      "loss": 1.7602,
      "step": 375
    },
    {
      "epoch": 3.99468791500664,
      "grad_norm": 0.08162377774715424,
      "learning_rate": 0.00018063829787234043,
      "loss": 1.7289,
      "step": 376
    },
    {
      "epoch": 4.00531208499336,
      "grad_norm": 0.07945385575294495,
      "learning_rate": 0.0001803191489361702,
      "loss": 1.6937,
      "step": 377
    },
    {
      "epoch": 4.01593625498008,
      "grad_norm": 0.08682263642549515,
      "learning_rate": 0.00017999999999999998,
      "loss": 1.5892,
      "step": 378
    },
    {
      "epoch": 4.0265604249667994,
      "grad_norm": 0.0864492729306221,
      "learning_rate": 0.00017968085106382975,
      "loss": 1.7943,
      "step": 379
    },
    {
      "epoch": 4.037184594953519,
      "grad_norm": 0.08212458342313766,
      "learning_rate": 0.00017936170212765957,
      "loss": 1.5951,
      "step": 380
    },
    {
      "epoch": 4.047808764940239,
      "grad_norm": 0.08783752471208572,
      "learning_rate": 0.00017904255319148936,
      "loss": 1.5932,
      "step": 381
    },
    {
      "epoch": 4.058432934926959,
      "grad_norm": 0.07418927550315857,
      "learning_rate": 0.00017872340425531912,
      "loss": 1.7352,
      "step": 382
    },
    {
      "epoch": 4.069057104913679,
      "grad_norm": 0.08125384151935577,
      "learning_rate": 0.00017840425531914894,
      "loss": 1.7102,
      "step": 383
    },
    {
      "epoch": 4.079681274900398,
      "grad_norm": 0.10382309556007385,
      "learning_rate": 0.0001780851063829787,
      "loss": 1.7569,
      "step": 384
    },
    {
      "epoch": 4.090305444887118,
      "grad_norm": 0.0920681357383728,
      "learning_rate": 0.0001777659574468085,
      "loss": 1.6197,
      "step": 385
    },
    {
      "epoch": 4.100929614873838,
      "grad_norm": 0.09092272818088531,
      "learning_rate": 0.00017744680851063826,
      "loss": 1.7003,
      "step": 386
    },
    {
      "epoch": 4.111553784860558,
      "grad_norm": 0.08193822205066681,
      "learning_rate": 0.00017712765957446808,
      "loss": 1.6719,
      "step": 387
    },
    {
      "epoch": 4.1221779548472774,
      "grad_norm": 0.09658247232437134,
      "learning_rate": 0.00017680851063829787,
      "loss": 1.5998,
      "step": 388
    },
    {
      "epoch": 4.132802124833997,
      "grad_norm": 0.08816409856081009,
      "learning_rate": 0.00017648936170212764,
      "loss": 1.6706,
      "step": 389
    },
    {
      "epoch": 4.143426294820717,
      "grad_norm": 0.10041596740484238,
      "learning_rate": 0.00017617021276595743,
      "loss": 1.6334,
      "step": 390
    },
    {
      "epoch": 4.154050464807437,
      "grad_norm": 0.07710612565279007,
      "learning_rate": 0.00017585106382978722,
      "loss": 1.676,
      "step": 391
    },
    {
      "epoch": 4.164674634794157,
      "grad_norm": 0.09319322556257248,
      "learning_rate": 0.000175531914893617,
      "loss": 1.6832,
      "step": 392
    },
    {
      "epoch": 4.175298804780876,
      "grad_norm": 0.09083358198404312,
      "learning_rate": 0.00017521276595744678,
      "loss": 1.6348,
      "step": 393
    },
    {
      "epoch": 4.185922974767596,
      "grad_norm": 0.08787515759468079,
      "learning_rate": 0.00017489361702127657,
      "loss": 1.6876,
      "step": 394
    },
    {
      "epoch": 4.196547144754316,
      "grad_norm": 0.10133128613233566,
      "learning_rate": 0.0001745744680851064,
      "loss": 1.6196,
      "step": 395
    },
    {
      "epoch": 4.207171314741036,
      "grad_norm": 0.08328977972269058,
      "learning_rate": 0.00017425531914893615,
      "loss": 1.6463,
      "step": 396
    },
    {
      "epoch": 4.2177954847277555,
      "grad_norm": 0.10493592172861099,
      "learning_rate": 0.00017393617021276594,
      "loss": 1.8684,
      "step": 397
    },
    {
      "epoch": 4.228419654714475,
      "grad_norm": 0.09794526547193527,
      "learning_rate": 0.0001736170212765957,
      "loss": 1.7403,
      "step": 398
    },
    {
      "epoch": 4.239043824701195,
      "grad_norm": 0.09793970733880997,
      "learning_rate": 0.00017329787234042553,
      "loss": 1.6962,
      "step": 399
    },
    {
      "epoch": 4.249667994687915,
      "grad_norm": 0.0798066183924675,
      "learning_rate": 0.0001729787234042553,
      "loss": 1.6876,
      "step": 400
    },
    {
      "epoch": 4.260292164674635,
      "grad_norm": 0.08122497797012329,
      "learning_rate": 0.00017265957446808508,
      "loss": 1.6101,
      "step": 401
    },
    {
      "epoch": 4.270916334661354,
      "grad_norm": 0.08621373772621155,
      "learning_rate": 0.0001723404255319149,
      "loss": 1.6278,
      "step": 402
    },
    {
      "epoch": 4.281540504648074,
      "grad_norm": 0.11235560476779938,
      "learning_rate": 0.00017202127659574467,
      "loss": 1.5454,
      "step": 403
    },
    {
      "epoch": 4.292164674634794,
      "grad_norm": 0.08514515310525894,
      "learning_rate": 0.00017170212765957446,
      "loss": 1.7219,
      "step": 404
    },
    {
      "epoch": 4.302788844621514,
      "grad_norm": 0.08616839349269867,
      "learning_rate": 0.00017138297872340422,
      "loss": 1.6962,
      "step": 405
    },
    {
      "epoch": 4.3134130146082335,
      "grad_norm": 0.08929223567247391,
      "learning_rate": 0.00017106382978723404,
      "loss": 1.5977,
      "step": 406
    },
    {
      "epoch": 4.324037184594953,
      "grad_norm": 0.09403519332408905,
      "learning_rate": 0.00017074468085106383,
      "loss": 1.7008,
      "step": 407
    },
    {
      "epoch": 4.334661354581673,
      "grad_norm": 0.09253322333097458,
      "learning_rate": 0.0001704255319148936,
      "loss": 1.5749,
      "step": 408
    },
    {
      "epoch": 4.345285524568393,
      "grad_norm": 0.0790000706911087,
      "learning_rate": 0.0001701063829787234,
      "loss": 1.747,
      "step": 409
    },
    {
      "epoch": 4.355909694555113,
      "grad_norm": 0.08245880901813507,
      "learning_rate": 0.00016978723404255318,
      "loss": 1.6973,
      "step": 410
    },
    {
      "epoch": 4.366533864541832,
      "grad_norm": 0.09024020284414291,
      "learning_rate": 0.00016946808510638297,
      "loss": 1.6234,
      "step": 411
    },
    {
      "epoch": 4.377158034528552,
      "grad_norm": 0.08827321976423264,
      "learning_rate": 0.00016914893617021274,
      "loss": 1.6729,
      "step": 412
    },
    {
      "epoch": 4.387782204515272,
      "grad_norm": 0.08084272593259811,
      "learning_rate": 0.00016882978723404253,
      "loss": 1.7622,
      "step": 413
    },
    {
      "epoch": 4.398406374501992,
      "grad_norm": 0.08709652721881866,
      "learning_rate": 0.00016851063829787235,
      "loss": 1.7578,
      "step": 414
    },
    {
      "epoch": 4.4090305444887115,
      "grad_norm": 0.07375506311655045,
      "learning_rate": 0.0001681914893617021,
      "loss": 1.6879,
      "step": 415
    },
    {
      "epoch": 4.419654714475431,
      "grad_norm": 0.07344821840524673,
      "learning_rate": 0.0001678723404255319,
      "loss": 1.7024,
      "step": 416
    },
    {
      "epoch": 4.430278884462151,
      "grad_norm": 0.09684319794178009,
      "learning_rate": 0.00016755319148936167,
      "loss": 1.5932,
      "step": 417
    },
    {
      "epoch": 4.440903054448871,
      "grad_norm": 0.09844055026769638,
      "learning_rate": 0.00016723404255319149,
      "loss": 1.556,
      "step": 418
    },
    {
      "epoch": 4.451527224435591,
      "grad_norm": 0.08193615078926086,
      "learning_rate": 0.00016691489361702125,
      "loss": 1.6937,
      "step": 419
    },
    {
      "epoch": 4.46215139442231,
      "grad_norm": 0.09007984399795532,
      "learning_rate": 0.00016659574468085104,
      "loss": 1.6359,
      "step": 420
    },
    {
      "epoch": 4.47277556440903,
      "grad_norm": 0.09519783407449722,
      "learning_rate": 0.00016627659574468086,
      "loss": 1.651,
      "step": 421
    },
    {
      "epoch": 4.48339973439575,
      "grad_norm": 0.08479089289903641,
      "learning_rate": 0.00016595744680851062,
      "loss": 1.6628,
      "step": 422
    },
    {
      "epoch": 4.49402390438247,
      "grad_norm": 0.08061783015727997,
      "learning_rate": 0.00016563829787234042,
      "loss": 1.6415,
      "step": 423
    },
    {
      "epoch": 4.5046480743691895,
      "grad_norm": 0.07615027576684952,
      "learning_rate": 0.00016531914893617018,
      "loss": 1.6324,
      "step": 424
    },
    {
      "epoch": 4.515272244355909,
      "grad_norm": 0.11197604238986969,
      "learning_rate": 0.000165,
      "loss": 1.6571,
      "step": 425
    },
    {
      "epoch": 4.525896414342629,
      "grad_norm": 0.10528656095266342,
      "learning_rate": 0.0001646808510638298,
      "loss": 1.5362,
      "step": 426
    },
    {
      "epoch": 4.536520584329349,
      "grad_norm": 0.09094486385583878,
      "learning_rate": 0.00016436170212765956,
      "loss": 1.6523,
      "step": 427
    },
    {
      "epoch": 4.547144754316069,
      "grad_norm": 0.09906522929668427,
      "learning_rate": 0.00016404255319148935,
      "loss": 1.7083,
      "step": 428
    },
    {
      "epoch": 4.557768924302788,
      "grad_norm": 0.08080286532640457,
      "learning_rate": 0.00016372340425531914,
      "loss": 1.6679,
      "step": 429
    },
    {
      "epoch": 4.568393094289509,
      "grad_norm": 0.07868797332048416,
      "learning_rate": 0.00016340425531914893,
      "loss": 1.7476,
      "step": 430
    },
    {
      "epoch": 4.579017264276229,
      "grad_norm": 0.08861221373081207,
      "learning_rate": 0.0001630851063829787,
      "loss": 1.6475,
      "step": 431
    },
    {
      "epoch": 4.589641434262949,
      "grad_norm": 0.09810563921928406,
      "learning_rate": 0.00016276595744680849,
      "loss": 1.647,
      "step": 432
    },
    {
      "epoch": 4.600265604249668,
      "grad_norm": 0.10526110976934433,
      "learning_rate": 0.0001624468085106383,
      "loss": 1.4973,
      "step": 433
    },
    {
      "epoch": 4.610889774236388,
      "grad_norm": 0.08911244571208954,
      "learning_rate": 0.00016212765957446807,
      "loss": 1.5336,
      "step": 434
    },
    {
      "epoch": 4.621513944223108,
      "grad_norm": 0.09485670179128647,
      "learning_rate": 0.00016180851063829786,
      "loss": 1.6682,
      "step": 435
    },
    {
      "epoch": 4.632138114209828,
      "grad_norm": 0.09771431982517242,
      "learning_rate": 0.00016148936170212765,
      "loss": 1.5663,
      "step": 436
    },
    {
      "epoch": 4.6427622841965475,
      "grad_norm": 0.10997948050498962,
      "learning_rate": 0.00016117021276595744,
      "loss": 1.7047,
      "step": 437
    },
    {
      "epoch": 4.653386454183267,
      "grad_norm": 0.07920756191015244,
      "learning_rate": 0.0001608510638297872,
      "loss": 1.5515,
      "step": 438
    },
    {
      "epoch": 4.664010624169987,
      "grad_norm": 0.1039215475320816,
      "learning_rate": 0.000160531914893617,
      "loss": 1.6032,
      "step": 439
    },
    {
      "epoch": 4.674634794156707,
      "grad_norm": 0.08610747754573822,
      "learning_rate": 0.00016021276595744682,
      "loss": 1.56,
      "step": 440
    },
    {
      "epoch": 4.685258964143427,
      "grad_norm": 0.11720161885023117,
      "learning_rate": 0.00015989361702127658,
      "loss": 1.6058,
      "step": 441
    },
    {
      "epoch": 4.695883134130146,
      "grad_norm": 0.09864203631877899,
      "learning_rate": 0.00015957446808510637,
      "loss": 1.6496,
      "step": 442
    },
    {
      "epoch": 4.706507304116866,
      "grad_norm": 0.10727197676897049,
      "learning_rate": 0.00015925531914893614,
      "loss": 1.6982,
      "step": 443
    },
    {
      "epoch": 4.717131474103586,
      "grad_norm": 0.07850947231054306,
      "learning_rate": 0.00015893617021276596,
      "loss": 1.6507,
      "step": 444
    },
    {
      "epoch": 4.727755644090306,
      "grad_norm": 0.08754520118236542,
      "learning_rate": 0.00015861702127659572,
      "loss": 1.6617,
      "step": 445
    },
    {
      "epoch": 4.7383798140770255,
      "grad_norm": 0.09267653524875641,
      "learning_rate": 0.00015829787234042551,
      "loss": 1.8033,
      "step": 446
    },
    {
      "epoch": 4.749003984063745,
      "grad_norm": 0.08620136231184006,
      "learning_rate": 0.00015797872340425528,
      "loss": 1.7341,
      "step": 447
    },
    {
      "epoch": 4.759628154050465,
      "grad_norm": 0.1350734680891037,
      "learning_rate": 0.0001576595744680851,
      "loss": 1.6085,
      "step": 448
    },
    {
      "epoch": 4.770252324037185,
      "grad_norm": 0.11248228698968887,
      "learning_rate": 0.0001573404255319149,
      "loss": 1.4928,
      "step": 449
    },
    {
      "epoch": 4.780876494023905,
      "grad_norm": 0.08677274733781815,
      "learning_rate": 0.00015702127659574465,
      "loss": 1.6722,
      "step": 450
    },
    {
      "epoch": 4.791500664010624,
      "grad_norm": 0.09123413264751434,
      "learning_rate": 0.00015670212765957444,
      "loss": 1.6423,
      "step": 451
    },
    {
      "epoch": 4.802124833997344,
      "grad_norm": 0.07917789369821548,
      "learning_rate": 0.00015638297872340426,
      "loss": 1.6609,
      "step": 452
    },
    {
      "epoch": 4.812749003984064,
      "grad_norm": 0.09896855056285858,
      "learning_rate": 0.00015606382978723403,
      "loss": 1.7087,
      "step": 453
    },
    {
      "epoch": 4.823373173970784,
      "grad_norm": 0.08619018644094467,
      "learning_rate": 0.00015574468085106382,
      "loss": 1.6669,
      "step": 454
    },
    {
      "epoch": 4.8339973439575035,
      "grad_norm": 0.08602439612150192,
      "learning_rate": 0.0001554255319148936,
      "loss": 1.7058,
      "step": 455
    },
    {
      "epoch": 4.844621513944223,
      "grad_norm": 0.10116724669933319,
      "learning_rate": 0.0001551063829787234,
      "loss": 1.616,
      "step": 456
    },
    {
      "epoch": 4.855245683930943,
      "grad_norm": 0.08918610960245132,
      "learning_rate": 0.00015478723404255317,
      "loss": 1.5261,
      "step": 457
    },
    {
      "epoch": 4.865869853917663,
      "grad_norm": 0.09363048523664474,
      "learning_rate": 0.00015446808510638296,
      "loss": 1.6659,
      "step": 458
    },
    {
      "epoch": 4.876494023904383,
      "grad_norm": 0.09472674131393433,
      "learning_rate": 0.00015414893617021278,
      "loss": 1.548,
      "step": 459
    },
    {
      "epoch": 4.887118193891102,
      "grad_norm": 0.07469363510608673,
      "learning_rate": 0.00015382978723404254,
      "loss": 1.5867,
      "step": 460
    },
    {
      "epoch": 4.897742363877822,
      "grad_norm": 0.1079103946685791,
      "learning_rate": 0.00015351063829787233,
      "loss": 1.7202,
      "step": 461
    },
    {
      "epoch": 4.908366533864542,
      "grad_norm": 0.093499094247818,
      "learning_rate": 0.0001531914893617021,
      "loss": 1.6437,
      "step": 462
    },
    {
      "epoch": 4.918990703851262,
      "grad_norm": 0.08074018359184265,
      "learning_rate": 0.00015287234042553192,
      "loss": 1.6674,
      "step": 463
    },
    {
      "epoch": 4.9296148738379815,
      "grad_norm": 0.0739220380783081,
      "learning_rate": 0.00015255319148936168,
      "loss": 1.6339,
      "step": 464
    },
    {
      "epoch": 4.940239043824701,
      "grad_norm": 0.10048957914113998,
      "learning_rate": 0.00015223404255319147,
      "loss": 1.6379,
      "step": 465
    },
    {
      "epoch": 4.950863213811421,
      "grad_norm": 0.09781236946582794,
      "learning_rate": 0.00015191489361702124,
      "loss": 1.6453,
      "step": 466
    },
    {
      "epoch": 4.961487383798141,
      "grad_norm": 0.07689167559146881,
      "learning_rate": 0.00015159574468085106,
      "loss": 1.5992,
      "step": 467
    },
    {
      "epoch": 4.972111553784861,
      "grad_norm": 0.10157697647809982,
      "learning_rate": 0.00015127659574468085,
      "loss": 1.5821,
      "step": 468
    },
    {
      "epoch": 4.98273572377158,
      "grad_norm": 0.07914111763238907,
      "learning_rate": 0.0001509574468085106,
      "loss": 1.668,
      "step": 469
    },
    {
      "epoch": 4.9933598937583,
      "grad_norm": 0.09411558508872986,
      "learning_rate": 0.0001506382978723404,
      "loss": 1.566,
      "step": 470
    },
    {
      "epoch": 5.00398406374502,
      "grad_norm": 0.10619127750396729,
      "learning_rate": 0.0001503191489361702,
      "loss": 1.7037,
      "step": 471
    },
    {
      "epoch": 5.01460823373174,
      "grad_norm": 0.0831579864025116,
      "learning_rate": 0.00015,
      "loss": 1.6265,
      "step": 472
    },
    {
      "epoch": 5.0252324037184595,
      "grad_norm": 0.08738834410905838,
      "learning_rate": 0.00014968085106382978,
      "loss": 1.5634,
      "step": 473
    },
    {
      "epoch": 5.035856573705179,
      "grad_norm": 0.07782075554132462,
      "learning_rate": 0.00014936170212765957,
      "loss": 1.7162,
      "step": 474
    },
    {
      "epoch": 5.046480743691899,
      "grad_norm": 0.10617092996835709,
      "learning_rate": 0.00014904255319148936,
      "loss": 1.681,
      "step": 475
    },
    {
      "epoch": 5.057104913678619,
      "grad_norm": 0.11156871169805527,
      "learning_rate": 0.00014872340425531913,
      "loss": 1.6094,
      "step": 476
    },
    {
      "epoch": 5.067729083665339,
      "grad_norm": 0.09550849348306656,
      "learning_rate": 0.00014840425531914892,
      "loss": 1.6072,
      "step": 477
    },
    {
      "epoch": 5.078353253652058,
      "grad_norm": 0.08579397201538086,
      "learning_rate": 0.0001480851063829787,
      "loss": 1.6528,
      "step": 478
    },
    {
      "epoch": 5.088977423638778,
      "grad_norm": 0.11182492226362228,
      "learning_rate": 0.0001477659574468085,
      "loss": 1.5768,
      "step": 479
    },
    {
      "epoch": 5.099601593625498,
      "grad_norm": 0.10075317323207855,
      "learning_rate": 0.0001474468085106383,
      "loss": 1.7012,
      "step": 480
    },
    {
      "epoch": 5.110225763612218,
      "grad_norm": 0.08118908107280731,
      "learning_rate": 0.00014712765957446808,
      "loss": 1.6745,
      "step": 481
    },
    {
      "epoch": 5.1208499335989375,
      "grad_norm": 0.07709164917469025,
      "learning_rate": 0.00014680851063829785,
      "loss": 1.6082,
      "step": 482
    },
    {
      "epoch": 5.131474103585657,
      "grad_norm": 0.11244022846221924,
      "learning_rate": 0.00014648936170212764,
      "loss": 1.6173,
      "step": 483
    },
    {
      "epoch": 5.142098273572377,
      "grad_norm": 0.0747818574309349,
      "learning_rate": 0.00014617021276595743,
      "loss": 1.6763,
      "step": 484
    },
    {
      "epoch": 5.152722443559097,
      "grad_norm": 0.09231389313936234,
      "learning_rate": 0.00014585106382978722,
      "loss": 1.5957,
      "step": 485
    },
    {
      "epoch": 5.163346613545817,
      "grad_norm": 0.09153174608945847,
      "learning_rate": 0.00014553191489361701,
      "loss": 1.8197,
      "step": 486
    },
    {
      "epoch": 5.173970783532536,
      "grad_norm": 0.0885164812207222,
      "learning_rate": 0.0001452127659574468,
      "loss": 1.5514,
      "step": 487
    },
    {
      "epoch": 5.184594953519256,
      "grad_norm": 0.09375826269388199,
      "learning_rate": 0.00014489361702127657,
      "loss": 1.6878,
      "step": 488
    },
    {
      "epoch": 5.195219123505976,
      "grad_norm": 0.08913598209619522,
      "learning_rate": 0.00014457446808510636,
      "loss": 1.7236,
      "step": 489
    },
    {
      "epoch": 5.205843293492696,
      "grad_norm": 0.09803961962461472,
      "learning_rate": 0.00014425531914893615,
      "loss": 1.6228,
      "step": 490
    },
    {
      "epoch": 5.2164674634794155,
      "grad_norm": 0.09030260890722275,
      "learning_rate": 0.00014393617021276595,
      "loss": 1.6589,
      "step": 491
    },
    {
      "epoch": 5.227091633466135,
      "grad_norm": 0.09441961348056793,
      "learning_rate": 0.00014361702127659574,
      "loss": 1.6511,
      "step": 492
    },
    {
      "epoch": 5.237715803452855,
      "grad_norm": 0.07686807960271835,
      "learning_rate": 0.00014329787234042553,
      "loss": 1.6918,
      "step": 493
    },
    {
      "epoch": 5.248339973439575,
      "grad_norm": 0.10178932547569275,
      "learning_rate": 0.00014297872340425532,
      "loss": 1.6092,
      "step": 494
    },
    {
      "epoch": 5.258964143426295,
      "grad_norm": 0.09245690703392029,
      "learning_rate": 0.00014265957446808508,
      "loss": 1.6307,
      "step": 495
    },
    {
      "epoch": 5.269588313413014,
      "grad_norm": 0.08925676345825195,
      "learning_rate": 0.00014234042553191488,
      "loss": 1.7104,
      "step": 496
    },
    {
      "epoch": 5.280212483399734,
      "grad_norm": 0.0943104475736618,
      "learning_rate": 0.00014202127659574467,
      "loss": 1.6007,
      "step": 497
    },
    {
      "epoch": 5.290836653386454,
      "grad_norm": 0.08029047399759293,
      "learning_rate": 0.00014170212765957446,
      "loss": 1.6875,
      "step": 498
    },
    {
      "epoch": 5.301460823373174,
      "grad_norm": 0.09092270582914352,
      "learning_rate": 0.00014138297872340425,
      "loss": 1.587,
      "step": 499
    },
    {
      "epoch": 5.3120849933598935,
      "grad_norm": 0.09528848528862,
      "learning_rate": 0.00014106382978723404,
      "loss": 1.634,
      "step": 500
    },
    {
      "epoch": 5.322709163346613,
      "grad_norm": 0.08183255791664124,
      "learning_rate": 0.0001407446808510638,
      "loss": 1.6064,
      "step": 501
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.08575296401977539,
      "learning_rate": 0.0001404255319148936,
      "loss": 1.5921,
      "step": 502
    },
    {
      "epoch": 5.343957503320053,
      "grad_norm": 0.09676309674978256,
      "learning_rate": 0.0001401063829787234,
      "loss": 1.6206,
      "step": 503
    },
    {
      "epoch": 5.354581673306773,
      "grad_norm": 0.08127184957265854,
      "learning_rate": 0.00013978723404255318,
      "loss": 1.5645,
      "step": 504
    },
    {
      "epoch": 5.365205843293492,
      "grad_norm": 0.09179391711950302,
      "learning_rate": 0.00013946808510638295,
      "loss": 1.6525,
      "step": 505
    },
    {
      "epoch": 5.375830013280212,
      "grad_norm": 0.11028962582349777,
      "learning_rate": 0.00013914893617021277,
      "loss": 1.5408,
      "step": 506
    },
    {
      "epoch": 5.386454183266932,
      "grad_norm": 0.097292959690094,
      "learning_rate": 0.00013882978723404253,
      "loss": 1.6735,
      "step": 507
    },
    {
      "epoch": 5.397078353253652,
      "grad_norm": 0.09973582625389099,
      "learning_rate": 0.00013851063829787232,
      "loss": 1.694,
      "step": 508
    },
    {
      "epoch": 5.4077025232403715,
      "grad_norm": 0.07912563532590866,
      "learning_rate": 0.0001381914893617021,
      "loss": 1.6338,
      "step": 509
    },
    {
      "epoch": 5.418326693227091,
      "grad_norm": 0.10975318402051926,
      "learning_rate": 0.0001378723404255319,
      "loss": 1.5696,
      "step": 510
    },
    {
      "epoch": 5.428950863213811,
      "grad_norm": 0.08942651748657227,
      "learning_rate": 0.0001375531914893617,
      "loss": 1.7851,
      "step": 511
    },
    {
      "epoch": 5.439575033200531,
      "grad_norm": 0.09808933734893799,
      "learning_rate": 0.0001372340425531915,
      "loss": 1.7244,
      "step": 512
    },
    {
      "epoch": 5.450199203187251,
      "grad_norm": 0.11539435386657715,
      "learning_rate": 0.00013691489361702128,
      "loss": 1.6693,
      "step": 513
    },
    {
      "epoch": 5.46082337317397,
      "grad_norm": 0.08226080983877182,
      "learning_rate": 0.00013659574468085104,
      "loss": 1.6916,
      "step": 514
    },
    {
      "epoch": 5.471447543160691,
      "grad_norm": 0.07972658425569534,
      "learning_rate": 0.00013627659574468084,
      "loss": 1.6873,
      "step": 515
    },
    {
      "epoch": 5.482071713147411,
      "grad_norm": 0.08495728671550751,
      "learning_rate": 0.00013595744680851063,
      "loss": 1.6141,
      "step": 516
    },
    {
      "epoch": 5.492695883134131,
      "grad_norm": 0.09083833545446396,
      "learning_rate": 0.00013563829787234042,
      "loss": 1.6613,
      "step": 517
    },
    {
      "epoch": 5.50332005312085,
      "grad_norm": 0.10134554654359818,
      "learning_rate": 0.00013531914893617018,
      "loss": 1.6429,
      "step": 518
    },
    {
      "epoch": 5.51394422310757,
      "grad_norm": 0.07423730194568634,
      "learning_rate": 0.000135,
      "loss": 1.7376,
      "step": 519
    },
    {
      "epoch": 5.52456839309429,
      "grad_norm": 0.09747423231601715,
      "learning_rate": 0.00013468085106382977,
      "loss": 1.5445,
      "step": 520
    },
    {
      "epoch": 5.53519256308101,
      "grad_norm": 0.08229785412549973,
      "learning_rate": 0.00013436170212765956,
      "loss": 1.6691,
      "step": 521
    },
    {
      "epoch": 5.5458167330677295,
      "grad_norm": 0.08877760916948318,
      "learning_rate": 0.00013404255319148935,
      "loss": 1.709,
      "step": 522
    },
    {
      "epoch": 5.556440903054449,
      "grad_norm": 0.08903516829013824,
      "learning_rate": 0.00013372340425531914,
      "loss": 1.6103,
      "step": 523
    },
    {
      "epoch": 5.567065073041169,
      "grad_norm": 0.08764796704053879,
      "learning_rate": 0.0001334042553191489,
      "loss": 1.6251,
      "step": 524
    },
    {
      "epoch": 5.577689243027889,
      "grad_norm": 0.08547395467758179,
      "learning_rate": 0.00013308510638297872,
      "loss": 1.5427,
      "step": 525
    },
    {
      "epoch": 5.588313413014609,
      "grad_norm": 0.08917687833309174,
      "learning_rate": 0.0001327659574468085,
      "loss": 1.6909,
      "step": 526
    },
    {
      "epoch": 5.598937583001328,
      "grad_norm": 0.08076391369104385,
      "learning_rate": 0.00013244680851063828,
      "loss": 1.6451,
      "step": 527
    },
    {
      "epoch": 5.609561752988048,
      "grad_norm": 0.0970434695482254,
      "learning_rate": 0.00013212765957446807,
      "loss": 1.6801,
      "step": 528
    },
    {
      "epoch": 5.620185922974768,
      "grad_norm": 0.12286479771137238,
      "learning_rate": 0.00013180851063829786,
      "loss": 1.5512,
      "step": 529
    },
    {
      "epoch": 5.630810092961488,
      "grad_norm": 0.1057468056678772,
      "learning_rate": 0.00013148936170212765,
      "loss": 1.6685,
      "step": 530
    },
    {
      "epoch": 5.6414342629482075,
      "grad_norm": 0.10741691291332245,
      "learning_rate": 0.00013117021276595745,
      "loss": 1.6619,
      "step": 531
    },
    {
      "epoch": 5.652058432934927,
      "grad_norm": 0.09850519895553589,
      "learning_rate": 0.00013085106382978724,
      "loss": 1.6852,
      "step": 532
    },
    {
      "epoch": 5.662682602921647,
      "grad_norm": 0.09215546399354935,
      "learning_rate": 0.000130531914893617,
      "loss": 1.6879,
      "step": 533
    },
    {
      "epoch": 5.673306772908367,
      "grad_norm": 0.10692782700061798,
      "learning_rate": 0.0001302127659574468,
      "loss": 1.6435,
      "step": 534
    },
    {
      "epoch": 5.683930942895087,
      "grad_norm": 0.09334197640419006,
      "learning_rate": 0.00012989361702127659,
      "loss": 1.577,
      "step": 535
    },
    {
      "epoch": 5.694555112881806,
      "grad_norm": 0.08773268759250641,
      "learning_rate": 0.00012957446808510638,
      "loss": 1.695,
      "step": 536
    },
    {
      "epoch": 5.705179282868526,
      "grad_norm": 0.11262401938438416,
      "learning_rate": 0.00012925531914893614,
      "loss": 1.5127,
      "step": 537
    },
    {
      "epoch": 5.715803452855246,
      "grad_norm": 0.09524055570363998,
      "learning_rate": 0.00012893617021276596,
      "loss": 1.7497,
      "step": 538
    },
    {
      "epoch": 5.726427622841966,
      "grad_norm": 0.08853912353515625,
      "learning_rate": 0.00012861702127659573,
      "loss": 1.5931,
      "step": 539
    },
    {
      "epoch": 5.7370517928286855,
      "grad_norm": 0.09326741099357605,
      "learning_rate": 0.00012829787234042552,
      "loss": 1.5892,
      "step": 540
    },
    {
      "epoch": 5.747675962815405,
      "grad_norm": 0.09264662116765976,
      "learning_rate": 0.0001279787234042553,
      "loss": 1.6091,
      "step": 541
    },
    {
      "epoch": 5.758300132802125,
      "grad_norm": 0.08398745954036713,
      "learning_rate": 0.0001276595744680851,
      "loss": 1.726,
      "step": 542
    },
    {
      "epoch": 5.768924302788845,
      "grad_norm": 0.08780533075332642,
      "learning_rate": 0.00012734042553191486,
      "loss": 1.7318,
      "step": 543
    },
    {
      "epoch": 5.779548472775565,
      "grad_norm": 0.09251528978347778,
      "learning_rate": 0.00012702127659574468,
      "loss": 1.5943,
      "step": 544
    },
    {
      "epoch": 5.790172642762284,
      "grad_norm": 0.10299787670373917,
      "learning_rate": 0.00012670212765957447,
      "loss": 1.5042,
      "step": 545
    },
    {
      "epoch": 5.800796812749004,
      "grad_norm": 0.09168503433465958,
      "learning_rate": 0.00012638297872340424,
      "loss": 1.5957,
      "step": 546
    },
    {
      "epoch": 5.811420982735724,
      "grad_norm": 0.083958700299263,
      "learning_rate": 0.00012606382978723403,
      "loss": 1.6074,
      "step": 547
    },
    {
      "epoch": 5.822045152722444,
      "grad_norm": 0.08220500499010086,
      "learning_rate": 0.00012574468085106382,
      "loss": 1.7965,
      "step": 548
    },
    {
      "epoch": 5.8326693227091635,
      "grad_norm": 0.09822497516870499,
      "learning_rate": 0.00012542553191489361,
      "loss": 1.5455,
      "step": 549
    },
    {
      "epoch": 5.843293492695883,
      "grad_norm": 0.09898269176483154,
      "learning_rate": 0.00012510638297872338,
      "loss": 1.5945,
      "step": 550
    },
    {
      "epoch": 5.853917662682603,
      "grad_norm": 0.09429475665092468,
      "learning_rate": 0.0001247872340425532,
      "loss": 1.6714,
      "step": 551
    },
    {
      "epoch": 5.864541832669323,
      "grad_norm": 0.1018121987581253,
      "learning_rate": 0.00012446808510638296,
      "loss": 1.5942,
      "step": 552
    },
    {
      "epoch": 5.875166002656043,
      "grad_norm": 0.10048047453165054,
      "learning_rate": 0.00012414893617021275,
      "loss": 1.5422,
      "step": 553
    },
    {
      "epoch": 5.885790172642762,
      "grad_norm": 0.09002681076526642,
      "learning_rate": 0.00012382978723404254,
      "loss": 1.7832,
      "step": 554
    },
    {
      "epoch": 5.896414342629482,
      "grad_norm": 0.07817044109106064,
      "learning_rate": 0.00012351063829787234,
      "loss": 1.7564,
      "step": 555
    },
    {
      "epoch": 5.907038512616202,
      "grad_norm": 0.0897207036614418,
      "learning_rate": 0.0001231914893617021,
      "loss": 1.4596,
      "step": 556
    },
    {
      "epoch": 5.917662682602922,
      "grad_norm": 0.08948300033807755,
      "learning_rate": 0.00012287234042553192,
      "loss": 1.702,
      "step": 557
    },
    {
      "epoch": 5.9282868525896415,
      "grad_norm": 0.08690115809440613,
      "learning_rate": 0.00012255319148936168,
      "loss": 1.6013,
      "step": 558
    },
    {
      "epoch": 5.938911022576361,
      "grad_norm": 0.09706718474626541,
      "learning_rate": 0.00012223404255319148,
      "loss": 1.671,
      "step": 559
    },
    {
      "epoch": 5.949535192563081,
      "grad_norm": 0.07992739230394363,
      "learning_rate": 0.00012191489361702125,
      "loss": 1.7149,
      "step": 560
    },
    {
      "epoch": 5.960159362549801,
      "grad_norm": 0.09663137793540955,
      "learning_rate": 0.00012159574468085106,
      "loss": 1.5812,
      "step": 561
    },
    {
      "epoch": 5.970783532536521,
      "grad_norm": 0.07880494743585587,
      "learning_rate": 0.00012127659574468084,
      "loss": 1.6664,
      "step": 562
    },
    {
      "epoch": 5.98140770252324,
      "grad_norm": 0.08890872448682785,
      "learning_rate": 0.00012095744680851063,
      "loss": 1.6863,
      "step": 563
    },
    {
      "epoch": 5.99203187250996,
      "grad_norm": 0.0944102481007576,
      "learning_rate": 0.00012063829787234042,
      "loss": 1.7874,
      "step": 564
    },
    {
      "epoch": 6.00265604249668,
      "grad_norm": 0.08871564269065857,
      "learning_rate": 0.0001203191489361702,
      "loss": 1.6205,
      "step": 565
    },
    {
      "epoch": 6.0132802124834,
      "grad_norm": 0.09335397928953171,
      "learning_rate": 0.00011999999999999999,
      "loss": 1.6749,
      "step": 566
    },
    {
      "epoch": 6.0239043824701195,
      "grad_norm": 0.09666929394006729,
      "learning_rate": 0.00011968085106382978,
      "loss": 1.6695,
      "step": 567
    },
    {
      "epoch": 6.034528552456839,
      "grad_norm": 0.12144884467124939,
      "learning_rate": 0.00011936170212765957,
      "loss": 1.5419,
      "step": 568
    },
    {
      "epoch": 6.045152722443559,
      "grad_norm": 0.08424141258001328,
      "learning_rate": 0.00011904255319148935,
      "loss": 1.6326,
      "step": 569
    },
    {
      "epoch": 6.055776892430279,
      "grad_norm": 0.07500024139881134,
      "learning_rate": 0.00011872340425531914,
      "loss": 1.75,
      "step": 570
    },
    {
      "epoch": 6.066401062416999,
      "grad_norm": 0.08679602295160294,
      "learning_rate": 0.00011840425531914892,
      "loss": 1.6066,
      "step": 571
    },
    {
      "epoch": 6.077025232403718,
      "grad_norm": 0.10383056104183197,
      "learning_rate": 0.00011808510638297871,
      "loss": 1.565,
      "step": 572
    },
    {
      "epoch": 6.087649402390438,
      "grad_norm": 0.0901254341006279,
      "learning_rate": 0.00011776595744680849,
      "loss": 1.7358,
      "step": 573
    },
    {
      "epoch": 6.098273572377158,
      "grad_norm": 0.09576290845870972,
      "learning_rate": 0.0001174468085106383,
      "loss": 1.6835,
      "step": 574
    },
    {
      "epoch": 6.108897742363878,
      "grad_norm": 0.0988578051328659,
      "learning_rate": 0.00011712765957446807,
      "loss": 1.7773,
      "step": 575
    },
    {
      "epoch": 6.1195219123505975,
      "grad_norm": 0.12322081625461578,
      "learning_rate": 0.00011680851063829786,
      "loss": 1.6602,
      "step": 576
    },
    {
      "epoch": 6.130146082337317,
      "grad_norm": 0.08952612429857254,
      "learning_rate": 0.00011648936170212764,
      "loss": 1.7255,
      "step": 577
    },
    {
      "epoch": 6.140770252324037,
      "grad_norm": 0.0864025428891182,
      "learning_rate": 0.00011617021276595743,
      "loss": 1.5145,
      "step": 578
    },
    {
      "epoch": 6.151394422310757,
      "grad_norm": 0.10717225819826126,
      "learning_rate": 0.00011585106382978721,
      "loss": 1.6128,
      "step": 579
    },
    {
      "epoch": 6.162018592297477,
      "grad_norm": 0.09313853085041046,
      "learning_rate": 0.00011553191489361702,
      "loss": 1.6228,
      "step": 580
    },
    {
      "epoch": 6.172642762284196,
      "grad_norm": 0.07994766533374786,
      "learning_rate": 0.00011521276595744681,
      "loss": 1.8329,
      "step": 581
    },
    {
      "epoch": 6.183266932270916,
      "grad_norm": 0.10525655746459961,
      "learning_rate": 0.00011489361702127659,
      "loss": 1.5937,
      "step": 582
    },
    {
      "epoch": 6.193891102257636,
      "grad_norm": 0.10143612325191498,
      "learning_rate": 0.00011457446808510638,
      "loss": 1.7139,
      "step": 583
    },
    {
      "epoch": 6.204515272244356,
      "grad_norm": 0.11310498416423798,
      "learning_rate": 0.00011425531914893616,
      "loss": 1.5762,
      "step": 584
    },
    {
      "epoch": 6.2151394422310755,
      "grad_norm": 0.09455671161413193,
      "learning_rate": 0.00011393617021276595,
      "loss": 1.5272,
      "step": 585
    },
    {
      "epoch": 6.225763612217795,
      "grad_norm": 0.08667468279600143,
      "learning_rate": 0.00011361702127659573,
      "loss": 1.6621,
      "step": 586
    },
    {
      "epoch": 6.236387782204515,
      "grad_norm": 0.09308133274316788,
      "learning_rate": 0.00011329787234042553,
      "loss": 1.6787,
      "step": 587
    },
    {
      "epoch": 6.247011952191235,
      "grad_norm": 0.0927157774567604,
      "learning_rate": 0.00011297872340425531,
      "loss": 1.7093,
      "step": 588
    },
    {
      "epoch": 6.257636122177955,
      "grad_norm": 0.08370234072208405,
      "learning_rate": 0.0001126595744680851,
      "loss": 1.5936,
      "step": 589
    },
    {
      "epoch": 6.268260292164674,
      "grad_norm": 0.09164613485336304,
      "learning_rate": 0.00011234042553191488,
      "loss": 1.6834,
      "step": 590
    },
    {
      "epoch": 6.278884462151394,
      "grad_norm": 0.10655350983142853,
      "learning_rate": 0.00011202127659574467,
      "loss": 1.6463,
      "step": 591
    },
    {
      "epoch": 6.289508632138114,
      "grad_norm": 0.08055901527404785,
      "learning_rate": 0.00011170212765957445,
      "loss": 1.6813,
      "step": 592
    },
    {
      "epoch": 6.300132802124834,
      "grad_norm": 0.07827971875667572,
      "learning_rate": 0.00011138297872340425,
      "loss": 1.6674,
      "step": 593
    },
    {
      "epoch": 6.3107569721115535,
      "grad_norm": 0.0856725424528122,
      "learning_rate": 0.00011106382978723403,
      "loss": 1.4725,
      "step": 594
    },
    {
      "epoch": 6.321381142098273,
      "grad_norm": 0.10479062795639038,
      "learning_rate": 0.00011074468085106382,
      "loss": 1.4786,
      "step": 595
    },
    {
      "epoch": 6.332005312084993,
      "grad_norm": 0.0950343906879425,
      "learning_rate": 0.0001104255319148936,
      "loss": 1.6603,
      "step": 596
    },
    {
      "epoch": 6.342629482071713,
      "grad_norm": 0.1065179854631424,
      "learning_rate": 0.00011010638297872339,
      "loss": 1.6668,
      "step": 597
    },
    {
      "epoch": 6.353253652058433,
      "grad_norm": 0.09609636664390564,
      "learning_rate": 0.00010978723404255317,
      "loss": 1.5643,
      "step": 598
    },
    {
      "epoch": 6.363877822045152,
      "grad_norm": 0.09408640116453171,
      "learning_rate": 0.00010946808510638296,
      "loss": 1.4824,
      "step": 599
    },
    {
      "epoch": 6.374501992031872,
      "grad_norm": 0.08825455605983734,
      "learning_rate": 0.00010914893617021277,
      "loss": 1.6499,
      "step": 600
    },
    {
      "epoch": 6.385126162018592,
      "grad_norm": 0.09492935985326767,
      "learning_rate": 0.00010882978723404255,
      "loss": 1.6638,
      "step": 601
    },
    {
      "epoch": 6.395750332005312,
      "grad_norm": 0.10318569839000702,
      "learning_rate": 0.00010851063829787234,
      "loss": 1.6115,
      "step": 602
    },
    {
      "epoch": 6.4063745019920315,
      "grad_norm": 0.08877597749233246,
      "learning_rate": 0.00010819148936170212,
      "loss": 1.663,
      "step": 603
    },
    {
      "epoch": 6.416998671978751,
      "grad_norm": 0.10230559855699539,
      "learning_rate": 0.00010787234042553191,
      "loss": 1.6825,
      "step": 604
    },
    {
      "epoch": 6.427622841965471,
      "grad_norm": 0.08308547735214233,
      "learning_rate": 0.00010755319148936169,
      "loss": 1.6215,
      "step": 605
    },
    {
      "epoch": 6.438247011952191,
      "grad_norm": 0.09903278201818466,
      "learning_rate": 0.00010723404255319149,
      "loss": 1.5541,
      "step": 606
    },
    {
      "epoch": 6.448871181938911,
      "grad_norm": 0.10025520622730255,
      "learning_rate": 0.00010691489361702127,
      "loss": 1.7032,
      "step": 607
    },
    {
      "epoch": 6.45949535192563,
      "grad_norm": 0.08384840935468674,
      "learning_rate": 0.00010659574468085106,
      "loss": 1.7543,
      "step": 608
    },
    {
      "epoch": 6.47011952191235,
      "grad_norm": 0.1001969501376152,
      "learning_rate": 0.00010627659574468084,
      "loss": 1.7149,
      "step": 609
    },
    {
      "epoch": 6.48074369189907,
      "grad_norm": 0.11542390286922455,
      "learning_rate": 0.00010595744680851063,
      "loss": 1.5903,
      "step": 610
    },
    {
      "epoch": 6.49136786188579,
      "grad_norm": 0.10699061304330826,
      "learning_rate": 0.00010563829787234041,
      "loss": 1.6043,
      "step": 611
    },
    {
      "epoch": 6.5019920318725095,
      "grad_norm": 0.10608775168657303,
      "learning_rate": 0.0001053191489361702,
      "loss": 1.6461,
      "step": 612
    },
    {
      "epoch": 6.512616201859229,
      "grad_norm": 0.07949263602495193,
      "learning_rate": 0.00010499999999999999,
      "loss": 1.5837,
      "step": 613
    },
    {
      "epoch": 6.523240371845949,
      "grad_norm": 0.08421275019645691,
      "learning_rate": 0.00010468085106382978,
      "loss": 1.5974,
      "step": 614
    },
    {
      "epoch": 6.533864541832669,
      "grad_norm": 0.08195365965366364,
      "learning_rate": 0.00010436170212765956,
      "loss": 1.6616,
      "step": 615
    },
    {
      "epoch": 6.544488711819389,
      "grad_norm": 0.09349551051855087,
      "learning_rate": 0.00010404255319148935,
      "loss": 1.6097,
      "step": 616
    },
    {
      "epoch": 6.555112881806108,
      "grad_norm": 0.08898666501045227,
      "learning_rate": 0.00010372340425531914,
      "loss": 1.6486,
      "step": 617
    },
    {
      "epoch": 6.565737051792829,
      "grad_norm": 0.08526021242141724,
      "learning_rate": 0.00010340425531914892,
      "loss": 1.6823,
      "step": 618
    },
    {
      "epoch": 6.576361221779549,
      "grad_norm": 0.1065848097205162,
      "learning_rate": 0.00010308510638297873,
      "loss": 1.684,
      "step": 619
    },
    {
      "epoch": 6.586985391766269,
      "grad_norm": 0.0932386964559555,
      "learning_rate": 0.0001027659574468085,
      "loss": 1.6491,
      "step": 620
    },
    {
      "epoch": 6.597609561752988,
      "grad_norm": 0.09324554353952408,
      "learning_rate": 0.0001024468085106383,
      "loss": 1.6719,
      "step": 621
    },
    {
      "epoch": 6.608233731739708,
      "grad_norm": 0.09225090593099594,
      "learning_rate": 0.00010212765957446807,
      "loss": 1.5182,
      "step": 622
    },
    {
      "epoch": 6.618857901726428,
      "grad_norm": 0.10216664522886276,
      "learning_rate": 0.00010180851063829787,
      "loss": 1.724,
      "step": 623
    },
    {
      "epoch": 6.629482071713148,
      "grad_norm": 0.08517412841320038,
      "learning_rate": 0.00010148936170212764,
      "loss": 1.6732,
      "step": 624
    },
    {
      "epoch": 6.6401062416998675,
      "grad_norm": 0.0851474478840828,
      "learning_rate": 0.00010117021276595744,
      "loss": 1.68,
      "step": 625
    },
    {
      "epoch": 6.650730411686587,
      "grad_norm": 0.09172206372022629,
      "learning_rate": 0.00010085106382978723,
      "loss": 1.6193,
      "step": 626
    },
    {
      "epoch": 6.661354581673307,
      "grad_norm": 0.0921960324048996,
      "learning_rate": 0.00010053191489361702,
      "loss": 1.6566,
      "step": 627
    },
    {
      "epoch": 6.671978751660027,
      "grad_norm": 0.09332924336194992,
      "learning_rate": 0.0001002127659574468,
      "loss": 1.6217,
      "step": 628
    },
    {
      "epoch": 6.682602921646747,
      "grad_norm": 0.09255784004926682,
      "learning_rate": 9.989361702127659e-05,
      "loss": 1.6576,
      "step": 629
    },
    {
      "epoch": 6.693227091633466,
      "grad_norm": 0.08449418842792511,
      "learning_rate": 9.957446808510637e-05,
      "loss": 1.7719,
      "step": 630
    },
    {
      "epoch": 6.703851261620186,
      "grad_norm": 0.08133725821971893,
      "learning_rate": 9.925531914893616e-05,
      "loss": 1.742,
      "step": 631
    },
    {
      "epoch": 6.714475431606906,
      "grad_norm": 0.1024121344089508,
      "learning_rate": 9.893617021276594e-05,
      "loss": 1.5963,
      "step": 632
    },
    {
      "epoch": 6.725099601593626,
      "grad_norm": 0.10769305378198624,
      "learning_rate": 9.861702127659574e-05,
      "loss": 1.5106,
      "step": 633
    },
    {
      "epoch": 6.7357237715803455,
      "grad_norm": 0.08917153626680374,
      "learning_rate": 9.829787234042552e-05,
      "loss": 1.6628,
      "step": 634
    },
    {
      "epoch": 6.746347941567065,
      "grad_norm": 0.08799558132886887,
      "learning_rate": 9.797872340425531e-05,
      "loss": 1.6211,
      "step": 635
    },
    {
      "epoch": 6.756972111553785,
      "grad_norm": 0.09701087325811386,
      "learning_rate": 9.76595744680851e-05,
      "loss": 1.6072,
      "step": 636
    },
    {
      "epoch": 6.767596281540505,
      "grad_norm": 0.0908808559179306,
      "learning_rate": 9.734042553191488e-05,
      "loss": 1.5762,
      "step": 637
    },
    {
      "epoch": 6.778220451527225,
      "grad_norm": 0.09304715692996979,
      "learning_rate": 9.702127659574469e-05,
      "loss": 1.6261,
      "step": 638
    },
    {
      "epoch": 6.788844621513944,
      "grad_norm": 0.09380560368299484,
      "learning_rate": 9.670212765957446e-05,
      "loss": 1.7607,
      "step": 639
    },
    {
      "epoch": 6.799468791500664,
      "grad_norm": 0.09804421663284302,
      "learning_rate": 9.638297872340426e-05,
      "loss": 1.6321,
      "step": 640
    },
    {
      "epoch": 6.810092961487384,
      "grad_norm": 0.08754219859838486,
      "learning_rate": 9.606382978723403e-05,
      "loss": 1.4972,
      "step": 641
    },
    {
      "epoch": 6.820717131474104,
      "grad_norm": 0.12465810775756836,
      "learning_rate": 9.574468085106382e-05,
      "loss": 1.5313,
      "step": 642
    },
    {
      "epoch": 6.8313413014608235,
      "grad_norm": 0.10624989867210388,
      "learning_rate": 9.54255319148936e-05,
      "loss": 1.6702,
      "step": 643
    },
    {
      "epoch": 6.841965471447543,
      "grad_norm": 0.08680021017789841,
      "learning_rate": 9.51063829787234e-05,
      "loss": 1.6818,
      "step": 644
    },
    {
      "epoch": 6.852589641434263,
      "grad_norm": 0.08378438651561737,
      "learning_rate": 9.478723404255317e-05,
      "loss": 1.6663,
      "step": 645
    },
    {
      "epoch": 6.863213811420983,
      "grad_norm": 0.09346722811460495,
      "learning_rate": 9.446808510638298e-05,
      "loss": 1.6702,
      "step": 646
    },
    {
      "epoch": 6.873837981407703,
      "grad_norm": 0.09889350086450577,
      "learning_rate": 9.414893617021276e-05,
      "loss": 1.5647,
      "step": 647
    },
    {
      "epoch": 6.884462151394422,
      "grad_norm": 0.0876079648733139,
      "learning_rate": 9.382978723404255e-05,
      "loss": 1.7138,
      "step": 648
    },
    {
      "epoch": 6.895086321381142,
      "grad_norm": 0.08516555279493332,
      "learning_rate": 9.351063829787233e-05,
      "loss": 1.6896,
      "step": 649
    },
    {
      "epoch": 6.905710491367862,
      "grad_norm": 0.084271639585495,
      "learning_rate": 9.319148936170212e-05,
      "loss": 1.6699,
      "step": 650
    },
    {
      "epoch": 6.916334661354582,
      "grad_norm": 0.10404012352228165,
      "learning_rate": 9.28723404255319e-05,
      "loss": 1.6163,
      "step": 651
    },
    {
      "epoch": 6.9269588313413015,
      "grad_norm": 0.1015656441450119,
      "learning_rate": 9.25531914893617e-05,
      "loss": 1.5539,
      "step": 652
    },
    {
      "epoch": 6.937583001328021,
      "grad_norm": 0.0878814235329628,
      "learning_rate": 9.223404255319149e-05,
      "loss": 1.7269,
      "step": 653
    },
    {
      "epoch": 6.948207171314741,
      "grad_norm": 0.09357529133558273,
      "learning_rate": 9.191489361702127e-05,
      "loss": 1.5695,
      "step": 654
    },
    {
      "epoch": 6.958831341301461,
      "grad_norm": 0.11949726939201355,
      "learning_rate": 9.159574468085106e-05,
      "loss": 1.6218,
      "step": 655
    },
    {
      "epoch": 6.969455511288181,
      "grad_norm": 0.1070319339632988,
      "learning_rate": 9.127659574468084e-05,
      "loss": 1.6492,
      "step": 656
    },
    {
      "epoch": 6.9800796812749,
      "grad_norm": 0.09626124799251556,
      "learning_rate": 9.095744680851063e-05,
      "loss": 1.5789,
      "step": 657
    },
    {
      "epoch": 6.99070385126162,
      "grad_norm": 0.08395138382911682,
      "learning_rate": 9.063829787234041e-05,
      "loss": 1.731,
      "step": 658
    },
    {
      "epoch": 7.00132802124834,
      "grad_norm": 0.10153135657310486,
      "learning_rate": 9.031914893617021e-05,
      "loss": 1.5543,
      "step": 659
    },
    {
      "epoch": 7.01195219123506,
      "grad_norm": 0.08570380508899689,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.7576,
      "step": 660
    },
    {
      "epoch": 7.0225763612217795,
      "grad_norm": 0.11494844406843185,
      "learning_rate": 8.968085106382978e-05,
      "loss": 1.6713,
      "step": 661
    },
    {
      "epoch": 7.033200531208499,
      "grad_norm": 0.0877055674791336,
      "learning_rate": 8.936170212765956e-05,
      "loss": 1.647,
      "step": 662
    },
    {
      "epoch": 7.043824701195219,
      "grad_norm": 0.0991748571395874,
      "learning_rate": 8.904255319148935e-05,
      "loss": 1.6584,
      "step": 663
    },
    {
      "epoch": 7.054448871181939,
      "grad_norm": 0.09973864257335663,
      "learning_rate": 8.872340425531913e-05,
      "loss": 1.5971,
      "step": 664
    },
    {
      "epoch": 7.065073041168659,
      "grad_norm": 0.09219928830862045,
      "learning_rate": 8.840425531914894e-05,
      "loss": 1.6007,
      "step": 665
    },
    {
      "epoch": 7.075697211155378,
      "grad_norm": 0.11578083783388138,
      "learning_rate": 8.808510638297871e-05,
      "loss": 1.5034,
      "step": 666
    },
    {
      "epoch": 7.086321381142098,
      "grad_norm": 0.10054633021354675,
      "learning_rate": 8.77659574468085e-05,
      "loss": 1.7368,
      "step": 667
    },
    {
      "epoch": 7.096945551128818,
      "grad_norm": 0.08892044425010681,
      "learning_rate": 8.744680851063828e-05,
      "loss": 1.6435,
      "step": 668
    },
    {
      "epoch": 7.107569721115538,
      "grad_norm": 0.10357515513896942,
      "learning_rate": 8.712765957446808e-05,
      "loss": 1.5731,
      "step": 669
    },
    {
      "epoch": 7.1181938911022575,
      "grad_norm": 0.08235009759664536,
      "learning_rate": 8.680851063829785e-05,
      "loss": 1.6603,
      "step": 670
    },
    {
      "epoch": 7.128818061088977,
      "grad_norm": 0.08821271359920502,
      "learning_rate": 8.648936170212765e-05,
      "loss": 1.566,
      "step": 671
    },
    {
      "epoch": 7.139442231075697,
      "grad_norm": 0.09569168090820312,
      "learning_rate": 8.617021276595745e-05,
      "loss": 1.6211,
      "step": 672
    },
    {
      "epoch": 7.150066401062417,
      "grad_norm": 0.09063892066478729,
      "learning_rate": 8.585106382978723e-05,
      "loss": 1.6545,
      "step": 673
    },
    {
      "epoch": 7.160690571049137,
      "grad_norm": 0.08494549989700317,
      "learning_rate": 8.553191489361702e-05,
      "loss": 1.8126,
      "step": 674
    },
    {
      "epoch": 7.171314741035856,
      "grad_norm": 0.08522332459688187,
      "learning_rate": 8.52127659574468e-05,
      "loss": 1.5701,
      "step": 675
    },
    {
      "epoch": 7.181938911022576,
      "grad_norm": 0.1036466434597969,
      "learning_rate": 8.489361702127659e-05,
      "loss": 1.5551,
      "step": 676
    },
    {
      "epoch": 7.192563081009296,
      "grad_norm": 0.08305107057094574,
      "learning_rate": 8.457446808510637e-05,
      "loss": 1.6995,
      "step": 677
    },
    {
      "epoch": 7.203187250996016,
      "grad_norm": 0.09776606410741806,
      "learning_rate": 8.425531914893617e-05,
      "loss": 1.6723,
      "step": 678
    },
    {
      "epoch": 7.2138114209827355,
      "grad_norm": 0.10283295810222626,
      "learning_rate": 8.393617021276595e-05,
      "loss": 1.5552,
      "step": 679
    },
    {
      "epoch": 7.224435590969455,
      "grad_norm": 0.08311526477336884,
      "learning_rate": 8.361702127659574e-05,
      "loss": 1.6952,
      "step": 680
    },
    {
      "epoch": 7.235059760956175,
      "grad_norm": 0.1129172146320343,
      "learning_rate": 8.329787234042552e-05,
      "loss": 1.6911,
      "step": 681
    },
    {
      "epoch": 7.245683930942895,
      "grad_norm": 0.0853729099035263,
      "learning_rate": 8.297872340425531e-05,
      "loss": 1.6713,
      "step": 682
    },
    {
      "epoch": 7.256308100929615,
      "grad_norm": 0.08721067011356354,
      "learning_rate": 8.265957446808509e-05,
      "loss": 1.6729,
      "step": 683
    },
    {
      "epoch": 7.266932270916334,
      "grad_norm": 0.09298019111156464,
      "learning_rate": 8.23404255319149e-05,
      "loss": 1.6155,
      "step": 684
    },
    {
      "epoch": 7.277556440903054,
      "grad_norm": 0.10082001984119415,
      "learning_rate": 8.202127659574467e-05,
      "loss": 1.774,
      "step": 685
    },
    {
      "epoch": 7.288180610889774,
      "grad_norm": 0.09342605620622635,
      "learning_rate": 8.170212765957446e-05,
      "loss": 1.6285,
      "step": 686
    },
    {
      "epoch": 7.298804780876494,
      "grad_norm": 0.09601520746946335,
      "learning_rate": 8.138297872340424e-05,
      "loss": 1.722,
      "step": 687
    },
    {
      "epoch": 7.3094289508632135,
      "grad_norm": 0.09241463243961334,
      "learning_rate": 8.106382978723403e-05,
      "loss": 1.6941,
      "step": 688
    },
    {
      "epoch": 7.320053120849933,
      "grad_norm": 0.09064702689647675,
      "learning_rate": 8.074468085106383e-05,
      "loss": 1.569,
      "step": 689
    },
    {
      "epoch": 7.330677290836653,
      "grad_norm": 0.10881178826093674,
      "learning_rate": 8.04255319148936e-05,
      "loss": 1.595,
      "step": 690
    },
    {
      "epoch": 7.341301460823373,
      "grad_norm": 0.09298048168420792,
      "learning_rate": 8.010638297872341e-05,
      "loss": 1.6509,
      "step": 691
    },
    {
      "epoch": 7.351925630810093,
      "grad_norm": 0.07987596839666367,
      "learning_rate": 7.978723404255319e-05,
      "loss": 1.5786,
      "step": 692
    },
    {
      "epoch": 7.362549800796812,
      "grad_norm": 0.09682396799325943,
      "learning_rate": 7.946808510638298e-05,
      "loss": 1.6882,
      "step": 693
    },
    {
      "epoch": 7.373173970783532,
      "grad_norm": 0.09759871661663055,
      "learning_rate": 7.914893617021276e-05,
      "loss": 1.522,
      "step": 694
    },
    {
      "epoch": 7.383798140770252,
      "grad_norm": 0.09339617192745209,
      "learning_rate": 7.882978723404255e-05,
      "loss": 1.6527,
      "step": 695
    },
    {
      "epoch": 7.394422310756972,
      "grad_norm": 0.08829611539840698,
      "learning_rate": 7.851063829787233e-05,
      "loss": 1.5962,
      "step": 696
    },
    {
      "epoch": 7.4050464807436915,
      "grad_norm": 0.08851917088031769,
      "learning_rate": 7.819148936170213e-05,
      "loss": 1.6576,
      "step": 697
    },
    {
      "epoch": 7.415670650730411,
      "grad_norm": 0.0929383859038353,
      "learning_rate": 7.787234042553191e-05,
      "loss": 1.6975,
      "step": 698
    },
    {
      "epoch": 7.426294820717131,
      "grad_norm": 0.09325280785560608,
      "learning_rate": 7.75531914893617e-05,
      "loss": 1.6189,
      "step": 699
    },
    {
      "epoch": 7.436918990703851,
      "grad_norm": 0.09691549092531204,
      "learning_rate": 7.723404255319148e-05,
      "loss": 1.7372,
      "step": 700
    },
    {
      "epoch": 7.447543160690571,
      "grad_norm": 0.11605171114206314,
      "learning_rate": 7.691489361702127e-05,
      "loss": 1.4914,
      "step": 701
    },
    {
      "epoch": 7.45816733067729,
      "grad_norm": 0.0931670069694519,
      "learning_rate": 7.659574468085105e-05,
      "loss": 1.6468,
      "step": 702
    },
    {
      "epoch": 7.468791500664011,
      "grad_norm": 0.09271963685750961,
      "learning_rate": 7.627659574468084e-05,
      "loss": 1.5808,
      "step": 703
    },
    {
      "epoch": 7.479415670650731,
      "grad_norm": 0.08412516117095947,
      "learning_rate": 7.595744680851062e-05,
      "loss": 1.6067,
      "step": 704
    },
    {
      "epoch": 7.490039840637451,
      "grad_norm": 0.08412562310695648,
      "learning_rate": 7.563829787234042e-05,
      "loss": 1.6498,
      "step": 705
    },
    {
      "epoch": 7.50066401062417,
      "grad_norm": 0.1077326163649559,
      "learning_rate": 7.53191489361702e-05,
      "loss": 1.6645,
      "step": 706
    },
    {
      "epoch": 7.51128818061089,
      "grad_norm": 0.10459844768047333,
      "learning_rate": 7.5e-05,
      "loss": 1.8186,
      "step": 707
    },
    {
      "epoch": 7.52191235059761,
      "grad_norm": 0.10317589342594147,
      "learning_rate": 7.468085106382979e-05,
      "loss": 1.611,
      "step": 708
    },
    {
      "epoch": 7.53253652058433,
      "grad_norm": 0.08266705274581909,
      "learning_rate": 7.436170212765956e-05,
      "loss": 1.512,
      "step": 709
    },
    {
      "epoch": 7.5431606905710495,
      "grad_norm": 0.09345398098230362,
      "learning_rate": 7.404255319148935e-05,
      "loss": 1.7138,
      "step": 710
    },
    {
      "epoch": 7.553784860557769,
      "grad_norm": 0.08943190425634384,
      "learning_rate": 7.372340425531915e-05,
      "loss": 1.6517,
      "step": 711
    },
    {
      "epoch": 7.564409030544489,
      "grad_norm": 0.09562159329652786,
      "learning_rate": 7.340425531914892e-05,
      "loss": 1.6368,
      "step": 712
    },
    {
      "epoch": 7.575033200531209,
      "grad_norm": 0.08376201242208481,
      "learning_rate": 7.308510638297872e-05,
      "loss": 1.691,
      "step": 713
    },
    {
      "epoch": 7.585657370517929,
      "grad_norm": 0.1002935916185379,
      "learning_rate": 7.276595744680851e-05,
      "loss": 1.5545,
      "step": 714
    },
    {
      "epoch": 7.596281540504648,
      "grad_norm": 0.0819728672504425,
      "learning_rate": 7.244680851063829e-05,
      "loss": 1.7184,
      "step": 715
    },
    {
      "epoch": 7.606905710491368,
      "grad_norm": 0.11043203622102737,
      "learning_rate": 7.212765957446808e-05,
      "loss": 1.6132,
      "step": 716
    },
    {
      "epoch": 7.617529880478088,
      "grad_norm": 0.11000783741474152,
      "learning_rate": 7.180851063829787e-05,
      "loss": 1.7152,
      "step": 717
    },
    {
      "epoch": 7.628154050464808,
      "grad_norm": 0.08897817879915237,
      "learning_rate": 7.148936170212766e-05,
      "loss": 1.5745,
      "step": 718
    },
    {
      "epoch": 7.6387782204515275,
      "grad_norm": 0.10918507725000381,
      "learning_rate": 7.117021276595744e-05,
      "loss": 1.5187,
      "step": 719
    },
    {
      "epoch": 7.649402390438247,
      "grad_norm": 0.09338153153657913,
      "learning_rate": 7.085106382978723e-05,
      "loss": 1.6007,
      "step": 720
    },
    {
      "epoch": 7.660026560424967,
      "grad_norm": 0.08496806770563126,
      "learning_rate": 7.053191489361702e-05,
      "loss": 1.6864,
      "step": 721
    },
    {
      "epoch": 7.670650730411687,
      "grad_norm": 0.10549221187829971,
      "learning_rate": 7.02127659574468e-05,
      "loss": 1.5109,
      "step": 722
    },
    {
      "epoch": 7.681274900398407,
      "grad_norm": 0.10590963810682297,
      "learning_rate": 6.989361702127659e-05,
      "loss": 1.5825,
      "step": 723
    },
    {
      "epoch": 7.691899070385126,
      "grad_norm": 0.10683133453130722,
      "learning_rate": 6.957446808510638e-05,
      "loss": 1.6779,
      "step": 724
    },
    {
      "epoch": 7.702523240371846,
      "grad_norm": 0.0916094034910202,
      "learning_rate": 6.925531914893616e-05,
      "loss": 1.6747,
      "step": 725
    },
    {
      "epoch": 7.713147410358566,
      "grad_norm": 0.0932493656873703,
      "learning_rate": 6.893617021276595e-05,
      "loss": 1.7253,
      "step": 726
    },
    {
      "epoch": 7.723771580345286,
      "grad_norm": 0.0859302282333374,
      "learning_rate": 6.861702127659574e-05,
      "loss": 1.7742,
      "step": 727
    },
    {
      "epoch": 7.7343957503320055,
      "grad_norm": 0.09412194043397903,
      "learning_rate": 6.829787234042552e-05,
      "loss": 1.6183,
      "step": 728
    },
    {
      "epoch": 7.745019920318725,
      "grad_norm": 0.09414759278297424,
      "learning_rate": 6.797872340425531e-05,
      "loss": 1.6006,
      "step": 729
    },
    {
      "epoch": 7.755644090305445,
      "grad_norm": 0.09562089294195175,
      "learning_rate": 6.765957446808509e-05,
      "loss": 1.6352,
      "step": 730
    },
    {
      "epoch": 7.766268260292165,
      "grad_norm": 0.0929509848356247,
      "learning_rate": 6.734042553191488e-05,
      "loss": 1.6145,
      "step": 731
    },
    {
      "epoch": 7.776892430278885,
      "grad_norm": 0.1091294139623642,
      "learning_rate": 6.702127659574467e-05,
      "loss": 1.6392,
      "step": 732
    },
    {
      "epoch": 7.787516600265604,
      "grad_norm": 0.11581815779209137,
      "learning_rate": 6.670212765957445e-05,
      "loss": 1.6188,
      "step": 733
    },
    {
      "epoch": 7.798140770252324,
      "grad_norm": 0.09341456741094589,
      "learning_rate": 6.638297872340424e-05,
      "loss": 1.5791,
      "step": 734
    },
    {
      "epoch": 7.808764940239044,
      "grad_norm": 0.1075185239315033,
      "learning_rate": 6.606382978723404e-05,
      "loss": 1.6229,
      "step": 735
    },
    {
      "epoch": 7.819389110225764,
      "grad_norm": 0.10521744936704636,
      "learning_rate": 6.574468085106383e-05,
      "loss": 1.6352,
      "step": 736
    },
    {
      "epoch": 7.8300132802124836,
      "grad_norm": 0.11255916953086853,
      "learning_rate": 6.542553191489362e-05,
      "loss": 1.5736,
      "step": 737
    },
    {
      "epoch": 7.840637450199203,
      "grad_norm": 0.09888484328985214,
      "learning_rate": 6.51063829787234e-05,
      "loss": 1.5827,
      "step": 738
    },
    {
      "epoch": 7.851261620185923,
      "grad_norm": 0.08509425073862076,
      "learning_rate": 6.478723404255319e-05,
      "loss": 1.7043,
      "step": 739
    },
    {
      "epoch": 7.861885790172643,
      "grad_norm": 0.0948648527264595,
      "learning_rate": 6.446808510638298e-05,
      "loss": 1.6144,
      "step": 740
    },
    {
      "epoch": 7.872509960159363,
      "grad_norm": 0.10867530852556229,
      "learning_rate": 6.414893617021276e-05,
      "loss": 1.6476,
      "step": 741
    },
    {
      "epoch": 7.883134130146082,
      "grad_norm": 0.0871119275689125,
      "learning_rate": 6.382978723404255e-05,
      "loss": 1.7195,
      "step": 742
    },
    {
      "epoch": 7.893758300132802,
      "grad_norm": 0.10046197474002838,
      "learning_rate": 6.351063829787234e-05,
      "loss": 1.7077,
      "step": 743
    },
    {
      "epoch": 7.904382470119522,
      "grad_norm": 0.09629342705011368,
      "learning_rate": 6.319148936170212e-05,
      "loss": 1.5435,
      "step": 744
    },
    {
      "epoch": 7.915006640106242,
      "grad_norm": 0.0989261195063591,
      "learning_rate": 6.287234042553191e-05,
      "loss": 1.6709,
      "step": 745
    },
    {
      "epoch": 7.9256308100929616,
      "grad_norm": 0.10115735977888107,
      "learning_rate": 6.255319148936169e-05,
      "loss": 1.7633,
      "step": 746
    },
    {
      "epoch": 7.936254980079681,
      "grad_norm": 0.08844965696334839,
      "learning_rate": 6.223404255319148e-05,
      "loss": 1.5285,
      "step": 747
    },
    {
      "epoch": 7.946879150066401,
      "grad_norm": 0.10033860802650452,
      "learning_rate": 6.191489361702127e-05,
      "loss": 1.5898,
      "step": 748
    },
    {
      "epoch": 7.957503320053121,
      "grad_norm": 0.10405581444501877,
      "learning_rate": 6.159574468085105e-05,
      "loss": 1.5561,
      "step": 749
    },
    {
      "epoch": 7.968127490039841,
      "grad_norm": 0.11518918722867966,
      "learning_rate": 6.127659574468084e-05,
      "loss": 1.6022,
      "step": 750
    },
    {
      "epoch": 7.97875166002656,
      "grad_norm": 0.09256396442651749,
      "learning_rate": 6.095744680851063e-05,
      "loss": 1.6012,
      "step": 751
    },
    {
      "epoch": 7.98937583001328,
      "grad_norm": 0.10426440834999084,
      "learning_rate": 6.063829787234042e-05,
      "loss": 1.5542,
      "step": 752
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.10007669031620026,
      "learning_rate": 6.031914893617021e-05,
      "loss": 1.5931,
      "step": 753
    },
    {
      "epoch": 8.01062416998672,
      "grad_norm": 0.09333339333534241,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.7565,
      "step": 754
    },
    {
      "epoch": 8.02124833997344,
      "grad_norm": 0.09122121334075928,
      "learning_rate": 5.9680851063829786e-05,
      "loss": 1.7459,
      "step": 755
    },
    {
      "epoch": 8.03187250996016,
      "grad_norm": 0.08640526235103607,
      "learning_rate": 5.936170212765957e-05,
      "loss": 1.6685,
      "step": 756
    },
    {
      "epoch": 8.04249667994688,
      "grad_norm": 0.08363476395606995,
      "learning_rate": 5.9042553191489356e-05,
      "loss": 1.6186,
      "step": 757
    },
    {
      "epoch": 8.053120849933599,
      "grad_norm": 0.09082282334566116,
      "learning_rate": 5.872340425531915e-05,
      "loss": 1.5708,
      "step": 758
    },
    {
      "epoch": 8.063745019920319,
      "grad_norm": 0.0932246595621109,
      "learning_rate": 5.840425531914893e-05,
      "loss": 1.5555,
      "step": 759
    },
    {
      "epoch": 8.074369189907038,
      "grad_norm": 0.09504728764295578,
      "learning_rate": 5.808510638297872e-05,
      "loss": 1.6439,
      "step": 760
    },
    {
      "epoch": 8.084993359893758,
      "grad_norm": 0.10564818978309631,
      "learning_rate": 5.776595744680851e-05,
      "loss": 1.6329,
      "step": 761
    },
    {
      "epoch": 8.095617529880478,
      "grad_norm": 0.09380362927913666,
      "learning_rate": 5.7446808510638294e-05,
      "loss": 1.6738,
      "step": 762
    },
    {
      "epoch": 8.106241699867198,
      "grad_norm": 0.10709302127361298,
      "learning_rate": 5.712765957446808e-05,
      "loss": 1.6053,
      "step": 763
    },
    {
      "epoch": 8.116865869853918,
      "grad_norm": 0.09673403948545456,
      "learning_rate": 5.680851063829786e-05,
      "loss": 1.5486,
      "step": 764
    },
    {
      "epoch": 8.127490039840637,
      "grad_norm": 0.07877417653799057,
      "learning_rate": 5.6489361702127655e-05,
      "loss": 1.7979,
      "step": 765
    },
    {
      "epoch": 8.138114209827357,
      "grad_norm": 0.08197999000549316,
      "learning_rate": 5.617021276595744e-05,
      "loss": 1.5202,
      "step": 766
    },
    {
      "epoch": 8.148738379814077,
      "grad_norm": 0.09732627123594284,
      "learning_rate": 5.5851063829787224e-05,
      "loss": 1.595,
      "step": 767
    },
    {
      "epoch": 8.159362549800797,
      "grad_norm": 0.09588738530874252,
      "learning_rate": 5.5531914893617016e-05,
      "loss": 1.6119,
      "step": 768
    },
    {
      "epoch": 8.169986719787516,
      "grad_norm": 0.09706466645002365,
      "learning_rate": 5.52127659574468e-05,
      "loss": 1.6617,
      "step": 769
    },
    {
      "epoch": 8.180610889774236,
      "grad_norm": 0.08348297327756882,
      "learning_rate": 5.4893617021276586e-05,
      "loss": 1.6923,
      "step": 770
    },
    {
      "epoch": 8.191235059760956,
      "grad_norm": 0.11269991844892502,
      "learning_rate": 5.4574468085106384e-05,
      "loss": 1.4877,
      "step": 771
    },
    {
      "epoch": 8.201859229747676,
      "grad_norm": 0.08473680168390274,
      "learning_rate": 5.425531914893617e-05,
      "loss": 1.7354,
      "step": 772
    },
    {
      "epoch": 8.212483399734396,
      "grad_norm": 0.09455994516611099,
      "learning_rate": 5.3936170212765954e-05,
      "loss": 1.6035,
      "step": 773
    },
    {
      "epoch": 8.223107569721115,
      "grad_norm": 0.08831460028886795,
      "learning_rate": 5.3617021276595745e-05,
      "loss": 1.5772,
      "step": 774
    },
    {
      "epoch": 8.233731739707835,
      "grad_norm": 0.09371376037597656,
      "learning_rate": 5.329787234042553e-05,
      "loss": 1.6272,
      "step": 775
    },
    {
      "epoch": 8.244355909694555,
      "grad_norm": 0.10084487497806549,
      "learning_rate": 5.2978723404255315e-05,
      "loss": 1.6043,
      "step": 776
    },
    {
      "epoch": 8.254980079681275,
      "grad_norm": 0.08981828391551971,
      "learning_rate": 5.26595744680851e-05,
      "loss": 1.6639,
      "step": 777
    },
    {
      "epoch": 8.265604249667994,
      "grad_norm": 0.08610837161540985,
      "learning_rate": 5.234042553191489e-05,
      "loss": 1.6838,
      "step": 778
    },
    {
      "epoch": 8.276228419654714,
      "grad_norm": 0.08751161396503448,
      "learning_rate": 5.2021276595744676e-05,
      "loss": 1.6388,
      "step": 779
    },
    {
      "epoch": 8.286852589641434,
      "grad_norm": 0.10013695061206818,
      "learning_rate": 5.170212765957446e-05,
      "loss": 1.6057,
      "step": 780
    },
    {
      "epoch": 8.297476759628154,
      "grad_norm": 0.0910910964012146,
      "learning_rate": 5.138297872340425e-05,
      "loss": 1.4709,
      "step": 781
    },
    {
      "epoch": 8.308100929614874,
      "grad_norm": 0.08940661698579788,
      "learning_rate": 5.106382978723404e-05,
      "loss": 1.6408,
      "step": 782
    },
    {
      "epoch": 8.318725099601593,
      "grad_norm": 0.10172027349472046,
      "learning_rate": 5.074468085106382e-05,
      "loss": 1.6499,
      "step": 783
    },
    {
      "epoch": 8.329349269588313,
      "grad_norm": 0.09930676221847534,
      "learning_rate": 5.0425531914893614e-05,
      "loss": 1.7434,
      "step": 784
    },
    {
      "epoch": 8.339973439575033,
      "grad_norm": 0.07886148989200592,
      "learning_rate": 5.01063829787234e-05,
      "loss": 1.5906,
      "step": 785
    },
    {
      "epoch": 8.350597609561753,
      "grad_norm": 0.08584027737379074,
      "learning_rate": 4.978723404255318e-05,
      "loss": 1.566,
      "step": 786
    },
    {
      "epoch": 8.361221779548472,
      "grad_norm": 0.09215527772903442,
      "learning_rate": 4.946808510638297e-05,
      "loss": 1.687,
      "step": 787
    },
    {
      "epoch": 8.371845949535192,
      "grad_norm": 0.07838325202465057,
      "learning_rate": 4.914893617021276e-05,
      "loss": 1.6549,
      "step": 788
    },
    {
      "epoch": 8.382470119521912,
      "grad_norm": 0.08557015657424927,
      "learning_rate": 4.882978723404255e-05,
      "loss": 1.5446,
      "step": 789
    },
    {
      "epoch": 8.393094289508632,
      "grad_norm": 0.09984306246042252,
      "learning_rate": 4.851063829787234e-05,
      "loss": 1.6634,
      "step": 790
    },
    {
      "epoch": 8.403718459495352,
      "grad_norm": 0.09797338396310806,
      "learning_rate": 4.819148936170213e-05,
      "loss": 1.7051,
      "step": 791
    },
    {
      "epoch": 8.414342629482071,
      "grad_norm": 0.12240539491176605,
      "learning_rate": 4.787234042553191e-05,
      "loss": 1.6348,
      "step": 792
    },
    {
      "epoch": 8.424966799468791,
      "grad_norm": 0.08715664595365524,
      "learning_rate": 4.75531914893617e-05,
      "loss": 1.6349,
      "step": 793
    },
    {
      "epoch": 8.435590969455511,
      "grad_norm": 0.0934719368815422,
      "learning_rate": 4.723404255319149e-05,
      "loss": 1.6198,
      "step": 794
    },
    {
      "epoch": 8.44621513944223,
      "grad_norm": 0.09271188080310822,
      "learning_rate": 4.6914893617021274e-05,
      "loss": 1.6527,
      "step": 795
    },
    {
      "epoch": 8.45683930942895,
      "grad_norm": 0.09488017857074738,
      "learning_rate": 4.659574468085106e-05,
      "loss": 1.6116,
      "step": 796
    },
    {
      "epoch": 8.46746347941567,
      "grad_norm": 0.09034846723079681,
      "learning_rate": 4.627659574468085e-05,
      "loss": 1.7377,
      "step": 797
    },
    {
      "epoch": 8.47808764940239,
      "grad_norm": 0.08139494806528091,
      "learning_rate": 4.5957446808510635e-05,
      "loss": 1.6975,
      "step": 798
    },
    {
      "epoch": 8.48871181938911,
      "grad_norm": 0.09340277314186096,
      "learning_rate": 4.563829787234042e-05,
      "loss": 1.5524,
      "step": 799
    },
    {
      "epoch": 8.49933598937583,
      "grad_norm": 0.1020977720618248,
      "learning_rate": 4.5319148936170204e-05,
      "loss": 1.6116,
      "step": 800
    }
  ],
  "logging_steps": 1,
  "max_steps": 940,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 200,
  "total_flos": 1.2672302188245811e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
